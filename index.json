[{"content":"Prometheus is written in Go and uses RE2 and that does not support negative lookahead. From convenience perspective, its easier to write human-readable regex using that, but as its not supported, we\u0026rsquo;ll see a workaround for that.\nSay for example, you have a label my_label and you\u0026rsquo;d like to relabel all the label values to custom_value where it contains my_value but only when it does not contain foobar.\nIf negative lookahead was supported, you could have written something like this:\n- source_labels: [my_label] regex: \u0026#39;my_value(?!_foobar)\u0026#39; target_label: my_label replacement: \u0026#39;my_value\u0026#39; So a matching metric would be:\nmy_metric{my_label=\u0026#34;my_value\u0026#34;} -\u0026gt; my_metric{my_label=\u0026#34;custom_value\u0026#34;} And a non-matching metric would be:\nmy_metric{my_label=\u0026#34;my_value_foobar_long_value\u0026#34;} -\u0026gt; my_metric{my_label=\u0026#34;my_value_foobar_long_value\u0026#34;} But since its not supported, you\u0026rsquo;ll get :\nerror parsing regexp: invalid or unsupported Perl syntax: `(?!` Solution What you can do is to specify the pattern with each possible suffix that you\u0026rsquo;d like to exclude. So in our case, we\u0026rsquo;d like to exclude foobar so the regex pattern would be:\nregex: (my_value)_(?:[^f]|f[^o]|fo[^o]|foo[^b]|foob[^a]|fooba[^r]).* Now, this is not as clean as negative lookahead, but this way works, and you can exclude any suffix you\u0026rsquo;d like, so the relabeling will work as expected.\n- source_labels: [my_label] regex: \u0026#39;(my_value)_(?:[^f]|f[^o]|fo[^o]|foo[^b]|foob[^a]|fooba[^r]).*\u0026#39; target_label: my_label replacement: \u0026#39;my_value\u0026#39; You can test this relabeling using https://relabeler.promlabs.com. Here\u0026rsquo;s how it looks :\nPossible Issues As we match each char, the relabeling will not work even if the label contains any char even if its just foo or fooba etc as long as it starts with f and ends with r. So you\u0026rsquo;ll have to be careful with the regex pattern you write. ","permalink":"https://tanmay-bhat.github.io/posts/prometheus-relabel-negetive-lookahead/","summary":"\u003cp\u003ePrometheus is written in Go and uses \u003ca href=\"https://github.com/google/re2/wiki/Syntax\"\u003eRE2\u003c/a\u003e and that does not support negative lookahead. From convenience perspective, its easier to write human-readable regex using that, but as its not supported, we\u0026rsquo;ll see a workaround for that.\u003c/p\u003e\n\u003cp\u003eSay for example, you have a label \u003ccode\u003emy_label\u003c/code\u003e and you\u0026rsquo;d like to relabel all the label values to \u003ccode\u003ecustom_value\u003c/code\u003e where it contains \u003ccode\u003emy_value\u003c/code\u003e but only when it does not contain \u003ccode\u003efoobar\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf negative lookahead was supported, you could have written something like this:\u003c/p\u003e","title":"Negetive Lookahead in Prometheus Relabel Config (sort of)"},{"content":"A Beginner\u0026rsquo;s Perspective Before we dive into the details, I recently started learning Go. This article is just a beginner\u0026rsquo;s perspective on combining Go learning and building a simple Prometheus metrics exporter.\nI had a requirement to build this exporter because, at my workplace, we use Datadog\u0026rsquo;s SLO product alongside RUM monitoring. However, since all our other analytics and metrics are in Prometheus, I built this to consolidate all SLOs in one place.\nDatadog API Response Example For an example request curl -X GET \u0026quot;https://api.datadoghq.com/api/v1/slo/${slo_id}/history\u0026quot;, the response is as follows:\n{ \u0026#34;data\u0026#34;: { \u0026#34;overall\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Example SLO\u0026#34;, \u0026#34;sli_value\u0026#34;: 0.99 ... }, \u0026#34;slo\u0026#34;: { \u0026#34;target_threshold\u0026#34;: 0.95, \u0026#34;timeframe\u0026#34;: \u0026#34;7d\u0026#34; ... }, ... } } Exporter Requirements For simplicity, let\u0026rsquo;s focus on creating a Prometheus exporter that:\nParses the Datadog API response Creates Prometheus metrics for the following example data: datadog_slo_uptime{rolling_timeframe=\u0026#34;\u0026lt;\u0026gt;\u0026#34; slo_name=\u0026#34;\u0026lt;\u0026gt;\u0026#34; threshold=\u0026#34;\u0026lt;\u0026gt;\u0026#34; window=\u0026#34;\u0026lt;\u0026gt;\u0026#34;} datadog_api_error_total{api_call=\u0026#34;\u0026lt;\u0026gt;\u0026#34;,status_code=\u0026#34;\u0026lt;\u0026gt;\u0026#34;} Importing Required Client Packages As we need clients of Datadog and Prometheus, let\u0026rsquo;s import both:\nimport ( \u0026#34;github.com/Datadog/datadog-api-client-go/v2/api/datadog\u0026#34; \u0026#34;github.com/Datadog/datadog-api-client-go/v2/api/datadogV1\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/push\u0026#34; ) Constructing Structs to Match API Response As the data we need is inside the data field, in the API response, let\u0026rsquo;s create a struct to parse them later on.\nA struct in Go is a container to group similar or related data together, somewhat similar to classes in other object-oriented languages.\nLet\u0026rsquo;s create a struct called Response which will contain Data of type Data struct and to match the overall structure of the API response :\ntype Response struct { Data Data `json:\u0026#34;data\u0026#34;` } We specified json:\u0026quot;data\u0026quot; because that\u0026rsquo;s the way of mapping the actual JSON response\u0026rsquo;s data field to this object.\nAs overall and slo are nested fields of data, let\u0026rsquo;s create a struct called Data and add these two inside that:\ntype Data struct { Overall Overall `json:\u0026#34;overall\u0026#34;` Slo Slo `json:\u0026#34;slo\u0026#34;` } Now, let\u0026rsquo;s create structs called Overall and Slo which will contain the actual data we need:\ntype Overall struct { Name string `json:\u0026#34;name\u0026#34;` SLI float64 `json:\u0026#34;sli_value\u0026#34;` } type Slo struct { Threshold float64 `json:\u0026#34;target_threshold\u0026#34;` Timeframe string `json:\u0026#34;timeframe\u0026#34;` } Global Variables and Metric Declarations We declare the following variables:\nvar ( API_KEY = os.Getenv(\u0026#34;DD_API_KEY\u0026#34;) APP_KEY = os.Getenv(\u0026#34;DD_APP_KEY\u0026#34;) SLO_ID = os.Getenv(\u0026#34;DD_SLO_ID\u0026#34;) PROM_ENDPOINT = os.Getenv(\u0026#34;PROMETHEUS_ENDPOINT\u0026#34;) ctx context.Context api *datadogV1.ServiceLevelObjectivesApi apiClient *datadog.APIClient // Declare Prometheus metrics for monitoring Datadog SLO DataDogSLOGauge = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: \u0026#34;datadog\u0026#34;, Name: \u0026#34;slo_uptime\u0026#34;, Help: \u0026#34;History details of a Datadog SLO\u0026#34;}, []string{\u0026#34;slo_name\u0026#34;, \u0026#34;threshold\u0026#34;, \u0026#34;window\u0026#34;, \u0026#34;rolling_timeframe\u0026#34;}, ) // Declare Prometheus metrics for monitoring Datadog API Errors DataDogAPIErrorCounter = prometheus.NewCounterVec(prometheus.CounterOpts{ Namespace: \u0026#34;datadog\u0026#34;, Name: \u0026#34;api_error_total\u0026#34;, Help: \u0026#34;Total Error count on requests to Datadog API\u0026#34;}, []string{\u0026#34;api_call\u0026#34;, \u0026#34;status_code\u0026#34;}, ) logger *log.Logger ) Implementing the Exporter Now that we have our structs defined to match the Datadog API response, let\u0026rsquo;s implement the core functionality of our exporter.\nInitializing the Datadog Client\nFirst, we initialize the Datadog client. We\u0026rsquo;ll create a function called InitDataDogClient():\nfunc InitDataDogClient() error { ctx = context.WithValue(context.Background(), datadog.ContextAPIKeys, map[string]datadog.APIKey{ \u0026#34;apiKeyAuth\u0026#34;: {Key: API_KEY}, \u0026#34;appKeyAuth\u0026#34;: {Key: APP_KEY}, }, ) configuration := datadog.NewConfiguration() configuration.RetryConfiguration.EnableRetry = true // defaults to 3 retries apiClient = datadog.NewAPIClient(configuration) api = datadogV1.NewServiceLevelObjectivesApi(apiClient) return nil } Context in Go is a way to manage timeouts, cancellation, and passing data like API keys across multiple functions or API boundaries. In the above example, context is used to pass the Datadog API credentials along with API requests.\nThe above function also checks if the necessary environment variables are set, creates a context with the API keys, and initializes the Datadog client and then create an instance of NewServiceLevelObjectivesApi API and set it to api variable.\nFetching SLO Data\nNext, let\u0026rsquo;s implement the GetSloData function to fetch SLO data from Datadog:\nfunc GetSloData(sloDataID string, daysWindow int) (sliValue float64, sloName string, threshold float64, rollingTimeframe string, error error) { if sloDataID == \u0026#34;\u0026#34; { logger.Fatal(\u0026#34;sloDataID environment variable is not set\u0026#34;) } fromTime := time.Now().AddDate(0, 0, -daysWindow).Unix() toTime := time.Now().Unix() resp, r, err := api.GetSLOHistory(ctx, sloDataID, fromTime, toTime) if err != nil { statusCode := 0 if r != nil { statusCode = r.StatusCode } DataDogAPIErrorCounter.WithLabelValues(\u0026#34;GetSLOHistory\u0026#34;, strconv.Itoa(statusCode)).Inc() return 0, \u0026#34;\u0026#34;, 0, \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;error when calling `ServiceLevelObjectivesApi.GetSLOHistory`: %v\u0026#34;, err) } responseContent, err := json.Marshal(resp) if err != nil { return 0, \u0026#34;\u0026#34;, 0, \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;error marshaling response: %v\u0026#34;, err) } var response Response err = json.Unmarshal(responseContent, \u0026amp;response) if err != nil { logger.Printf(\u0026#34;Error unmarshaling JSON: %s\\n\u0026#34;, err) return } sli = response.Data.Overall.SLI slo = response.Data.Overall.Name threshold = response.Data.Slo.Threshold rollingTimeframe = response.Data.Slo.Timeframe return sli, slo, threshold, rollingTimeframe, nil } This function takes the SLO ID and the number of days to look back as parameters. It then makes a request to the Datadog API, parses the response, and returns the relevant SLO data.\nAPI Request api.GetSLOHistory is called with the provided SLO ID and time window. Returns three values: resp (SLOHistoryResponse struct): Contains the API response data. r (_nethttp.Response): The actual HTTP response. err (error): Any error that occurred during the request. Error Handling \u0026amp; Metric Update If an error occurs, update the DataDogAPIErrorCounter metric: Set the api_call label to \u0026quot;GetSLOHistory\u0026quot;. Convert the HTTP status code (r.StatusCode) to a string using strconv.Itoa() and set it as the status_code label. This is needed as labels can only be of type string in Prometheus. Increment the metric value using Inc(). Response Parsing As we know that resp is of type SLOHistoryResponse which is a struct, convert it into a JSON using json.Marshal(resp) and store the result in responseContent. Declare a Response struct variable, response. Unmarshal the JSON data into the response struct using json.Unmarshal(responseContent, \u0026amp;response). It takes JSON data and a pointer as arguments. In our case, we passed a pointer to the response variable (\u0026amp;response). Extract Relevant SLO Data Access the parsed data using the response struct, e.g., sliValue = response.Data.Overall.SLI Note: Prometheus labels are always sorted alphabetically.\nMain Function\nNow, let\u0026rsquo;s implement our main() function to tie everything together:\nfunc main() { logger = log.New(os.Stdout, \u0026#34;\u0026#34;, log.Ldate|log.Ltime|log.Lshortfile) err := InitDataDogClient() if err != nil { logger.Printf(\u0026#34;Failed to initialize Datadog client: %v\\n\u0026#34;, err) return } logger.Printf(\u0026#34;Datadog client initialized successfully\\n\u0026#34;) // Use a custom registry for consistency and to drop default go_.* metrics registry := prometheus.NewRegistry() registry.MustRegister(DataDogSLOGauge, DataDogAPIErrorCounter) days := []int{7, 30, 90} // Fetch SLO data for different time windows for _, day := range days { logger.Printf(\u0026#34;Fetching SLO history for %d days\\n\u0026#34;, day) sliValue, sloName, threshold, rollingTimeframe, err := GetSloData(SLO_ID, day) if err != nil { logger.Printf(\u0026#34;Error getting SLO data: %v\\n\u0026#34;, err) } else { g := DataDogSLOGauge.WithLabelValues( sloName, fmt.Sprintf(\u0026#34;%.2f\u0026#34;, threshold), fmt.Sprintf(\u0026#34;%dd\u0026#34;, day), rollingTimeframe, ) g.Set(sliValue) } } // Collect all the metrics and push to Prometheus endpoint pusher := push.New(PROM_ENDPOINT, \u0026#34;datadog-slo-exporter\u0026#34;).Gatherer(registry) if err := pusher.Push(); err != nil { logger.Printf(\u0026#34;Error pushing metrics to VictoriaMetrics: %v\\n\u0026#34;, err) } else { logger.Printf(\u0026#34;Metrics pushed to VictoriaMetrics successfully\\n\u0026#34;) } } I set the model to push instead of Prometheus\u0026rsquo;s default pull since we don\u0026rsquo;t need to run this as a service. The best way to run this is as a cronjob.\nKey Concepts in Prometheus Client Library Collector : A collector is a part of an exporter that represents a set of metrics. It may be a single metric if it is part of direct instrumentation, or many metrics if it is pulling metrics from another system. In our case, we used two collector types, Gauge and Counter. Registry : A registry is a central place to store and manage multiple collectors. It keeps track of all registered metrics and their values. Prometheus client by default has a registry which collects go related metrics, for our simple use case, it\u0026rsquo;s overkill, hence we create a new registry and register our metrics to that. registry := prometheus.NewRegistry() registry.MustRegister(DataDogSLOGauge, DataDogAPIErrorCounter) Gatherer : A gatherer is an interface responsible for collecting metrics from registered collectors. It gathers metrics from all collectors in a registry and prepares them for scraping by Prometheus. As we use a custom registry called registry, while pushing the metric, we specify that. push.New(PROM_ENDPOINT, \u0026#34;datadog-slo-exporter\u0026#34;).Gatherer(registry) Using Prometheus Pull (Alternative to Push) import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) func main() { http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler(registry)) http.ListenAndServe(\u0026#34;:2112\u0026#34;, nil) } References https://docs.datadoghq.com/api/latest/service-level-objectives/#get-an-slos-history https://github.com/prometheus/client_golang https://github.com/Datadog/datadog-api-client-go https://prometheus.io/docs/practices/instrumentation/ Complete code can be found here : https://github.com/tanmay-bhat/datadog-slo-exporter\n","permalink":"https://tanmay-bhat.github.io/posts/learning-go-by-instrumenting-a-go-application-for-prometheus-metrics/","summary":"\u003ch3 id=\"a-beginners-perspective\"\u003eA Beginner\u0026rsquo;s Perspective\u003c/h3\u003e\n\u003cp\u003eBefore we dive into the details, I recently started learning Go. This article is just a beginner\u0026rsquo;s perspective on combining Go learning and building a simple Prometheus metrics exporter.\u003c/p\u003e\n\u003cp\u003eI had a requirement to build this exporter because, at my workplace, we use \u003ca href=\"https://www.datadoghq.com/product/service-level-objectives/\"\u003eDatadog\u0026rsquo;s SLO\u003c/a\u003e product alongside RUM monitoring. However, since all our other analytics and metrics are in Prometheus, I built this to consolidate all SLOs in one place.\u003c/p\u003e","title":"Learning Go by Instrumenting a Go Application for Prometheus Metrics"},{"content":"In the world of GitOps, even experienced DevOps engineers occasionally encounter issues stemming from simple typos or misconfigurations in these YAML files. To mitigate these risks and ensure compliance with organizational policies, we can bring in validation tools.\nThis post explores two powerful options: Conftest, which leverages Open Policy Agent (OPA), and Kubeconform. We\u0026rsquo;ll dive into how these tools can be implemented to streamline your validation processes.\nArgoCD custom policy validation with Conftest (OPA) Open Policy Agent\u0026rsquo;s Conftest serves as an excellent tool for custom config rule validation, offering a more flexible alternative to script-based validation.\nYou can write any custom logic that is required as per your organization and enforce those rules easily.\nTo get started, download the Conftest binary from the Github releases page.\nConftest policies are written in Rego, a declarative language built on top of Go. Here\u0026rsquo;s a basic policy to validate the existence of a required label:\nHere are a few example policies to demonstrate a couple of use cases :\nEnforcing label on each application package main required_labels := {\u0026#34;owner-team\u0026#34;, \u0026#34;cost-center\u0026#34;, \u0026#34;environment\u0026#34;} deny[msg] { some required_label in required_labels not input.metadata.labels[required_label] msg := sprintf(\u0026#34;Missing required label: \u0026#39;%v\u0026#39;\u0026#34;, [required_label]) } One can use iteration through the some keyword, allowing you to traverse YAML structures. The input object serves as the root of your YAML document, from which you can access nested fields using dot notation.\nFAIL - kubeseal.yaml - main - Missing required label: \u0026#39;owner-team\u0026#39; FAIL - kubeseal.yaml - main - Missing required label: \u0026#39;cost-center\u0026#39; FAIL - kubeseal.yaml - main - Missing required label: \u0026#39;environment\u0026#39; 3 tests, 0 passed, 0 warnings, 3 failures, 0 exceptions Enforce SSH for git repositories deny[msg] { not startswith(input.spec.source.repoURL, \u0026#34;git@github.com:\u0026#34;) msg := sprintf(\u0026#34;Invalid repoURL: \u0026#39;%v\u0026#39;. Must start with \u0026#39;git@github.com:\u0026#39;\u0026#34;, [input.spec.source.repoURL]) } Restrict ArgoCD projects allowed_projects := {\u0026#34;prod\u0026#34;, \u0026#34;staging\u0026#34;, \u0026#34;dev\u0026#34;} deny[msg] { not allowed_projects[input.spec.project] msg := sprintf(\u0026#34;Project \u0026#39;%v\u0026#39; is not allowed. Must be one of: %v\u0026#34;, [input.spec.project, allowed_projects]) } FAIL - kubeseal.yaml - main - Project \u0026#39;foobar\u0026#39; is not allowed. Must be one of: {\u0026#34;dev\u0026#34;, \u0026#34;prod\u0026#34;, \u0026#34;staging\u0026#34;} Warn on suggested sync changes warn[msg] { not \u0026#34;ApplyOutOfSyncOnly=true\u0026#34; in input.spec.syncPolicy.syncOptions msg := \u0026#34;ApplyOutOfSyncOnly=true is not set in syncOptions. This may lead to unnecessary syncs.\u0026#34; } warn[msg] { not input.spec.syncPolicy.automated.selfHeal msg := \u0026#34;SelfHeal is not enabled in syncPolicy.automated. This may prevent automatic drift corrections.\u0026#34; } ArgoCD schema validation with Kubeconform Kubeconform is a powerful Kubernetes resource schema validator that supports custom resources. It\u0026rsquo;s particularly useful for catching common typos and structural errors in ArgoCD Application resources. Download the Kubeconform binary from the official releases page. Let\u0026rsquo;s validate an example manifest with an incorrect structure:\nTo test the schema validation, let\u0026rsquo;s have an example manifest with incorrect value saved as kubeseal.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: sealed-secrets namespace: argocd spec: project: default sources: chart: sealed-secrets repoURL: https://bitnami-labs.github.io/sealed-secrets targetRevision: 1.16.1 helm: releaseName: sealed-secrets destination: server: \u0026#34;https://kubernetes.default.svc\u0026#34; namespace: kubeseal When we run schema validation with kubeconform :\nkubeconform --output pretty -strict -exit-on-error \\ -schema-location default -schema-location \\ \u0026#39;https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/{{.Group}}/{{.ResourceKind}}_{{.ResourceAPIVersion}}.json\u0026#39; kubeseal.yaml ✖ argocd-applications/us-west-2/prod-us-new/bad-example.yaml: Application sealed-secrets is invalid: problem validating schema. Check JSON formatting: jsonschema: \u0026#39;/spec/sources\u0026#39; does not validate with https://raw.githubusercontent.com/datreeio/CRDs-catalog/main/argoproj.io/application_v1alpha1.json#/properties/spec/properties/sources/type: expected array, but got object The error message shows a schema violation: the spec.sources field should be an array, but in our example, it was incorrectly defined as a single object. This type of structural error is exactly what Kubeconform excels at catching.\nKubeconform can be run on directory as well, allowing for batch validation.\nNotable points Both conftest and kubeconform are best suited to run inside CI to catch before its committed, just like any other test cases. Although some of these use cases can be caught if the application manifests are generated as part of helm template, they still provide easy to configure validations. Conftest extends beyond Kubernetes, can be applied to terraform, docker and other configurations as well. References https://www.conftest.dev/\nhttps://www.openpolicyagent.org/\nhttps://github.com/yannh/kubeconform\n","permalink":"https://tanmay-bhat.github.io/posts/validate-argocd-with-opa/","summary":"\u003cp\u003eIn the world of GitOps, even experienced DevOps engineers occasionally encounter issues stemming from simple typos or misconfigurations in these YAML files. To mitigate these risks and ensure compliance with organizational policies, we can bring in validation tools.\u003c/p\u003e\n\u003cp\u003eThis post explores two powerful options: Conftest, which leverages Open Policy Agent (OPA), and Kubeconform. We\u0026rsquo;ll dive into how these tools can be implemented to streamline your validation processes.\u003c/p\u003e\n\u003ch3 id=\"argocd-custom-policy-validation-with-conftest-opa\"\u003eArgoCD custom policy validation with Conftest (OPA)\u003c/h3\u003e\n\u003cp\u003eOpen Policy Agent\u0026rsquo;s Conftest serves as an excellent tool for custom config rule validation, offering a more flexible alternative to script-based validation.\u003c/p\u003e","title":"Validating ArgoCD Application Manifest With Open Policy Agent"},{"content":"In this article, you\u0026rsquo;ll learn how to leverage ArgoCD ApplicationSets with custom generators to streamline multi-tenant deployments. By the end, you\u0026rsquo;ll understand how to create a custom generator plugin, set up an ApplicationSet, and use features like selective deployments and Go templating.\nPrerequisite : ArgoCD installed in a Kubernetes cluster. Writing custom generator plugin Generators are responsible for generating parameters, which are then rendered into the template: fields of the ApplicationSet resource. See the Introduction for an example of how generators work with templates, to create Argo CD Applications.\nThe plugin can be written in any language as long as it responds to HTTP requests, and in our example, we will be writing it in Python.\nLet’s assume we have a DB that keeps track of tenant’s state.\nNow, we will write the generator plugin service which returns of tenant and thier status in list of dictionary as result :\nimport os import json from typing import List, Dict from fastapi import FastAPI, HTTPException, Depends, Security from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials import psycopg2 from psycopg2.extras import RealDictCursor # Plugin secret PLUGIN_TOKEN = os.getenv(\u0026#34;AUTH_TOKEN\u0026#34;) # PostgreSQL connection parameters DB_PARAMS = { \u0026#39;dbname\u0026#39;: os.getenv(\u0026#39;DB_NAME\u0026#39;, \u0026#39;organization\u0026#39;), \u0026#39;user\u0026#39;: os.getenv(\u0026#39;DB_USER\u0026#39;, \u0026#39;postgres\u0026#39;), \u0026#39;host\u0026#39;: os.getenv(\u0026#39;DB_HOST\u0026#39;, \u0026#39;localhost\u0026#39;), \u0026#39;port\u0026#39;: os.getenv(\u0026#39;DB_PORT\u0026#39;, \u0026#39;5432\u0026#39;) } app = FastAPI() security = HTTPBearer() async def query_postgres() -\u0026gt; List[Dict[str, str]]: try: conn = psycopg2.connect(**DB_PARAMS) with conn.cursor(cursor_factory=RealDictCursor) as cur: cur.execute(\u0026#34;SELECT tenant, status FROM organization\u0026#34;) rows = cur.fetchall() conn.close() return [dict(row) for row in rows] except Exception as e: print(f\u0026#34;Database error: {e}\u0026#34;) return [] def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)): if credentials.scheme != \u0026#34;Bearer\u0026#34; or credentials.credentials != PLUGIN_TOKEN: raise HTTPException(status_code=403, detail=\u0026#34;Invalid or missing token\u0026#34;) return credentials @app.post(\u0026#34;/api/v1/getparams.execute\u0026#34;) async def get_params_execute(credentials: HTTPAuthorizationCredentials = Depends(verify_token)): tenants = await query_postgres() return { \u0026#34;output\u0026#34;: { \u0026#34;parameters\u0026#34;: tenants } } if __name__ == \u0026#34;__main__\u0026#34;: import uvicorn uvicorn.run(app, host=\u0026#34;0.0.0.0\u0026#34;, port=4355) Let’s send a curl request to test the plugin response :\ncurl -X POST http://localhost:4355/api/v1/getparams.execute \\ -H \u0026#34;Authorization: Bearer $AUTH_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; {\u0026#34;output\u0026#34;:{\u0026#34;parameters\u0026#34;:[{\u0026#34;tenant\u0026#34;:\u0026#34;Acme Corp\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;active\u0026#34;}, {\u0026#34;tenant\u0026#34;:\u0026#34;Gamma Industries\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;trial\u0026#34;}]}} All the files, needed for this demo such as Dockerfile, k8s manifests and Postgres DB are in this GitHub repository : https://github.com/tanmay-bhat/argocd-plugin-generator-demo\nCreating ArgoCD Applicationset Now that we have plugin ready, let’s create an applicationset with below contents :\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: tenant-generator-plugin-demo namespace: argocd spec: generators: - plugin: configMapRef: name: tenant-generator-plugin requeueAfterSeconds: 120 goTemplate: true template: metadata: name: \u0026#39;foobar-app-{{ .tenant | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; | trunc 53 }}\u0026#39; labels: org: \u0026#39;{{ .tenant }}\u0026#39; spec: project: default destination: server: \u0026#34;https://kubernetes.default.svc\u0026#34; namespace: foobar-service source: chart: podinfo repoURL: https://stefanprodan.github.io/podinfo targetRevision: 6.7.0 helm: parameters: - name: \u0026#34;fullnameOverride\u0026#34; value: \u0026#39;foobar-app-{{ .tenant | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; | trunc 53 }}\u0026#39; syncPolicy: automated: prune: true selfHeal: true allowEmpty: true syncOptions: - ApplyOutOfSyncOnly=true - CreateNamespace=true spec.generators[0].plugin.configMapRef : This is a config which has details on endpoint to connect to plugin and the auth token :\napiVersion: v1 data: baseUrl: http://tenant-generator-plugin:8080 token: $api-credentials:plugin.auth.token kind: ConfigMap For the token, the syntax is : $secret_name:plugin.myplugin.token.\nWe can set goTemplate: true and then use templating, such has lower | replace \u0026quot;_\u0026quot; \u0026quot;-\u0026quot;\nrequeueAfterSeconds : The interval on which ArgoCD queries the plugin for updates.\nHere’s how the flow looks like at the end:\n[Custom Generator Plugin] --\u0026gt; [ApplicationSet] --\u0026gt; [Argo CD Applications] ^ | | v [PostgreSQL Database] [Kubernetes Cluster] Once we apply the above applicationset, we can see the applications generated for each tenant :\nk get applications NAME SYNC STATUS HEALTH STATUS foobar-app-global-tech Synced Healthy foobar-app-beta-solutions Synced Healthy foobar-app-jupiter-networks Synced Healthy foobar-app-echo-innovations Synced Healthy foobar-app-innova-systems Synced Healthy foobar-app-acme-corp Synced Healthy foobar-app-horizon-enterprises Synced Healthy foobar-app-foxtrot-consulting Synced Healthy foobar-app-gamma-industries Synced Healthy foobar-app-delta-technologies Synced Healthy As we can see from above, we have ArgoCD application for each tenant.\nDeprovisioning As the application is created using ArgoCD, as soon as the plugin stops sending details for a deployed tenant, ArgoCD assumes that its not needed anymore and deprovisions it.\nIt happens not on request failure, but change in output parameters from plugin.\nSelective deployments using output filtering If we have a requirement to deploy only tenants in status: active and not in trial state, we can use the below to filter :\nspec: generators: - plugin: configMapRef: name: tenant-generator-plugin selector: matchExpressions: - key: status operator: In values: - active It\u0026rsquo;s a standard Kubernetes selector, and it\u0026rsquo;s a list, multiple values can be provided.\nPassing Input parameters to Plugin Instead of above filtering that will happen on client i.e ArgoCD side, if you implement the logic in plugin service to accept parameters, ArgoCD can send that while fetching tenants, for example : curl -X POST http://localhost:4355/api/v1/getparams.execute \\ -H \u0026#34;Authorization: Bearer $AUTH_TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;applicationSetName\u0026#34;: \u0026#34;fake-appset\u0026#34;, \u0026#34;input\u0026#34;: { \u0026#34;parameters\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } } }\u0026#39; Note :\nAs of now, the applicationSet controller doesn\u0026rsquo;t provide built-in metrics for tracking request rates to plugin services. To address this and gain better visibility into your custom generator\u0026rsquo;s performance, consider adding a small metrics exporter within your generator code. You can also combine generator plugin with existing generators while creating an application. Reference :\nhttps://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Plugin/\nhttps://pkg.go.dev/github.com/plumber-cd/argocd-applicationset-namespaces-generator-plugin#section-readme\nhttps://github.com/prometheus/client_python\nhttps://github.com/tanmay-bhat/argocd-plugin-generator-demo\n","permalink":"https://tanmay-bhat.github.io/posts/argocd-plugin-generator-multitenant-deployment/","summary":"\u003cp\u003eIn this article, you\u0026rsquo;ll learn how to leverage ArgoCD ApplicationSets with custom generators to streamline multi-tenant deployments. By the end, you\u0026rsquo;ll understand how to create a custom generator plugin, set up an ApplicationSet, and use features like selective deployments and Go templating.\u003c/p\u003e\n\u003ch3 id=\"prerequisite-\"\u003ePrerequisite :\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eArgoCD installed in a Kubernetes cluster.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"writing-custom-generator-plugin\"\u003eWriting custom generator plugin\u003c/h3\u003e\n\u003cp\u003eGenerators are responsible for generating \u003cem\u003eparameters\u003c/em\u003e, which are then rendered into the \u003ccode\u003etemplate:\u003c/code\u003e fields of the ApplicationSet resource. See the \u003ca href=\"https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/\"\u003eIntroduction\u003c/a\u003e for an example of how generators work with templates, to create Argo CD Applications.\u003c/p\u003e","title":"Leveraging ArgoCD ApplicationSet with Plugin Generator to Streamline Multi-Tenant Deployments"},{"content":"JSON Web Tokens (JWTs) offer seamless sign-in, allowing users to carry their authentication securely across different applications within the same ecosystem. In this article, let\u0026rsquo;s go through the process of configuring JWT-based authentication with Grafana for a smoother user experience.\nCreate a RSA key pair Begin by creating an RSA key pair. The private key will sign the JWT token, while the public key will verify it.\nssh-keygen -t rsa -b 4096 -m PEM -f grafana.key -N \u0026#34;\u0026#34; openssl rsa -in grafana.key -pubout -outform PEM -out grafana.key.pub Configure JWT authentication With the key pair in place, configure JWT authentication in Grafana. You can use environment variables or a configuration file. Here\u0026rsquo;s an example using environment variables:\nGF_AUTH_JWT_ENABLED=true GF_AUTH_JWT_URL_LOGIN=true GF_AUTH_JWT_HEADER_NAME=X-AUTH-TOKEN GF_AUTH_JWT_KEY_FILE=/etc/grafana/public-key.pem GF_AUTH_JWT_EMAIL_CLAIM=sub GF_AUTH_JWT_USERNAME_CLAIM=user If you would like to use configuration file, you can use the following configuration in your grafana.ini file:\n[auth.jwt] enabled = true header_name = X-AUTH-TOKEN email_claim = sub username_claim = user key_file = /etc/grafana/public-key.pem url_login = true In the above configuration, we have set GF_AUTH_JWT_URL_LOGIN to true. This will enable the URL based login by passing the JWT token as a query parameter. We will see this in action in the next section.\nX-AUTH-TOKEN is used to pass the JWT token via HTTP header. If you would like to use a different header, you can set the GF_AUTH_JWT_HEADER_NAME environment variable.\nGF_AUTH_JWT_EMAIL_CLAIM=sub specifies that Grafana should look for the email address of the user in the sub claim of the JWT token. The sub claim is traditionally used to represent the subject of the token, but in this case, it is repurposed to carry the email information.\nGF_AUTH_JWT_USERNAME_CLAIM=user indicates that Grafana should retrieve the username of the user from the user claim in the JWT token. The user claim is a custom claim, and its content depends on how the JWT tokens are generated in your authentication system.\nGenerate JWT token Once we configured Grafana to use JWT authentication, let\u0026rsquo;s generate a JWT token using the below Python script:\nimport jwt import time import webbrowser private_key = open(\u0026#39;./grafana.key\u0026#39;, \u0026#39;r\u0026#39;).read() payload = { \u0026#34;user\u0026#34;: \u0026#34;foobar\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;foobar@example.com\u0026#34;, \u0026#34;iat\u0026#34;: int(time.time()) } token = jwt.encode(payload, private_key, algorithm=\u0026#39;RS256\u0026#39;) base_url = \u0026#39;http://localhost:3000\u0026#39; url_with_token = f\u0026#39;{base_url}?auth_token={token}\u0026#39; webbrowser.open_new_tab(url_with_token) The above script will generate a JWT token and open Grafana in a new tab with the JWT token as a query parameter. Grafana will validate the JWT token and log the user in.\nIf you go to the Admin page, you will see the user details populated from the JWT token.\nConclusion Depending on your use case, this setup can be extended to embed Grafana dashboards in your application via iframe. Explore additional options, such as using JSON Web Key Sets (JWKS) and assigning roles to users, in the Grafana documentation.\n","permalink":"https://tanmay-bhat.github.io/posts/configure-jwt-auth-grafana/","summary":"\u003cp\u003eJSON Web Tokens (JWTs) offer seamless sign-in, allowing users to carry their authentication securely across different applications within the same ecosystem. In this article, let\u0026rsquo;s go through the process of configuring JWT-based authentication with Grafana for a smoother user experience.\u003c/p\u003e\n\u003ch3 id=\"create-a-rsa-key-pair\"\u003eCreate a RSA key pair\u003c/h3\u003e\n\u003cp\u003eBegin by creating an RSA key pair. The private key will sign the JWT token, while the public key will verify it.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003essh-keygen -t rsa -b \u003cspan style=\"color:#ae81ff\"\u003e4096\u003c/span\u003e -m PEM -f grafana.key -N \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eopenssl rsa -in grafana.key -pubout -outform PEM -out grafana.key.pub\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"configure-jwt-authentication\"\u003eConfigure JWT authentication\u003c/h3\u003e\n\u003cp\u003eWith the key pair in place, configure JWT authentication in Grafana. You can use environment variables or a configuration file. Here\u0026rsquo;s an example using environment variables:\u003c/p\u003e","title":"Configure JWT Authentication with Grafana"},{"content":"You might have come across the situation where you want to restrict certain cluster or environment specific data from being queried by users, for example finance data or other business critical data and not every Grafana user should be able to see this data. This is where label-proxy comes in handy.\nLabel-proxy is a small proxy that sits between Grafana and Prometheus and restricts the label values that are being queried.\nInstallation The installation process is straightforward. Download the binary from here, and you\u0026rsquo;re ready to get started.\nRestricting Access to Specific Label Values If you want to have a separate datasource in Grafana to allow query only for a label values, we can use the below example config :\nThe scrape_job label has multiple values :\ncurl -s https://prometheus.demo.do.prometheus.io/api/v1/label/scrape_job/values | jq .data [ \u0026#34;alertmanager\u0026#34;, \u0026#34;blackbox\u0026#34;, \u0026#34;caddy\u0026#34;, \u0026#34;grafana\u0026#34;, \u0026#34;node\u0026#34;, \u0026#34;prometheus\u0026#34;, \u0026#34;random\u0026#34; ] Let\u0026rsquo;s restrict the scrape_job label to only grafana values :\n./prom-label-proxy \\ -label scrape_job \\ -label-value grafana \\ -upstream http://demo.do.prometheus.io:9090 \\ -insecure-listen-address 127.0.0.1:8080 Now, let\u0026rsquo;s try to query the metric prometheus_target_scrape_pool_sync_total which has that label and see if we can get the data :\ncurl -s \u0026#34;http://127.0.0.1:8081/api/v1/query?query=prometheus_target_scrape_pool_sync_total\u0026#34; | jq \u0026#39;.data.result[]\u0026#39; { \u0026#34;metric\u0026#34;: { \u0026#34;__name__\u0026#34;: \u0026#34;prometheus_target_scrape_pool_sync_total\u0026#34;, \u0026#34;instance\u0026#34;: \u0026#34;demo.do.prometheus.io:9090\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;scrape_job\u0026#34;: \u0026#34;grafana\u0026#34; }, \u0026#34;value\u0026#34;: [ 1703860408.019, \u0026#34;4614\u0026#34; ] } As you can see, we are able to query the data for only grafana scrape_job label value and all other values are restricted.\nWe can also specify multiple label values to restrict, for example if we want to allow only grafana and caddy scrape_job label values, we can use the below config :\n./prom-label-proxy \\ -label scrape_job \\ -label-value grafana \\ -label-value caddy \\ -upstream http://demo.do.prometheus.io:9090 \\ -insecure-listen-address 127.0.0.1:8080 If we try to query the same metric again, we should be able to see only grafana and caddy scrape_job label values :\ncurl -s \u0026#34;http://127.0.0.1:8081/api/v1/query?query=prometheus_target_scrape_pool_sync_total\u0026#34; | jq \u0026#39;.data.result[].metric | {__name__, scrape_job}\u0026#39; { \u0026#34;__name__\u0026#34;: \u0026#34;prometheus_target_scrape_pool_sync_total\u0026#34;, \u0026#34;scrape_job\u0026#34;: \u0026#34;caddy\u0026#34; } { \u0026#34;__name__\u0026#34;: \u0026#34;prometheus_target_scrape_pool_sync_total\u0026#34;, \u0026#34;scrape_job\u0026#34;: \u0026#34;grafana\u0026#34; } Enforcing Label Values for All Queries We can also enforce a specific label value for all queries, for example if we want to enforce scrape_job label value for all queries, we can use the below config :\n./prom-label-proxy \\ -label scrape_job \\ -upstream http://demo.do.prometheus.io:9090 \\ -insecure-listen-address 127.0.0.1:8080 What this does is, it checks for all incoming queries and if the query has scrape_job label, it will add the label value to the query and forward it to Prometheus. If the query doesn\u0026rsquo;t have scrape_job label, it will return an error.\ncurl \u0026#34;http://127.0.0.1:8081/api/v1/query?query=node_memory_MemAvailable_bytes\u0026#34; {\u0026#34;error\u0026#34;:\u0026#34;The \\\u0026#34;scrape_job\\\u0026#34; query parameter must be provided.\u0026#34;,\u0026#34;errorType\u0026#34;:\u0026#34;prom-label-proxy\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;error\u0026#34;} Use Cases If you have a central Prometheus server where data is not stored in native multi-tenant mode (Thanos or Cortex) and you want to restrict certain data from being queried by different team users like financial data or namespace specific data. You are a multi tenant SaaS provider and want to expose certain internal Prometheus metrics via Grafana iframe to your customers via your portal but want to restrict them to query only specific metric and for their own tenant. ","permalink":"https://tanmay-bhat.github.io/posts/restrict-label-values-prometheus/","summary":"\u003cp\u003eYou might have come across the situation where you want to restrict certain cluster or environment specific data from being queried by users, for example finance data or other business critical data and not every Grafana user should be able to see this data. This is where label-proxy comes in handy.\u003c/p\u003e\n\u003cp\u003eLabel-proxy is a small proxy that sits between Grafana and Prometheus and restricts the label values that are being queried.\u003c/p\u003e","title":"Restricting Label Values in Prometheus via prom-label-proxy"},{"content":"ZSH functions are a great way to automate tasks and make your life easier. I’ve been using ZSH for a while now, and I recently started using functions to automate tasks that I do on a daily basis.\nEvery day, I routinely switch between various AWS profiles, depending on the account and the role that I’m working on. So I wanted to make it easier to switch between profiles.\nIn this blog, we’ll see how we can use ZSH functions to switch between AWS profiles. Along with customizing the prompt to show the current AWS profile that we’re using so that we are sure on the account before deleting a database.\nPre-requisites:\nOhMyZsh (If you’re not using OhMyZsh, you should :P) AWS CLI Configure AWS CLI with different profiles First, we need to make sure that we have configured the AWS CLI with the profiles that we want to use. We can add as many profiles into ~/.aws/config file. An example of the config file is shown below :\n[sso-session my-org-sso] sso_start_url = https://my-org-aws-sso.awsapps.com/start sso_region = us-east-1 sso_registration_scopes = sso:account:access [profile staging-admin] sso_session = my-org-sso sso_account_id = 123456789012 sso_role_name = Admin [profile staging-observer] sso_session = my-org-sso sso_account_id = 123456789012 sso_role_name = Observer The above example is for AWS SSO. If you’re using AWS CLI with IAM credentials (I suggest not to use static IAM credentials), then you need to first configure the credential in ~/.aws/credentials file like below :\n[staging-admin] aws_access_key_id=AKIAIOSFODNN7EXAMPLE aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY [staging-observer] aws_access_key_id=AKIAI44QH8DHBEXAMPLE aws_secret_access_key=je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY And then configure the profile in ~/.aws/config file :\n[profile staging-admin] region = us-west-2 [profile staging-observer] region = us-west-2 A detailed guide on how to configure AWS CLI with different profiles can be found here.\nZSH Functions with OhMyZsh In OhMyZsh, we can create a custom config inside $ZSH_CUSTOM. Any *.zsh file inside $ZSH_CUSTOM will be loaded automatically.\nLet\u0026rsquo;s create a file called $ZSH_CUSTOM/aws.zsh folder and add the below code :\n# function to list all the profiles in ~/.aws/config function aws_profiles() { profiles=$(aws --no-cli-pager configure list-profiles 2\u0026gt; /dev/null) if [[ -z \u0026#34;$profiles\u0026#34; ]]; then echo \u0026#34;No AWS profiles found in \u0026#39;$HOME/.aws/config, check if ~/.aws/config exists and properly configured.\u0026#39;\u0026#34; return 1 else echo $profiles fi } # function to set AWS profile, sso login and clear the profile function asp() { available_profiles=$(aws_profiles) if [[ -z \u0026#34;$1\u0026#34; ]]; then unset AWS_DEFAULT_PROFILE AWS_PROFILE echo \u0026#34;Zero argument provided, AWS profile cleared.\u0026#34; return fi echo \u0026#34;$available_profiles\u0026#34; | grep -qw \u0026#34;$1\u0026#34; if [[ $? -ne 0 ]]; then echo \u0026#34;Profile \u0026#39;$1\u0026#39; not configured in \u0026#39;$HOME/.aws/config\u0026#39;.\\n\u0026#34; echo \u0026#34;Available profiles: \\n$available_profiles\\n\u0026#34; return 1 else export AWS_DEFAULT_PROFILE=\u0026#34;$1\u0026#34; AWS_PROFILE=\u0026#34;$1\u0026#34; fi } # function to set AWS region and clear the region function asr() { if [[ -z \u0026#34;$1\u0026#34; ]]; then unset AWS_DEFAULT_REGION AWS_REGION echo \u0026#34;No argument provided, cleared AWS region.\u0026#34; return else export AWS_DEFAULT_REGION=$1 AWS_REGION=$1 fi } # function to list all the profiles function alp() { aws_profiles } # function to update the PS1 prompt with current AWS profile and region function aws_ps1() { local profile_color=\u0026#34;%{$(tput setaf 6)%}\u0026#34; # Cyan color local region_color=\u0026#34;%{$(tput setaf 2)%}\u0026#34; # Green color local reset_color=\u0026#34;%{$(tput sgr0)%}\u0026#34; # Reset color echo -en \u0026#34;($profile_color$AWS_PROFILE$reset_color:$region_color$AWS_REGION$reset_color)\u0026#34; } Now, In order to use the aws_ps1() to change the prompt, we need to add the below code ~$HOME/.zshrc file :\nPS1=\u0026#39;$(aws_ps1)\u0026#39;$PS1 I usually prefer to have a default profile and region set when I open a new terminal. So, I’ll add the below code in my $HOME/.zshrc file :\n# set prod observer profile and us-west-2 as default asp prod-observer asr us-west-2 Once you save both the files and open a new terminal, you should see the below in your prompt of new terminal :\n(prod-observer:us-west-2) ➜ ~ Let\u0026rsquo;s list all the profiles :\n(prod-observer:us-west-2) ➜ ~ alp staging-admin staging-observer Switch to staging-admin profile :\n(prod-observer:us-west-2) ➜ ~ asp staging-admin (staging-admin:us-west-2) ➜ ~ Similarly, when the unknown profile is set:\n(staging-admin:us-west-2) ➜ ~ asp foobar Profile \u0026#39;foobar\u0026#39; not configured in \u0026#39;/Users/tanmay_bhat/.aws/config\u0026#39;. Available profiles: staging-admin staging-observer (staging-admin:us-west-2) ➜ ~ Using this, you don\u0026rsquo;t need to give --profile and --region flags every time you run an AWS CLI command and no need to run aws sts get-caller-identity to check the current profile that you’re using before running a destructive command.\n","permalink":"https://tanmay-bhat.github.io/posts/multiple-aws-profile-zsh/","summary":"\u003cp\u003eZSH functions are a great way to automate tasks and make your life easier. I’ve been using ZSH for a while now, and I recently started using functions to automate tasks that I do on a daily basis.\u003c/p\u003e\n\u003cp\u003eEvery day, I routinely switch between various AWS profiles, depending on the account and the role that I’m working on. So I wanted to make it easier to switch between profiles.\u003c/p\u003e\n\u003cp\u003eIn this blog, we’ll see how we can use ZSH functions to switch between AWS profiles. Along with customizing the prompt to show the current AWS profile that we’re using so that we are sure on the account before deleting a database.\u003c/p\u003e","title":"Switch Between Multiple AWS Profiles In Terminal Using ZSH Functions"},{"content":"Starting from v1.1, Terraform provides a powerful feature known as the moved block. This feature allows you to reorganize your Terraform configuration without causing Terraform to perceive the refactor as a deletion and creation of resources.\nIn this article, we will walk through a few examples of Terraform refactoring using the moved block.\nPrerequisites Terraform (\u0026gt;=1.1) Move a Resource Into Module First, we will create a sample S3 bucket to reference as a standalone resource. In your Terraform configuration directory, create a new ./lab-demo/s3.tf file.\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;moved_demo\u0026#34; { bucket = \u0026#34;moved-demo\u0026#34; tags = { env = \u0026#34;lab\u0026#34; } } Now, let\u0026rsquo;s create a module directory ./modules/aws/s3 and move the ./lab-demo/s3.tf into ./modules/aws/. The directory structure should look like this:\n├── lab-demo │ ├── main.tf │ ├── s3.tf # Moved to ./modules/aws/s3/s3.tf │ ├── modules │ └── aws │ └── s3 │ └── s3.tf Since we have the resource in the module, we need to update the reference in the ./lab-demo/main.tf file.\nmodule \u0026#34;s3\u0026#34; { source = \u0026#34;./modules/aws/s3\u0026#34; } If we run terraform init \u0026amp;\u0026amp; terraform plan now, Terraform will propose to destroy and recreate the same S3 bucket. This is because Terraform thinks that the object reference does not exist anymore.\n# aws_s3_bucket.moved_demo will be destroyed # (because aws_s3_bucket.moved_demo is not in configuration) ... ... # module.s3.aws_s3_bucket.moved_demo will be created To fix this, we need to add a moved block to the ./lab-demo/main.tf file.\nmodule \u0026#34;s3\u0026#34; { source = \u0026#34;./modules/aws/s3\u0026#34; } moved { from = aws_s3_bucket.moved_demo to = module.s3.aws_s3_bucket.moved_demo } If you do the terraform plan again, Terraform will recognize the move and not propose to destroy and recreate the resource.\n# aws_s3_bucket.moved_demo has moved to module.s3.aws_s3_bucket.moved_demo resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;moved_demo\u0026#34; { id = \u0026#34;moved-demo\u0026#34; tags = { \u0026#34;env\u0026#34; = \u0026#34;lab\u0026#34; } } Plan: 0 to add, 0 to change, 0 to destroy. Once you apply the configuration, you can remove the moved block from the ./lab-demo/main.tf file.\nMoving Resource Between Modules The moved block can also be used to move resources between modules. Let\u0026rsquo;s create a new module ./modules/aws/another_module and move the ./modules/aws/s3/s3.tf into ./modules/aws/another_module. The directory structure should look like this:\n├── lab-demo │ ├── main.tf │ ├── modules │ └── aws │ ├── s3 │ └── another_module │ └── s3.tf # Moved from ./modules/aws/s3/s3.tf Since we have the resource in the module, we need to update the reference in the ./lab-demo/main.tf file.\nmodule \u0026#34;s3\u0026#34; { source = \u0026#34;./modules/aws/another_module\u0026#34; } In order to move the resource from s3 module another_module , we can use the same moved block to the ./lab-demo/main.tf file.\nmodule \u0026#34;s3\u0026#34; { source = \u0026#34;./modules/aws/s3\u0026#34; } moved { from = module.s3.aws_s3_bucket.moved_demo to = module.another_module.aws_s3_bucket.moved_demo } And the terraform plan will show the resource move.\n# module.s3.aws_s3_bucket.moved_demo has moved to module.another_module.aws_s3_bucket.moved_demo resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { id = \u0026#34;moved-demo\u0026#34; tags = { \u0026#34;env\u0026#34; = \u0026#34;lab\u0026#34; } } Plan: 0 to add, 0 to change, 0 to destroy. Renaming a Module Similarly, we can also rename the module using the moved block. Let\u0026rsquo;s rename the another_module to s3 and move the resource back to the s3 module. Below is an example main.tf file.\n## Before: main.tf module \u0026#34;s3\u0026#34; { source = \u0026#34;./modules/aws/s3\u0026#34; } ## After: main.tf module \u0026#34;s3_renamed_module\u0026#34; { source = \u0026#34;./modules/aws/s3\u0026#34; } moved { from = module.s3 to = module.s3_renamed_module } In this case, all the resources in the s3 module will be moved to the s3_renamed_module module.\nResources : https://developer.hashicorp.com/terraform/language/v1.1.x/modules/develop/refactoring\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-move-a-terraform-resource-into-a-module-using-moved-block/","summary":"\u003cp\u003eStarting from v1.1, Terraform provides a powerful feature known as the \u003ccode\u003emoved\u003c/code\u003e block. This feature allows you to reorganize your Terraform configuration without causing Terraform to perceive the refactor as a deletion and creation of resources.\u003c/p\u003e\n\u003cp\u003eIn this article, we will walk through a few examples of Terraform refactoring using the \u003ccode\u003emoved\u003c/code\u003e block.\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTerraform (\u0026gt;=1.1)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"move-a-resource-into-module\"\u003eMove a Resource Into Module\u003c/h2\u003e\n\u003cp\u003eFirst, we will create a sample S3 bucket to reference as a standalone resource. In your Terraform configuration directory, create a new \u003ccode\u003e./lab-demo/s3.tf\u003c/code\u003e file.\u003c/p\u003e","title":"How to Move a Terraform Resource Into a Module Using Moved Block"},{"content":"Fluentd is a powerful log collection and processing tool. In this blog post, I will walk you through the process of adding custom label and key to the log records, so that you can better understand your logs and filter them more effectively.\nFor example, you might want to add a custom label to your records to indicate the environment in which the log was generated, or the cluster name. This would allow you to filter your logs by environment or cluster name in Elasticsearch or another storage destination.\nAdding a custom label or key into the records Say you have a Nginx service which is sending logs to fluentd, and you want to add a custom label i.e., cluster name into the records.\nFor this, I\u0026rsquo;m using the below docker-compose file to spin up a nginx service and a fluentd service :\nservices: fake-logger: container_name: flog image: tanmaybhat/flog-multiarch command: -d 1 logging: driver: \u0026#34;fluentd\u0026#34; options: fluentd-address: localhost:24224 tag: nginx fluentd: container_name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1.16.2-debian-elasticsearch8-1.0 volumes: - ./fluentd/conf:/fluentd/etc ports: - \u0026#34;24224:24224\u0026#34; - \u0026#34;24224:24224/udp\u0026#34; Here\u0026rsquo;s what the docker-compose file is doing:\nSpinning up a logger service which sends fake nginx logs to stdout one line at a time. We are using fluentd as the logging driver for the logger service and add the tag as nginx to easily identify the logs later. We are running a fluentd service which is using the Docker image which has the elasticsearch plugin installed. (Optional) Finally we are mounting the fluentd configuration file from the host to the container. Now, lets see how the fluentd configuration file ./fluentd/conf/fluent.conf looks like:\n\u0026lt;source\u0026gt; @type forward port 24224 bind 0.0.0.0 \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type stdout \u0026lt;/match\u0026gt; In the above configuration file, we are using the forward plugin to receive the logs from the Docker logger service and then using the stdout plugin to print the logs to the stdout.\nLet\u0026rsquo;s run both the services using the below command:\ndocker-compose up If you observe the logs this is how its going to look like:\nflog | 217.32.48.100 - [30/Jul/2023:18:05:00] \u0026#34;GET /synergize/platforms HTTP/1.0\u0026#34; 200 14231 fluentd | 2023-07-30 18:05:01 nginx: {\u0026#34;container_name\u0026#34;:\u0026#34;flog\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;stdout\u0026#34;,\u0026#34;log\u0026#34;:\u0026#34;217.32.48.100 - [30/Jul/2023:18:05:00 +0000] \\\u0026#34;GET /synergize/platforms HTTP/1.0\\\u0026#34; 200 14231\u0026#34;,\u0026#34;container_id\u0026#34;:\u0026#34;xxx\u0026#34;} We can now add a custom label or key into the records using the `record_transformer`` plugin. Let\u0026rsquo;s see how the configuration file looks like:\n\u0026lt;filter **\u0026gt; @type record_transformer enable_ruby true auto_typecast true renew_record true \u0026lt;record\u0026gt; cluster \u0026#34;lab_cluster\u0026#34; \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; Once the configuration file is updated, the logs will now look like this:\nflog | 91.143.216.150 - walker6126 [30/Jul/2023:18:13:26 +0000] \u0026#34;PATCH /relationships HTTP/1.0\u0026#34; 200 8286 fluentd | 2023-07-30 18:13:27.000000000 +0000 nginx: {\u0026#34;cluster\u0026#34;:\u0026#34;lab_cluster\u0026#34;} Interesting right? fluentd is now adding an extra key called cluster into the records. When you use the record_transformer plugin and add an extra key, Anything you add in \u0026lt;record\u0026gt; will be replaced as the actual record.\nIn our case, the whole record is replaced with the key cluster. But this is not what we want. We\u0026rsquo;d like the cluster name to be added to the existing log records.\nIf you observe the previous output from fluentd, the logs are inside a key called log. So we need to add the cluster name, keep the log as it is along with the tag. Let\u0026rsquo;s see how we can do that:\n\u0026lt;record\u0026gt; cluster \u0026#34;lab_cluster\u0026#34; tag ${tag} log ${record[\u0026#34;log\u0026#34;]} \u0026lt;/record\u0026gt; Here, we are setting log key as the value of the log key from the record. record[\u0026quot;\u0026lt;key\u0026gt;\u0026quot;] is used to get the value of the key from the record.\nAny key which is present in the original record can be extracted and be used to add a new key into the record or manipulate the record.\nflog | 154.255.224.80 - - [30/Jul/2023:18:31:45] \u0026#34;POST /back-end/efficient HTTP/2.0\u0026#34; 405 6598 fluentd | 2023-07-30 18:31:46 nginx: {\u0026#34;cluster\u0026#34;:\u0026#34;lab_cluster\u0026#34;,\u0026#34;tag\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;log\u0026#34;:\u0026#34;154.255.224.80 - - [30/Jul/2023:18:31:45 +0000] \\\u0026#34;POST /back-end/efficient HTTP/2.0\\\u0026#34; 405 6598\u0026#34;} And here\u0026rsquo;s how it looks in Elasticsearch:\nMerging the custom label or key with the existing record key I stumbled upon a unique use case where I had to merge the custom label, say cluster into existing record key of text. This was needed as we were sending the logs to Coralogix which uses OpenSearch as the backend and any additional key other than text will be ignored.\nIn this case, I used the merge function of ruby to merge the custom label with the existing record key. Let\u0026rsquo;s see how the configuration file looks like :\n\u0026lt;record\u0026gt; text ${record.merge(\u0026#39;cluster\u0026#39; =\u0026gt; \u0026#39;lab_cluster\u0026#39;)} \u0026lt;/record\u0026gt; In production, I suggest to set keys like cluster as environment variables and use them in the configuration file instead of hard-coding them.\n\u0026lt;record\u0026gt; text ${record.merge(\u0026#39;cluster\u0026#39; =\u0026gt; ENV[\u0026#39;CLUSTER\u0026#39;])} \u0026lt;/record\u0026gt; Resources: https://docs.fluentd.org/filter/record_transformer\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-add-custom-labels-or-keys-to-records-in-fluentd/","summary":"\u003cp\u003eFluentd is a powerful log collection and processing tool. In this blog post, I will walk you through the process of adding custom label and key to the log records, so that you can better understand your logs and filter them more effectively.\u003c/p\u003e\n\u003cp\u003eFor example, you might want to add a custom label to your records to indicate the environment in which the log was generated, or the cluster name. This would allow you to filter your logs by environment or cluster name in Elasticsearch or another storage destination.\u003c/p\u003e","title":"How to Add Custom Label or Key to Records in Fluentd"},{"content":"Introduction Syslog logging is a widely used method for collecting and storing log data. It is a standard that is supported by many applications and platforms. In this blog post, we will take a look at the basics of syslog parsing with Fluentd.\nWhat Is Syslog? Syslog is a protocol used for collecting log data from various sources. It is usually used to collect log data from network devices such as routers and switches, as well as from OS and applications. The log data is then stored on a central syslog server.\nUsing Fluentd for Syslog Logging Fluentd is an open-source data collector that can be used to collect and store syslog data. Fluentd can be configured to filter and parse the log data, making it easier to analyze and process. It can also be used to forward log data to other systems, such as a SIEM or an analytics platform.\nConfigure Rsyslog forward with Fluentd Edit the /etc/rsyslog.conf file and update it to forward logs to fluentd: # Old format to send log messages to Fluentd #*.* @127.0.0.1:5140 # New format to send log messages to Fluentd *.* action(type=\u0026#34;omfwd\u0026#34; target=\u0026#34;127.0.0.1\u0026#34; port=\u0026#34;5140\u0026#34; protocol=\u0026#34;tcp\u0026#34;) Restart the rsyslog service: systemctl restart rsyslog Configure Fluentd for syslog input If you don\u0026rsquo;t have fluentd(td-agent) installed, you can install it using the following command:\n# td-agent 4 curl -fsSL https://toolbelt.treasuredata.com/sh/install-ubuntu-focal-td-agent4.sh | sh You can refer to the following link for more details on how to install fluentd on different platforms: https://docs.fluentd.org/installation\nEdit the /etc/td-agent/td-agent.conf file and add the following configuration: \u0026lt;system\u0026gt; log_level error \u0026lt;/system\u0026gt; \u0026lt;source\u0026gt; @type syslog port 5140 bind 0.0.0.0 tag syslog \u0026lt;transport tcp\u0026gt; \u0026lt;/transport\u0026gt; \u0026lt;parse\u0026gt; @type syslog with_priority true message_format rfc3164 \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match syslog.**\u0026gt; type stdout \u0026lt;/match\u0026gt; Let\u0026rsquo;s go through the configuration file and understand what each section does:\n\u0026lt;system\u0026gt; : sets the log level to error. This will prevent Fluentd from logging too much information into its log file.\n\u0026lt;source\u0026gt; : configures the syslog input plugin.\nThe port parameter is used to set the port that Fluentd will listen on for syslog messages. The default is 5140. The bind is the interface on which fluentd listens for incoming syslog messages. The \u0026lt;transport\u0026gt; section is used to configure the protocol that Fluentd will use to receive the log data. Default is UDP. I\u0026rsquo;ve specified TCP since it ensures message delivery.\n\u0026lt;parse\u0026gt; is the configuration block the Fluentd will use to parse the incoming syslog messages. The tag parameter is used to set the tag for the log data.\n@type syslog : specifies the parser to use. In this case, we are using the default syslog parser. with_priority : specifies whether the syslog message contains a priority field. message_format : specifies the format of the syslog message. The default is rfc3164. I\u0026rsquo;ve specified rfc5424 since it is the newer syslog format. \u0026lt;match\u0026gt; : configures the stdout plugin. This plugin will print the parsed log data to the stdout. This is useful for testing / debugging purposes.\nRestart the td-agent service: systemctl restart td-agent Validate that Fluentd is listening on port 5140: netstat -tulpn | grep 5140 Parsing rfc5424 syslog with Fluentd To test the syslog ingestion with Fluentd, I\u0026rsquo;m using a tool called flog which will generate syslog messages in both rfc3164 and rfc5424 formats. You can download the binary from the following link: https://github.com/mingrammer/flog/releases\nGenerate syslog rfc5424 messages using flog: flog -f rfc5424 -l -n 1 -d 1 This will generate the rfc5424 format syslog messages and print them to the stdout. The -n parameter specifies the number of messages to generate. The -d parameter specifies the delay between each message.\nExample output:\n\u0026lt;100\u0026gt;2 2023-02-11T09:09:19.003Z customermesh.org cum 1023 ID922 - The SAS sensor is down, reboot the wireless hard drive so we can copy the IB port! \u0026lt;9\u0026gt;1 2023-02-11T09:09:19.003Z dynamicconvergence.io facere 9279 ID665 - You can\u0026#39;t copy the protocol without indexing the wireless RAM program Since fluentd is listening on port 5140, we can use netcat to send the generated syslog messages to fluentd: sudo flog -f rfc5424 -l -n 1 -d 1 | nc localhost 5140 We can check if the syslog messages are being received at port 5140 using tcpdump: sudo tcpdump -i any -A dst port 5140 09:16:46.596202 IP localhost.16564 \u0026gt; localhost.5140: Flags [P.], seq 163:309, ack 1, win 128, options [nop,nop,TS val 4039646268 ecr 4039645268], length 146 09:16:47.596379 IP localhost.16564 \u0026gt; localhost.5140: Flags [P.], seq 309:460, ack 1, win 128, options [nop,nop,TS val 4039647268 ecr 4039646268], length 151 ........\u0026lt;20\u0026gt;1 2023-02-11T09:19:32.541Z dynamice-business.com iste 7111 ID497 - You can\u0026#39;t copy the protocol without indexing the wireless RAM program Lets verify if the syslog messages are being received by Fluentd from its logs: tail -f /var/log/td-agent/td-agent.log #output 2023-02-11 09:12:32.541000000 +0000 syslog.user.crit: {\u0026#34;host\u0026#34;:\u0026#34;dynamicconvergence.io\u0026#34;,\u0026#34;ident\u0026#34;:\u0026#34;corporis\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;7866\u0026#34;,\u0026#34;msgid\u0026#34;:\u0026#34;ID616\u0026#34;,\u0026#34;extradata\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34; You can\u0026#39;t copy the protocol without indexing the wireless RAM program\u0026#34;} 2023-02-11 09:12:33.541000000 +0000 syslog.local2.notice: {\u0026#34;host\u0026#34;:\u0026#34;chiefb2b.org\u0026#34;,\u0026#34;ident\u0026#34;:\u0026#34;eum\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;1867\u0026#34;,\u0026#34;msgid\u0026#34;:\u0026#34;ID551\u0026#34;,\u0026#34;extradata\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;You can\u0026#39;t compress the array without backing up the bluetooth ADP card!\u0026#34;} The above are the syslog messages that are parsed by Fluentd. That\u0026rsquo;s why you can see the messages are segregated into multiple fields like host, host etc that will help in readability, complex search queries and faster indexing.\nParsing rfc3164 syslog with Fluentd : Generate syslog rfc3164 messages using flog: flog -f rfc3164 -l -n 1 -d 1 Update the Fluentd configuration file to parse the rfc3164 syslog messages and restart the td-agent service: \u0026lt;parse\u0026gt; @type syslog with_priority true message_format rfc3164 \u0026lt;/parse\u0026gt; Validate that the syslog messages are being parsed by Fluentd: tail -f /var/log/td-agent/td-agent.log #output 2023-02-11 10:50:08.000000000 +0000 syslog.local7.crit: {\u0026#34;host\u0026#34;:\u0026#34;gulgowski7481\u0026#34;,\u0026#34;ident\u0026#34;:\u0026#34;qui\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;6901\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;Try to program the EXE panel, maybe it will connect the cross-platform system!\u0026#34;} 2023-02-11 10:50:09.000000000 +0000 syslog.local3.crit: {\u0026#34;host\u0026#34;:\u0026#34;mertz1456\u0026#34;,\u0026#34;ident\u0026#34;:\u0026#34;omnis\u0026#34;,\u0026#34;pid\u0026#34;:\u0026#34;2294\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;If we program the sensor, we can get to the COM panel through the cross-platform SDD application!\u0026#34;} Parsing custom format syslog with Fluentd : Let\u0026rsquo;s say we have a custom format syslog message that we want to parse with Fluentd. For example, the following is a custom format syslog message:\n\u0026lt;83\u0026gt;Feb 11 11:26:19 steuber4240 compressing the back-end SSL firewall The above log sample doesn\u0026rsquo;t have ident and msgid fields. We can use Fluentd\u0026rsquo;s regex parser to parse the custom format syslog messages.\nA thing to note when it comes to parsing custom format syslog messages is that it expects the incoming logs to have priority field by default, if your log doesn\u0026rsquo;t have a priority field, you can disable it by setting with_priority to false in the Fluentd configuration, or we can also use tcp input plugin as well since it\u0026rsquo;s almost identical to syslog input plugin.\nThe following is the Fluentd configuration that will parse the custom format syslog messages:\n\u0026lt;source\u0026gt; @type tcp port 5140 bind 0.0.0.0 @log_level trace \u0026lt;transport tcp\u0026gt; \u0026lt;/transport\u0026gt; \u0026lt;parse\u0026gt; @type regexp expression /^\\\u0026lt;(?\u0026lt;pri\u0026gt;[0-9]+)\\\u0026gt;(?\u0026lt;time\u0026gt;[^ ]* {1,2}[^ ]* [^ ]*) (?\u0026lt;host\u0026gt;[^ ]*) *(?\u0026lt;message\u0026gt;.*)$/ time_format %b %d %H:%M:%S \u0026lt;/parse\u0026gt; tag tcp.events \u0026lt;/source\u0026gt; Let\u0026rsquo;s check if the custom format syslog messages are being parsed by Fluentd:\ntail -f /var/log/td-agent/td-agent.log #output 2023-02-11 11:26:19.000000000 +0530 tcp.events: {\u0026#34;pri\u0026#34;:\u0026#34;83\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;steuber4240\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;compressing the back-end SSL firewall\u0026#34;} Parsing both rfc5424 and rfc3164 syslog with Fluentd : If you\u0026rsquo;re receiving both rfc5424 and rfc3164 syslog messages, then you can use message_format:autoin Fluentd configuration to parse both formats:\n\u0026lt;parse\u0026gt; @type syslog with_priority true message_format auto \u0026lt;/parse\u0026gt; Resources : Fluentd syslog input plugin : https://docs.fluentd.org/input/syslog\nRuby regex validator : https://rubular.com/\n","permalink":"https://tanmay-bhat.github.io/posts/syslog-parsing-with-fluentd/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSyslog logging is a widely used method for collecting and storing log data. It is a standard that is supported by many applications and platforms. In this blog post, we will take a look at the basics of syslog parsing with Fluentd.\u003c/p\u003e\n\u003ch2 id=\"what-is-syslog\"\u003eWhat Is Syslog?\u003c/h2\u003e\n\u003cp\u003eSyslog is a protocol used for collecting log data from various sources. It is usually used to collect log data from network devices such as routers and switches, as well as from OS and applications. The log data is then stored on a central syslog server.\u003c/p\u003e","title":"An Overview of Syslog Parsing with Fluentd"},{"content":"In this blog post, we will be discussing how to set up monitoring and alerting for Nginx ingress in a Kubernetes environment.\nWe will cover the installation and configuration of ingress-nginx, Prometheus and Grafana, and the setup of alerts for key Ingress metrics.\nPre-requisites : A Kubernetes cluster Helm v3 Install Prometheus and Grafana In this step, we will install Prometheus to collect metrics, and Grafana to visualize and create alerts based on those metrics.\nLet\u0026rsquo;s install the kube-prometheus-stack helm chart by copying the below-mentioned commands to your terminal. This will setup Grafana, Prometheus and other monitoring components.\n# Add and update the prometheus-community helm repository. helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update cat \u0026lt;\u0026lt;EOF | helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \\ --create-namespace -n monitoring -f - grafana: enabled: true adminPassword: \u0026#34;admin\u0026#34; persistence: enabled: true accessModes: [\u0026#34;ReadWriteOnce\u0026#34;] size: 1Gi ingress: enabled: true ingressClassName: nginx hosts: - grafana.localdev.me EOF Let\u0026rsquo;s see if the installed components are up and running :\nkubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE kube-prometheus-stack-grafana-7bb55544c9-qwkrg 3/3 Running 0 3m38s prometheus-kube-prometheus-stack-prometheus-0 2/2 Running 0 3m14s ... Looks great, let\u0026rsquo;s proceed to the next section.\nInstall \u0026amp; Configure Ingress Nginx In this step, we will install and configure Nginx ingress controller and enable metrics that can be scraped by Prometheus.\nLet\u0026rsquo;s install ingress-nginx into our cluster using below command: helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace \\ --set controller.metrics.enabled=true \\ --set controller.metrics.serviceMonitor.enabled=true \\ --set controller.metrics.serviceMonitor.additionalLabels.release=\u0026#34;kube-prometheus-stack\u0026#34; Here we\u0026rsquo;re specifying serviceMonitor.additionalLabels to be release: kube-prometheus-stack so that Prometheus can discover the service monitor and automatically scrape metrics from it.\nOnce the chart is installed, let’s deploy a sample app podinfo into default namespace. helm install --wait podinfo --namespace default \\ oci://ghcr.io/stefanprodan/charts/podinfo Now, create an ingress for the deployed podinfo deployment : cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: podinfo-ingress spec: ingressClassName: nginx rules : - host: podinfo.localdev.me defaultBackend: service: name: podinfo port: number: 9898 EOF Let’s understand a bit about the above ingress config :\nWe’re using ingress-nginx as our ingress controller, hence the ingress class is defined as nginx. In the above config, I’ve used the host address for my Ingress as podinfo.localdev.me. The DNS *.localdev.me resolves to 127.0.0.1, hence for any local testing this DNS can be used without the hassle of adding an entry into /etc/hosts file. Podinfo app serves HTTP API over port 9898, hence it\u0026rsquo;s specified for the backend port i.e. when the traffic arrives for the domain http://podinfo.localdev.me, it will be forwarded to 9898 of podinfo service. Next, from your terminal, port-forward the ingress-nginx service so that you can send traffic from your local terminal. kubectl port-forward -n ingress-nginx service/ingress-nginx-controller 8080:80 \u0026gt; /dev/null \u0026amp; Port 80 on the host is a privileged port, so we’re not using that, instead we’re binding port 80 of nginx service to 8080 of host machine. You can specify any valid port of your choice. Note: If you’re running this in any cloud, port-forwarding is not required as LoadBalancer for ingress-nginx service will be auto-created since the service type is defined as LoadBalancer by default.\nNow, you can run the below curl request to the podinfo endpoint, which should respond with : \u0026gt; curl http://podinfo.localdev.me:8080 \u0026#34;hostname\u0026#34;: \u0026#34;podinfo-59cd496d88-8dcsx\u0026#34; \u0026#34;message\u0026#34;: \u0026#34;greetings from podinfo v6.2.2\u0026#34; You can also get the prettier look in the browser with URL : http://podinfo.localdev.me:8080/ Configure Grafana Dashboards for Ingress Nginx Monitoring To access Grafana, you can open the below URL in your browser with the credential admin:admin : http://grafana.localdev.me:8080/.\nCopy thenginx.json from here and paste it into http://grafana.localdev.me:8080/dashboard/import to import the dashboard.\nOnce imported the dashboard should look like this :\nAlerting over SLI metrics Now that we have the dashboard and metrics ready in our Grafana, it’s time to set alerting on important SLI like Error Rate \u0026amp; Latency.\nGenerate sample loads Inorder to get traffic on our my podinfo application, we’ll be using vegeta as a loadtesting tool. Please install it from here.\nLet\u0026rsquo;s generate a sample HTTP 4xx traffic. To do that, you can run the below command which will run at a request rate of 10 RPS for 10 minutes.\necho \u0026#34;GET http://podinfo.localdev.me:8080/status/400\u0026#34; | vegeta attack -duration=10m -rate=10/s You can just change the status code from 400 to 500 and run as well for test 5xx throughput. For latency tests, I’ve used the below command as GET /delay/{seconds} waits for the specified period :\necho \u0026#34;GET http://podinfo.localdev.me:8080/delay/3\u0026#34; | vegeta attack -duration=10m -rate=100/s Note: You can read more on the endpoints available in podinfo app from here.\nGrafana Alerting The newer Grafana comes with its own alerting engine. That helps in keeping all config, rules and even firing alertsin one place. Let\u0026rsquo;s configure alerts for common SLI.\n4xx Error Rate Let\u0026rsquo;s create an alert by going to http://grafana.localdev.me:8080/alerting/new We can use the following formula to get 4xx error rate percentage : (total number of 4xx requests / total number of requests) * 100\nAdd the below expression for the query : (sum(rate(nginx_ingress_controller_requests{status=~\u0026#39;4..\u0026#39;}[1m])) by (ingress) / sum(rate(nginx_ingress_controller_requests[1m])) by (ingress)) * 100 \u0026gt; 5 In Expression B, use Reduce operation with the function Mean for input A. In Alert Details, Name the alert as per your liking, I’ve named it Ingress_Nginx_4xx. For Summary, we can keep it as short as possible, by just displaying the Ingress name with label {{ $labels.ingress }}. Ingress High Error Rate : 4xx on *{{ $labels.ingress }}* For Description, I’ve used printf \u0026quot;%0.2f\u0026quot; to display upto two decimals on the percentage value. 4xx : High Error rate : `{{ printf \u0026#34;%0.2f\u0026#34; $values.B.Value }}%` on *{{ $labels.ingress }}*. Overall alert should look similar to the below snapshot : In the end, you can add a custom label like severity : critical. 5xx Error Rate Similar to 4xx alert config, 5xx error rate can also be queried with the below query :\nsum(rate(nginx_ingress_controller_requests{status=~\u0026#39;5..\u0026#39;}[1m])) by (ingress,cluster) / sum(rate(nginx_ingress_controller_requests[1m]))by (ingress) * 100 \u0026gt; 5 Note: I’ve configured the alert to be triggered then the 5xx/4xx percentage is \u0026gt; 5%. You can set it as per your error budget.\nHigh Latency (p95) To calculate the 95th percentile of request durations over the last 15m we can use the nginx_ingress_controller_request_duration_seconds_bucket metric.\nIt gives you The request processing time in milliseconds and since its a bucket we can use histogram_quantile function.\nSimilarly create a alert to above examples and use the below query :\nhistogram_quantile(0.95,sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[15m])) by (le,ingress)) \u0026gt; 1.5 I’ve set the threshold to 1.5 seconds, it can be updated as per your SLO.\nHigh Request rate To get the request rate per second (RPS), we can use the below query :\nsum(rate(nginx_ingress_controller_requests[5m])) by (ingress) \u0026gt; 2000 The above query will trigger an alert when the request rate is greater than 2000 RPS.\nOther SLIs to consider Connection rate: This measures the number of active connections to Nginx ingress, and can be used to identify potential issues with connection handling.\nrate(nginx_ingress_controller_nginx_process_connections{ingress=\u0026#34;ingress-name\u0026#34;}[5m]) Upstream response time: The time it takes for the underlying service to respond to a request, this will help to identify issues with the service and not just the ingress.\nhistogram_quantile(0.95,sum(rate(nginx_ingress_controller_response_duration_seconds_bucket[15m])) by (le,ingress)) Slack Alert Template To make alert messages meaningful, we can use alert templates in Grafana.\nIn order to configure them, go to http://grafana.localdev.me:8080/alerting/notifications and create a new template named slack by pasting the below code block : {{ define \u0026#34;alert_severity_prefix_emoji\u0026#34; -}} {{- if ne .Status \u0026#34;firing\u0026#34; -}} :white_check_mark: {{- else if eq .CommonLabels.severity \u0026#34;critical\u0026#34; -}} :fire: {{- else if eq .CommonLabels.severity \u0026#34;warning\u0026#34; -}} :warning: {{- end -}} {{- end -}} {{ define \u0026#34;slack.title\u0026#34; -}} {{ template \u0026#34;alert_severity_prefix_emoji\u0026#34; . }} {{- .Status | toUpper -}}{{- if eq .Status \u0026#34;firing\u0026#34; }} x {{ .Alerts.Firing | len -}}{{- end }} | {{ .CommonLabels.alertname -}} {{- end -}} {{- define \u0026#34;slack.text\u0026#34; -}} {{- range .Alerts -}} {{ if gt (len .Annotations) 0 }} *Summary*: {{ .Annotations.summary}} *Description*: {{ .Annotations.description }} Labels: {{ range .Labels.SortedPairs }}{{ if or (eq .Name \u0026#34;ingress\u0026#34;) (eq .Name \u0026#34;cluster\u0026#34;) }}• {{ .Name }}: `{{ .Value }}` {{ end }}{{ end }} {{ end }} {{ end }} {{ end }} Configure a new contact point of type Slack. For this, you need to create an incoming webhook from Slack. Refer this doc for more detailed steps.\nEdit the contact point slack and scroll down and select the option Optional Slack settings.\nIn the Title, enter the below to specify the template to use:\n{{ template \u0026#34;slack.title\u0026#34; . }} In the Text Body, enter the below and save it : {{ template \u0026#34;slack.text\u0026#34; . }} Go to http://grafana.localdev.me:8080/alerting/routes and configure the Default contact point to be Slack. Finally, the alert message arrives! After configuring all the steps, finally we arrive at the end and below are the snapshots of how the alert will look on your slack.\n4xx Error Rate :\n5xx Error Rate :\nLatency P95 :\nThere are lots of things one can improve according to their requirements. For example, if you have mulltple Kubernetes clusters, you can add a cluster label which will help in identifying the source cluster for the alert.\nReferences https://grafana.com/docs/grafana/latest/alerting/\nhttps://kubernetes.github.io/ingress-nginx/user-guide/monitoring/\nhttps://github.com/stefanprodan/podinfo\n","permalink":"https://tanmay-bhat.github.io/posts/slo-based-alert-on-ingress-nginx/","summary":"\u003cp\u003eIn this blog post, we will be discussing how to set up monitoring and alerting for Nginx ingress in a Kubernetes environment.\u003c/p\u003e\n\u003cp\u003eWe will cover the installation and configuration of ingress-nginx, Prometheus and Grafana, and the setup of alerts for key Ingress metrics.\u003c/p\u003e\n\u003ch3 id=\"pre-requisites-\"\u003ePre-requisites :\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA Kubernetes cluster\u003c/li\u003e\n\u003cli\u003eHelm v3\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"install-prometheus-and-grafana\"\u003eInstall Prometheus and Grafana\u003c/h2\u003e\n\u003cp\u003eIn this step, we will install Prometheus to collect metrics, and Grafana to visualize and create alerts based on those metrics.\u003c/p\u003e","title":"How to Configure Alerting on Ingress-NGINX in Kubernetes"},{"content":"Helm is a popular open-source package manager for Kubernetes that simplifies the process of installing, upgrading, and managing applications on a Kubernetes cluster. Helm packages, called charts, contain all the necessary resources and configuration for deploying an application on a Kubernetes cluster.\nAs with any software project, it\u0026rsquo;s important to test charts before deploying them to ensure that they are reliable and function as intended. Chart testing is the process of verifying the functionality and correctness of a Helm chart before it is deployed to a Kubernetes cluster.\nThere are a variety of tools and approaches available for testing Helm charts, ranging from simple shell scripts to more advanced testing frameworks. In this blog post, we\u0026rsquo;ll explore how to use the chart-testing tool, which is maintained by the Helm project, to test Helm charts with GitHub Actions.\nHelm Chart Testing with Pull Requests Lets create a new file in the .github/workflows directory of your repository called chart-testing.yml. This file will define the workflow and specify the steps required to run the chart tests. Update the chart-testing.yml file with below contents :\nname: Lint and Test Charts on: pull_request jobs: lint-test: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: fetch-depth: 0 - name: Set up Helm uses: azure/setup-helm@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: 3.7 - name: Set up chart-testing uses: helm/chart-testing-action@v2.3.1 - name: Run chart-testing (list-changed) id: list-changed run: | changed=$(ct list-changed --config ct.yaml) if [[ -n \u0026#34;$changed\u0026#34; ]]; then echo \u0026#34;{changed}={true}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi - name: Run chart-testing (lint) run: ct lint --config ct.yaml - name: Create kind cluster uses: helm/kind-action@v1.5.0 if: ${{ needs.list-changed.outputs.changed }} == \u0026#39;true\u0026#39; - name: Run chart-testing (install) run: ct install --config ct.yaml Let’s understand the above config in detail.\non: pull_request : This indicates when the workflow should trigger, in our case whenever a pull request is opened. Checkout: This step uses the actions/checkout@v3 action to check out (clone) the code in the repository so that the workflow can access it. The fetch-depth parameter is set to 0 to fetch the entire repository history i.e all history for all branches and tags. Set up Helm: This step uses the azure/setup-helm@v3 action to install and set up Helm v3 latest on the machine. Set up Python: This step uses the actions/setup-python@v4 action to install and set up Python on the machine. The python-version parameter is set to 3.7. Set up chart-testing: This step uses the helm/chart-testing-action@v2.3.1 action to install and set up chart-testing, a tool for testing Helm charts. Run chart-testing (list-changed): This step runs the ct list-changed command using chart-testing to list the charts that have changed since the last commit by referring to the main target branch. The id field is used to give this step an identifier, which can be used to reference it later in the workflow execution. If there are changed charts, the step sets an output named changed to true using the echo \u0026quot;{changed}={true}\u0026quot; \u0026gt;\u0026gt; $GITHUB_OUTPUT syntax. Run chart-testing (lint): This step runs the ct lint command using chart-testing to lint the charts in the repository. Create kind cluster: This step uses the helm/kind-action@v1.5.0 action to create a Kubernetes cluster using kind (Kubernetes IN Docker). The if field is used to specify that this step should only be run if the changed output of the list-changed step is true. I.e this step will only run if there are changes in the helm charts compared to main branch helm charts, basically git diff between them result should not be empty. Run chart-testing (install): This step runs the ct install command using chart-testing to install the charts in the repository into the kind cluster. Now, create a file called ct.yaml which will store the configs for the chart-testing: target-branch: main chart-dirs: - deploy/helm/charts helm-extra-args: --timeout 600s check-version-increment: false The target-branch field specifies the target branch used to identify changed charts.\nThe chart-dirs field specifies the directories where the charts to be tested are located. In this case, the value is deploy/charts, which means that the charts are located in the deploy/charts directory. Update this according to your directory structure.\nThe helm-extra-args field specifies additional arguments to pass to the Helm command when installing or upgrading charts. In this case, the value is --timeout 600s, which means that the Helm command will have a timeout of 600 seconds (10 minutes).\nThe check-version-increment field specifies whether chart-testing should check that the chart version is incremented in Chart.yaml. In this case, the value is false\nThe directory structure finally looks like this :\nRepository └── ct.yaml └── Deploy └── Charts └── example-helm-chart ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── ingress.yaml │ ├── service.yaml └── values.yaml Now that we have configure, we can see the flow in action.\nConsider a case where we wanted to enable Ingress for my app, but didn’t complete the all correct values in the helm chart and pushed to my test-branch .\n#supplied values for my-app ingress: enabled: true annotations: {} hosts: - host: chart-example.local paths: [] #I didn\u0026#39;t specify the path and helm install should fail. As you can see the below, the Action failed :\nSince the problem is with helm templating, the lint step passsed sucessfully.\nRun ct lint --config ct.yaml Linting charts... ------------------------------------------------------------------------------------------------------------------------ Charts to be processed: ------------------------------------------------------------------------------------------------------------------------ my-app =\u0026gt; (version: \u0026#34;0.2.8\u0026#34;, path: \u0026#34;charts/my-app\u0026#34;) ------------------------------------------------------------------------------------------------------------------------ Linting chart \u0026#34;my-app =\u0026gt; (version: \\\u0026#34;0.2.8\\\u0026#34;, path: \\\u0026#34;charts/my-app\\\u0026#34;)\u0026#34; Validating /home/runner/work/helm-actions-demo/my-app/Chart.yaml... Validation success! 👍 Validating maintainers... ==\u0026gt; Linting /charts/my-app 1 chart(s) linted, 0 chart(s) failed ------------------------------------------------------------------------------------------------------------------------ ✔︎ example-v2 =\u0026gt; (version: \u0026#34;0.2.8\u0026#34;, path: \u0026#34;charts/my-app\u0026#34;) ------------------------------------------------------------------------------------------------------------------------ All charts linted successfully Helm templating and helm install should fail since the Ingress config is incomplete : Run ct install --config ct.yaml Installing charts... ------------------------------------------------------------------------------------------------------------------------ Charts to be processed: ------------------------------------------------------------------------------------------------------------------------ my-app =\u0026gt; (version: \u0026#34;0.2.8\u0026#34;, path: \u0026#34;charts/my-app\u0026#34;) ------------------------------------------------------------------------------------------------------------------------ Installing chart \u0026#34;my-app =\u0026gt; (version: \\\u0026#34;0.2.8\\\u0026#34;, path: \\\u0026#34;my-app\\\u0026#34;)\u0026#34;... Error: INSTALLATION FAILED: Ingress.extensions \u0026#34;my-app-ingress\u0026#34; is invalid: spec.rules[0].http.paths: Required value Trigger GitHub Actions for Push and other Events Similar to the above config, you can just configure GitHub actions to trigger whenever a push to the main (or any other) branch happens. Or you can combine the trigger with push and pull request.\n#For both on: [pull_request,push] #For Push event on: push #For push to specifc branches on: push: branches: - main - test-branch #For PR on specific branches on: pull_request: branches: - main - develop You can more about GitHub workflow trigger events here.\nSpecifying Multiple Helm Values Files You can specify values in a folder called cilocated in the root of your chart repository. The ci folder should contain a values.yamlfile with the desired values, and chart-testing will automatically use these values when running the ct installcommand.\nFor example, if you have a chart repository with the following structure:\nRepository └── ct.yaml └── Deploy └── Charts └── example-helm-chart ├── ci │ ├── values-us-east-1.yaml │ ├── values-us-west-2.yaml │ └── values-eu-west-1.yaml ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── ingress.yaml │ ├── service.yaml └── values.yaml It will iterate over every values-*.yaml and the chart is installed and tested for each of these files.\nHelm Upgrades To test an in-place upgrade of a chart, you can use the upgrade flag with the ct install command. This flag will cause chart-testing to test an in-place upgrade of the chart from its previous revision.\nAdd upgrade: true into your ct.yaml to enable the upgrade of the Helm charts.\nIf the current version should not introduce a breaking change according to the SemVer specification, the upgrade will be tested.\nHandling Dependencies in Chart Testing When testing Helm charts that have dependencies on other charts, it is important to ensure that these dependencies are properly installed and configured in order for the tests to be successful. By default, ct install handles dependency of helm charts i.e it will run helm dependency build to fetch the chart specified.\nYou need to add the repository in ct.yaml so that repository index can be fetched, Bitnami in this example : chart-repos: - bitnami=https://charts.bitnami.com/bitnami Next, you need to specify the required chart in your Chart.yaml of your specific helm chart, Redis for Example : dependencies: - name: redis version: 17.4.0 repository: https://charts.bitnami.com/bitnami Then you can use your custom values for the Redis chart in the values.yaml, for example : redis: enabled: true cluster: enabled: false References : https://GitHub.com/helm/chart-testing\nhttps://GitHub.com/marketplace/actions/helm-chart-testing\nhttps://docs.GitHub.com/en/actions/using-jobs/using-jobs-in-a-workflow#overview\n","permalink":"https://tanmay-bhat.github.io/posts/helm-chart-testing-github-actions/","summary":"\u003cp\u003eHelm is a popular open-source package manager for Kubernetes that simplifies the process of installing, upgrading, and managing applications on a Kubernetes cluster. Helm packages, called charts, contain all the necessary resources and configuration for deploying an application on a Kubernetes cluster.\u003c/p\u003e\n\u003cp\u003eAs with any software project, it\u0026rsquo;s important to test charts before deploying them to ensure that they are reliable and function as intended. Chart testing is the process of verifying the functionality and correctness of a Helm chart before it is deployed to a Kubernetes cluster.\u003c/p\u003e","title":"Automate Your Helm Chart Testing Workflow with GitHub Actions"},{"content":" I’ve recently got a gaming laptop and monitoring the CPU \u0026amp; GPU temperature of it has been a tedious task, like install MSI Afterburner, configure statistics server, configure overlay etc.\nThat made me use Grafana’s product suite to configure monitoring of key components such as CPU, GPU, Network, Disk and alerting for my laptop such that I can game in peace and when my laptop’s temperature reaches a certain threshold limit, I’ll get a phone call.\nInstall and Configure OhmGraphite OhmGraphite is a Windows service that exposes hardware sensor data to a metric store, allowing one to observe health and status of one’s system over below components:\nPower consumption of the CPU and GPU CPU voltages and frequencies Load breakdown on individual GPU components CPU, GPU, disk, and motherboard temperature readings Disk activity, space overview Network consumption Lets install and configure OhmGraphite in our laptop :\nCreate a directory that will be the base directory for OhmGraphite (like C:\\Program Files\\OhmGraphite). Download the latest zip and extract to C:\\Program Files\\OhmGraphite . Next, we need to update the app configuration so that the app can scrape the laptop metrics and expose it at localhost:4445/metrics. In order to do that, edit the file OhmGraphite.exe.config and update it with below values : \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appSettings\u0026gt; \u0026lt;add key=\u0026#34;type\u0026#34; value=\u0026#34;prometheus\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;prometheus_port\u0026#34; value=\u0026#34;4445\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;prometheus_host\u0026#34; value=\u0026#34;localhost\u0026#34; /\u0026gt; \u0026lt;/appSettings\u0026gt; \u0026lt;/configuration\u0026gt; We need the above changes since the app can expose metrics in metric stores for example, Graphite, Prometheus \u0026amp; even push to TimescaleDB \u0026amp; InfluxdDB. Install the app by running the command .\\OhmGraphite.exe install. This command will install OhmGraphite as a Windows service. Now, we can start the OhmGraphite service by running the below command : #PowerShell start-Service OhmGraphite #CommandLine sc start OhmGraphite You can curl the endpoint http://localhost:4445/metrics and the scraped metrics are listed there. #Sample response curl http://localhost:4445/metrics # HELP ohm_gpunvidia_bytes Metric reported by open hardware sensor # TYPE ohm_gpunvidia_bytes gauge ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Used\u0026#34;,} 1072693248 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Free\u0026#34;\u0026#34;} 3221225472 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Total\u0026#34;} 4294967296 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;D3D Shared Memory Used\u0026#34;} 69447680 Install and Configure Grafana Agent Download and install Grafana Agent for Windows from this link (latest release). Once it’s installed, edit the file C:\\Program Files\\Grafana Agent\\agent-config.yaml and update the below values : metrics: global: scrape_interval: 60s wal_directory: /tmp/grafana-agent-wal configs: - name: laptop_exporter remote_write: - basic_auth: password: \u0026lt;your-grafana-cloud-api-key\u0026gt; username: \u0026lt;your-grafana-cloud-username-id\u0026gt; url: \u0026lt;your-grafana-cloud-prometheus-push-endpoint\u0026gt; scrape_configs: - job_name: laptop_exporter static_configs: - targets: [\u0026#34;localhost:4445\u0026#34;] I’ve kept the scrape_interval to 1 minute. You can update to lower value(like 30s) if you need the values to be updated quicker. Job name can be anything you want. Better to keep something identifiable and meaningful. If you already have a Grafana Cloud account, then you can get the API Key, User-ID, \u0026amp; the Prometheus metrics push endpoint at your Prometheus stack page : Restart the Grafana Agent to update your new config : #PowerShell Restart-Service \u0026#39;Grafana Agent\u0026#39; #CommandLine sc stop \u0026#39;Grafana Agent\u0026#39; sc start \u0026#39;Grafana Agent\u0026#39; Verify that Agent is able to recognize the target and scraping the metrics : curl -s http://localhost:12345/agent/api/v1/metrics/targets | jq { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: [ { ... \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4445/metrics\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;instance\u0026#34;: \u0026#34;localhost:4445\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;laptop_exporter\u0026#34; ... ] } Grafana Agent also has Health \u0026amp; ready status API. This might be handy when you’re debugging Agent’s health and scrape issues. You can check them by running :\ncurl -s http://localhost:12345/-/ready #sample output Agent is Ready. curl -s http://localhost:12345/-/healthy #sample output Agent is Healthy. Scrape metrics using self hosted Prometheus (optional) If you want to use your own Prometheus instead of Grafana Agent, then you can add a scrape job to your existing Prometheus config prometheus.yml like below :\nglobal: scrape_interval: 30s scrape_configs: - job_name: \u0026#39;laptop_exporter\u0026#39; static_configs: - targets: [\u0026#34;localhost:4445\u0026#34;] Import Grafana Dashboard Once everything is setup, you can Import the Dashboard-ID : 11587 in your Grafana to visualize the metrics collected.\nI’ve updated a few panels from the imported dashboard. For example for CPU temperature, my PromQL query is :\navg_over_time(ohm_cpu_celsius{instance=\u0026#34;$instance\u0026#34;}[5m]) Here’s how my Grafana Dashboard looks after the edit :\nCPU Metrics:\nGPU Metrics:\nAlerting I don’t want to have an overlay of CPU and GPU temperatures on my screen while I’m gaming. Hence, I thought, Instead of having an overlay, why not create alerts for high temperatures and integrate those alerts to Grafana OnCall such that when my laptop is hot, I’ll get a call because who checks messages while gaming :D.\nAlert for High CPU Temperature\nI’ve configured the alert threshold to 80 degree. So, if my laptop CPU temperature stays above 80 degree for 5 minutes straight, I’ll get a phone call. Here’s how my alert config looks like.\nOnce I receive the call, I can choose to lower the game resolution and continue to play which should give a bit of breathing room for my laptop or close it so that laptop can cool itself.\nAlert for High GPU Temperature\nSame config goes for GPU temperature alert as well. I’m averaging out the temperature value since I don’t want to be paged for a temperature spike. I’d like the alert to be fired only when the temperature is consistently higher than the threshold.\nAlerting via AlertManager There’s no specific requirement to use Grafana’s new alerting. If you’d like to use your own AlertManager, you can easily write an alert config by referring to an example from Awesome Prometheus alerts.\nFor notification configuration, use the \u0026lt;webhook_config\u0026gt; section and use the webhook from Grafana OnCall’s integration. Here’s the doc for webhook config options for AlertManager.\nGrafana OnCall Configuring Grafana OnCall is fairly simple for Phone call alerts.\nCreate or verify the user which phone number on the Users section. Set the Default notification Method to : Phone Create a default escalation policy like below : Integrate the escalation policy you created to the Grafana Integration : Click on the “How to connect” as shown in the above screenshot which will give you the Grafana Oncall Webhook. Copy that. Go to Alerting → Contact Points → New → Select type as : Webhook and past the URL you copied from above step. That’s it, whenever the alert’s threshold reaches its value, you should get a phone call.\nReferences https://github.com/nickbabcock/OhmGraphite\nhttps://prometheus.io/docs/alerting/latest/configuration\nhttps://awesome-prometheus-alerts.grep.to/\nhttps://grafana.com/docs/oncall/latest/\n","permalink":"https://tanmay-bhat.github.io/posts/monitor-gaming-laptop-using-grafana--ohmgraphite/","summary":"\u003cp\u003e\u003cimg alt=\"Grafana laptop diagram\" loading=\"lazy\" src=\"/grafana-laptop-image.png\"\u003e\nI’ve recently got a gaming laptop and monitoring the CPU \u0026amp; GPU temperature of it has been a tedious task, like install MSI Afterburner, configure statistics server, configure overlay etc.\u003c/p\u003e\n\u003cp\u003eThat made me use Grafana’s product suite to configure monitoring of key components such as CPU, GPU, Network, Disk and alerting for my laptop such that I can game in peace and when my laptop’s temperature reaches a certain threshold limit, I’ll get a phone call.\u003c/p\u003e","title":"Monitor Gaming Laptop Using Grafana and OhmGraphite"},{"content":"\nIn this article, we’ll see how to setup Grafana with a remote database so that we can scale Grafana to N instances.\nThe default SQLite database will not work with scaling beyond 1 instance since the SQLite3 DB is embedded inside Grafana container.\nCreate Remote PostgreSQL using fly.io (Optional) For this demonstration, I’ll be using fly.io’s PostgreSQL service. Its free (no need to add credit card) and it has 1 GB of storage for DB and should be enough to try out with Grafana. Note : If you already have a remote DB such as AWS RDS (or local DB) running, you can skip this step.\nFirst, download and install the flyctl by referring this doc. Create the account and sign in by referring to this doc. Create the DB cluster using the command : flyctl postgres create Set the name for the cluster : grafana Choose the region near to you, for example : India Select Development - Single node, 1x shared CPU, 256MB RAM, 1GB disk After a minute, the cluster should get created and the username, password and URL will be visible like below, save it somewhere. Postgres cluster grafana created Username: postgres Password: super-secret-pasword Hostname: grafana.internal Proxy Port: 5432 Postgres Port: 5433 Now, the cluster is only accessible inside fly.io’s network since it doesn\u0026rsquo;t have a public address to reach over the internet. Use the below command to port-forward the DB connection to localhost at 5432: flyctl proxy 5432 -a grafana #expected output Proxying local port 5432 to remote [grafana.internal]:5432 You can connect to the DB using below command : flyctl postgres connect -a grafana Create Grafana Database Since we’re using remote database, when the first time Grafana starts, it starts DB migration and that will fail if it can\u0026rsquo;t find a database called grafana.\nHence, let’s create the DB using the command below (via psql client) :\ncreate database grafana; Grafana with Docker-Compose Once the DB is up and running, let\u0026rsquo;s create a docker-compose.yaml file for our Grafana:\nversion: \u0026#39;3\u0026#39; services: grafana: image: grafana/grafana:9.0.7 container_name: grafana ports: - 3000:3000 environment: - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=postgres - GF_DATABASE_PASSWORD=super-secret-password - GF_DATABASE_TYPE=postgres - GF_DATABASE_HOST=host.docker.internal:5432 Please update the environment variables as per your DB details.\nStart Grafana container using the command :\ndocker-compose up -d Grafana should be running now and DB migration should start and logs will indicate that : logger=sqlstore t=2022-08-20T15:24:28.83593716Z level=info msg=\u0026#34;Connecting to DB\u0026#34; dbtype=postgres logger=migrator t=2022-08-20T15:24:29.231523787Z level=info msg=\u0026#34;Starting DB migrations\u0026#34; logger=migrator t=2022-08-20T15:24:29.352920106Z level=info msg=\u0026#34;Executing migration\u0026#34; id=\u0026#34;create migration_log table\u0026#34; logger=migrator t=2022-08-20T15:24:29.5825907Z level=info msg=\u0026#34;Executing migration\u0026#34; id=\u0026#34;create user table\u0026#34; Open Grafana by navigating to http://localhost:3000 Gotchas In the above example, I’ve mentioned DB host as host.docker.internal since my DB is accessible through localhost of the host machine. If it\u0026rsquo;s AWS RDS or similar managed DB, just mention the DB connection URL and enable SSL verification. Regarding DB user, for this example, I haven’t created a separate DB user, but if you’re running a similar setup in production, it is highly advised that you create one. Data Persistence in Docker Grafana v9 stores almost all data inside its database, including alert configurations. We can check that by listing the tables inside our Grafana database : Schema | Name | Type | Owner --------+----------------------------+-------+---------- public | alert | table | postgres public | alert_configuration | table | postgres public | alert_notification | table | postgres public | api_key | table | postgres public | dashboard | table | postgres public | dashboard_provisioning | table | postgres public | data_source | table | postgres public | org | table | postgres public | org_user | table | postgres public | permission | table | postgres public | preferences | table | postgres public | team | table | postgres public | team_member | table | postgres public | user | table | postgres public | user_auth | table | postgres public | user_auth_token | table | postgres public | user_role | table | postgres ... In case of docker, if the container restarts, none of your configurations or dashboards will be lost.\nTo test the persistence, let’s create a Prometheus data source :\nImport the node exporter dashboard, Dashboard ID : 1860 Now, you can test that by simply restarting the Grafana container : docker-compose restart grafana The logs should show something similar to below : logger=settings t=2022-08-20T15:47:49.880986785Z level=info msg=\u0026#34;App mode production\u0026#34; logger=sqlstore t=2022-08-20T15:47:49.881148929Z level=info msg=\u0026#34;Connecting to DB\u0026#34; dbtype=postgres logger=migrator t=2022-08-20T15:47:54.783450339Z level=info msg=\u0026#34;Starting DB migrations\u0026#34; logger=migrator t=2022-08-20T15:47:55.043328627Z level=info msg=\u0026#34;migrations completed\u0026#34; performed=0 skipped=426 duration=894.268µs High Availability Grafana Setup in Docker To try our HA for Grafana in Docker, let\u0026rsquo;s create 2 grafana replicas behind an nginx proxy : version: \u0026#39;3\u0026#39; services: grafana: image: grafana/grafana:9.0.7 expose: - \u0026#34;3000\u0026#34; environment: - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=postgres - GF_DATABASE_PASSWORD=super-secret-password - GF_DATABASE_TYPE=postgres - GF_DATABASE_HOST=host.docker.internal:5432 deploy: replicas: 2 nginx: image: nginx:latest container_name: nginx volumes: - \u0026#39;./nginx.conf:/etc/nginx/nginx.conf\u0026#39; depends_on: - grafana ports: - \u0026#34;8000:8000\u0026#34; If you look closely, we’ve exposed 3000 port from Grafana inside docker network and added Nginx as a proxy which will serve at port 8000. Let’s create a file called nginx.conf so that it can forward the traffic to 3000 port of Grafana containers : user nginx; events { worker_connections 1000; } http { server { listen 8000; location / { proxy_pass http://grafana:3000; proxy_set_header Host $http_host; } } } Now, you can start this stack by running docker-compose up -d and you can access Grafana by going to http://localhost:8000 Data Persistence \u0026amp; HA in Kubernetes For this demo, I’ll be using minikube as my Kubernetes cluster.\nFirst, let’s install Grafana using helm.\nRun the below commands to add the Grafana helm repository to your cluster :\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Once added, create a file called custom-values.yaml and add the values : grafana.ini: database: type: \u0026#34;postgres\u0026#34; host: \u0026#34;host.minikube.internal:5432\u0026#34; user: \u0026#34;postgres\u0026#34; password: \u0026#34;super-secret-pasword\u0026#34; name: \u0026#34;grafana\u0026#34; replicas: 2 Note : Since my database is accessible at localhost, I’m using host has host.minikube.internal. If you’re using AWS RDS or a similar service, you should put the DNS address in the host section.\nInstall Grafana helm chart using the below command : helm install grafana grafana/grafana -f custom-values.yaml Port forward Grafana service : kubectl port-forward svc/grafana 80:80\nYou can access the Grafana now by going to: http://localhost:80.\nAny number of pods you scale up for Grafana, they all now will connect to a shared Database.\nGotchas If you don’t have a shared database for Grafana and try to scale the replicas to \u0026gt; 1 it may result in unexpected results because by default each pod will have its own SQLite3 DB and they won\u0026rsquo;t be in sync.\nFor example, I have 2 Grafana replicas running without a shared database connection.\n\u0026gt; kubectl get pod NAME READY STATUS RESTARTS AGE grafana-975c48997-kw5vk 1/1 Running 0 65m grafana-975c48997-sq9wl 1/1 Running 0 65m After port-forwarding, I added a data source along with a dashboard for it.\nThe first pod is receiving traffic, and the second pod has no clue what’s happening on the other side.\n\u0026#34;Request Completed\u0026#34; method=POST path=/api/ds/query status=400 remote_addr=127.0.0.1 time_ms=3247 duration=3.247032415s size=110 referer=\u0026#34;http://localhost/d/rYdddlPWk/node-exporter-full?orgId=1\u0026amp;refresh=1m\u0026#34; traceID=00000000000000000000000000000000 logger=context traceID=00000000000000000000000000000000 userId=1 orgId=1 uname=admin t=2022-08-22T12:23:55.285644474Z level=info msg=\u0026#34;Request Completed\u0026#34; method=POST path=/api/ds/query status=400 remote_addr=127.0.0.1 time_ms=3247 duration=3.247181905s size=110 referer=\u0026#34;http://localhost/d/rYdddlPWk/node-exporter-full?orgId=1\u0026amp;refresh=1m\u0026#34; traceID=00000000000000000000000000000000 Whenever the request is sent to the second pod, connection gets logged out with auth error since in that pod, data doesn\u0026rsquo;t exist.\n#first pod level=info msg=\u0026#34;Successful Login\u0026#34; User=admin@localhost #second pod level=error msg=\u0026#34;Failed to look up user based on cookie\u0026#34; error=\u0026#34;user token not found\u0026#34; To keep it fair, I even observed the traffic flow to those pods using Linkerd. As you can see, the requests are almost equally balanced between pods :\nlinkerd viz stat pod grafana-55d88bb8b9-445dk grafana-55d88bb8b9-mrnf9 NAME STATUS MESHED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 TCP_CONN grafana-55d88bb8b9-445dk Running 1/1 100.00% 1.1rps 2ms 50ms 89ms 11 grafana-55d88bb8b9-mrnf9 Running 1/1 100.00% 1.6rps 2ms 18ms 77ms 9 Bonus You can run a load test on your Grafana setup using K6 and scale accordingly with the results.\nAdditional Note : You can run a local PostgreSQL and use that as your DB as well, no need to use AWS RDS or similar kind. The only added advantage with Managed Databases is that there’s no operational overhead.\nReferences https://grafana.com/docs/grafana/latest/setup-grafana/set-up-for-high-availability/\nhttps://fly.io/docs/reference/postgres/\nhttps://minikube.sigs.k8s.io/docs/handbook/host-access/\nhttps://github.com/grafana/grafana/blob/main/devenv/docker/ha_test\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-configure-grafana-to-use-remote-database-for-ha/","summary":"\u003cp\u003e\u003cimg alt=\"Grafana HA diagram\" loading=\"lazy\" src=\"/grafana-ha.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this article, we’ll see how to setup Grafana with a remote database so that we can scale Grafana to N instances.\u003c/p\u003e\n\u003cp\u003eThe default SQLite database will not work with scaling beyond 1 instance since the SQLite3 DB is embedded inside Grafana container.\u003c/p\u003e\n\u003ch3 id=\"create-remote-postgresql-using-flyiohttpflyio-optional\"\u003eCreate Remote \u003cstrong\u003ePostgreSQL using \u003ca href=\"http://fly.io\"\u003efly.io\u003c/a\u003e (Optional)\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFor this demonstration, I’ll be using \u003ca href=\"https://fly.io/docs/reference/postgres/\"\u003efly.io\u003c/a\u003e’s PostgreSQL service.\u003c/li\u003e\n\u003cli\u003eIts free (no need to add credit card) and it has 1 GB of storage for DB and should be enough to try out with Grafana.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e : If you already have a remote DB such as AWS RDS (or local DB) running, you can skip this step.\u003c/p\u003e","title":"How to Configure Grafana to Use Remote Database for HA"},{"content":"OAuth2 Proxy is a reverse proxy that provides authentication using Providers such as Google, GitHub, and others to validate accounts by email, domain or group.\nIn this article, we’ll setup and configure OAuth2 Proxy to enable Google SSO for Kibana in Kubernetes.\nPrerequisite: Kibana in Kubernetes Nginx Ingress Setup Google Credentials In Google Cloud Console, select OAuth consent screen Select the User Type as : “Internal” Complete the app registration form with your Authorized domain, then click Save. In the left Nav pane, choose Credentials In the center pane, choose \u0026ldquo;Credentials\u0026rdquo; tab. Open the \u0026ldquo;New credentials\u0026rdquo; drop down Choose \u0026ldquo;OAuth client ID\u0026rdquo; Choose \u0026ldquo;Web application\u0026rdquo; Application name can be anything, for simplicity, let’s say Kibana. Authorized JavaScript origins is your Kibana endpoint ex: https://kibana.example.com Authorized redirect URIs is the location of oauth2/callback ex: https://kibana.example.com.com/oauth2/callback Choose \u0026ldquo;Create\u0026rdquo; Download the Credentials JSON file. Setup Oauth2 Proxy Generate a random cookie secret with the below command and copy it to clipboard : docker run -ti --rm python:3-alpine python -c \u0026#39;import secrets,base64; print(base64.b64encode(base64.b64encode(secrets.token_bytes(16))));\u0026#39; Add the Helm repository : helm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests Install the helm chart with specified parameters as below : helm upgrade --install oauth2-proxy oauth2-proxy/oauth2-proxy \\ --set config.clientID=\u0026#34;GOOGLE_CLIENT_ID_HERE\u0026#34; \\ --set config.clientSecret=\u0026#34;GOOGLE_CLIENT_SECRET_HERE\u0026#34; \\ --set config.cookieSecret=\u0026#34;GENERATED_COOKIE_SECRET_HERE\u0026#34; \\ --set extraArgs.provider=\u0026#34;google\u0026#34; Use the Google Client_ID and SECRET from the downloaded JSON file. Create and apply the Ingress for Oauth2 Proxy using the below YAML manifest : cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: oauth2-proxy spec: ingressClassName: nginx rules: - host: https://kibana.example.com http: paths: - path: /oauth2 pathType: Prefix backend: service: name: oauth2-proxy port: number: 4180 EOF Setup Ingress annotation for Kibana This article assumes you already have a Kibana setup running in Kubernetes with an Ingress. Append the two nginx annotations mentioned below to existing Kibana Ingress. Once updated, the Ingress should look similar to : apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kibana annotations: nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth spec: ingressClassName: nginx rules: - host: kibana.example.com http: paths: - backend: service: name: kibana port: number: 5601 path: / pathType: ImplementationSpecific Now if you hit the endpoint kibana.example.com. It should ask you to login via Google. Since we have mentioned the OAuth type as Internal and also specified Authorised Domain, any google login except your specified Domain will be 401. References :\nhttps://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/\nhttps://oauth2-proxy.github.io/oauth2-proxy/\n","permalink":"https://tanmay-bhat.github.io/posts/google-sso-kibana/","summary":"\u003cp\u003e\u003cstrong\u003eOAuth2 Proxy\u003c/strong\u003e is a reverse proxy that provides authentication using Providers such as Google, GitHub, and others to validate accounts by email, domain or group.\u003c/p\u003e\n\u003cp\u003eIn this article, we’ll setup and configure OAuth2 Proxy to enable Google SSO for Kibana in Kubernetes.\u003c/p\u003e\n\u003ch3 id=\"prerequisite\"\u003ePrerequisite:\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eKibana in Kubernetes\u003c/li\u003e\n\u003cli\u003eNginx Ingress\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"setup-google-credentials\"\u003eSetup Google Credentials\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eIn Google Cloud Console, select \u003ca href=\"https://console.cloud.google.com/apis/credentials/consent\"\u003eOAuth consent screen\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eSelect the User Type as : “\u003cstrong\u003eInternal”\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eComplete the app registration form with your \u003cstrong\u003e\u003cstrong\u003eAuthorized domain\u003c/strong\u003e\u003c/strong\u003e, then click \u003cstrong\u003eSave.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eIn the left Nav pane, choose \u003ca href=\"https://console.cloud.google.com/apis/credentials\"\u003eCredentials\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eIn the center pane, choose \u003cstrong\u003e\u0026ldquo;Credentials\u0026rdquo;\u003c/strong\u003e tab.\n\u003cul\u003e\n\u003cli\u003eOpen the \u003cstrong\u003e\u0026ldquo;New credentials\u0026rdquo;\u003c/strong\u003e drop down\u003c/li\u003e\n\u003cli\u003eChoose \u003cstrong\u003e\u0026ldquo;OAuth client ID\u0026rdquo;\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eChoose \u003cstrong\u003e\u0026ldquo;Web application\u0026rdquo;\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eApplication name can be anything, for simplicity, let’s say Kibana.\u003c/li\u003e\n\u003cli\u003eAuthorized JavaScript origins is your Kibana endpoint ex: \u003ccode\u003ehttps://kibana.example.com\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eAuthorized redirect URIs is the location of \u003cstrong\u003eoauth2/callback\u003c/strong\u003e ex: \u003ccode\u003ehttps://kibana.example.com.com/oauth2/callback\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eChoose \u003cstrong\u003e\u0026ldquo;Create\u0026rdquo;\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDownload the Credentials JSON file.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"setup-oauth2-proxy\"\u003eSetup Oauth2 Proxy\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eGenerate a random cookie secret with the below command and copy it to clipboard :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run -ti --rm python:3-alpine python -c \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;import secrets,base64; print(base64.b64encode(base64.b64encode(secrets.token_bytes(16))));\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eAdd the Helm repository :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehelm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eInstall the helm chart with specified parameters as below :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehelm upgrade --install oauth2-proxy oauth2-proxy/oauth2-proxy \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e--set config.clientID\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;GOOGLE_CLIENT_ID_HERE\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e--set config.clientSecret\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;GOOGLE_CLIENT_SECRET_HERE\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e--set config.cookieSecret\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;GENERATED_COOKIE_SECRET_HERE\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003e\u003c/span\u003e--set extraArgs.provider\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;google\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eUse the Google Client_ID and SECRET from the downloaded JSON file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eCreate and apply the Ingress for Oauth2 Proxy using the below YAML manifest :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yml\" data-lang=\"yml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003ecat \u0026lt;\u0026lt;EOF | kubectl create -f -\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eapiVersion\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enetworking.k8s.io/v1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ekind\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eIngress\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003emetadata\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eoauth2-proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003espec\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eingressClassName\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003erules\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#f92672\"\u003ehost\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttps://kibana.example.com\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ehttp\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003epaths\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#f92672\"\u003epath\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e/oauth2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003epathType\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ePrefix\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003ebackend\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          \u003cspan style=\"color:#f92672\"\u003eservice\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eoauth2-proxy\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003eport\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e              \u003cspan style=\"color:#f92672\"\u003enumber\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e4180\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ae81ff\"\u003eEOF\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"setup-ingress-annotation-for-kibana\"\u003eSetup Ingress annotation for Kibana\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThis article assumes you already have a Kibana setup running in Kubernetes with an Ingress.\u003c/li\u003e\n\u003cli\u003eAppend the two nginx annotations mentioned below to existing Kibana Ingress. Once updated, the Ingress should look similar to :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yml\" data-lang=\"yml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eapiVersion\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enetworking.k8s.io/v1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ekind\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eIngress\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003emetadata\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ekibana\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eannotations\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003enginx.ingress.kubernetes.io/auth-signin\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttps://$host/oauth2/start?rd=$escaped_request_uri\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003enginx.ingress.kubernetes.io/auth-url\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ehttps://$host/oauth2/auth\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003espec\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eingressClassName\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003enginx\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003erules\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  - \u003cspan style=\"color:#f92672\"\u003ehost\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ekibana.example.com\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003ehttp\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"color:#f92672\"\u003epaths\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      - \u003cspan style=\"color:#f92672\"\u003ebackend\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          \u003cspan style=\"color:#f92672\"\u003eservice\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003ename\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003ekibana\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003eport\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e              \u003cspan style=\"color:#f92672\"\u003enumber\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e5601\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003epath\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e/\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f92672\"\u003epathType\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003eImplementationSpecific\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eNow if you hit the endpoint \u003ca href=\"http://kibana.example.com\"\u003ekibana.example.com\u003c/a\u003e. It should ask you to login via Google.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"kibana-sso\" loading=\"lazy\" src=\"/kibana-google.png\"\u003e\u003c/p\u003e","title":"How to enable Google SSO in Kibana using OAuth2 Proxy [Kubernetes]"},{"content":"In this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.\n1. Node exporter: Node exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started. Although 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like Grafana cloud, AWS Prometheus and Google Cloud for Prometheus, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent \u0026amp; stored. It’s not necessary that you should cut down on metric scraping if you’re sending metrics to any of the vendors mentioned above. This also implies to local storage of Prometheus data, since too many time series over time can hinder Prometheus performance. What’s a collector ? Main components of a node are referred to as collector, for example CPU, file-system, memory etc. Each collector exposes a set of metrics about the component it covers. Here’s the list of collectors that are enabled by default. Disable collectors : A collector can be disabled by providing the flag : --no-collector.\u0026lt;collector-name\u0026gt; #the command will look like this : node_exporter --no-collector.nfs Disable self metrics of node-exporter : Node-exporter exposes ~ 80 metrics about itself at /metrics along with node metrics. The metrics about node-exporter starts with prefix promhttp_*, process_*, go_*. Below is the list of some of them : process_cpu_seconds_total process_max_fds process_open_fds process_resident_memory_bytes process_start_time_seconds process_virtual_memory_bytes process_virtual_memory_max_bytes promhttp_metric_handler_errors_total promhttp_metric_handler_requests_in_flight promhttp_metric_handler_requests_total . . \u0026amp; around 68 go based metrics. You can disable all the above ~80 metrics by running node-exporter with flag -web.disable-exporter-metrics #the command will look like this : node_exporter --web.disable-exporter-metrics Enable only the collectors required : Opposite to disabling certain collectors, Node exporter has a flag --collector.disable-defaults which disables all collectors at once. Combining that flag with the collector of your choice will only collect the metrics of the collectors you enabled and discard everything else. For example, If you want to collect only the CPU and Memory metrics of a node, you can run the below command :\nnode_exporter --collector.disable-defaults --collector.cpu --collector.meminfo Filter collectors via scrape config: This is especially useful when you don’t have control over the nodes \u0026amp; have around 100’s of nodes where changing the config on each of them is not feasible. You can mention the names of the collectors which you want to enable (i.e., to be scraped) at scrape config of Prometheus, and all metrics from other collectors will be dropped. Below is an example Prometheus config: # scrape config to collect only cpu \u0026amp; memory metrics via node-exporter. scrape_configs: - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] params: collect[]: - cpu - meminfo 2. cAdvisor cAdvisor (Container Advisor) is a running daemon that collects, aggregates, processes, and exports information about running containers.\nDisable metrics: Similar to collectors, you can also disable metrics by passing flag : --disable_metrics Below is an example argument which disables a bunch of metric sources : --disable_metrics=accelerator,percpu,sched,resctrl,sched,process,hugetlb,referenced_memory,cpu_topology,memory_numa,tcp,advtcp,resctrl,udp\u0026#39; An example docker-compose file consisting Prometheus \u0026amp; cAdvisor :\nversion: \u0026#39;3.2\u0026#39; services: prometheus: image: prom/prometheus:latest container_name: prometheus ports: - 9090:9090 command: - --config.file=/etc/prometheus/prometheus.yml volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro depends_on: - cadvisor cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor ports: - 8080:8080 volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro command: - \u0026#39;--disable_metrics=accelerator,percpu,sched,resctrl,sched,process,hugetlb,referenced_memory,cpu_topology,memory_numa,tcp,advtcp,resctrl,udp\u0026#39; Prometheus config to scrape cAdvisor metrics, prometheus.yml :\nscrape_configs: - job_name: cadvisor static_configs: - targets: - cadvisor:8080 Drop costly metrics : Let’s find out what are the top 10 costly metrics by cAdvisor. We can get that result by running the below promQL expression :\n#syntax topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;,job=\u0026#34;cadvisor_job_name\u0026#34;})) #example topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;,job=\u0026#34;kubernetes-nodes-cadvisor\u0026#34;})) Which will look like this : Let’s say we want to drop container_memory_rss metrics. We can utilize Prometheus metric relabeling on this. Update the below to Prometheus scrape config : scrape_configs: - job_name: \u0026#34;cadvisor\u0026#34; scrape_interval: 15s static_configs: - targets: [\u0026#34;cadvisor:8080\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: \u0026#39;(container_memory_rss)\u0026#39; action: drop You can refer my article on Prometheus metrics drop \u0026amp; deletion to understand more about this.\n3. kube-state-metrics kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes, and pods.\nDisable Collectors: Just like the above-mentioned services, kube-state-metrics also has collectors which collect metrics about specific components like statefulset, daemonset, PVC etc. You can find the list of collectors here. All collectors are enabled by default, which in most scenarios not needed. You can enable only the collectors you need ( i.e., disable others) by mentioning them via flag --resources. Here’s how it looks :\nimage: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0 args: - --resources=deployments,persistentvolumeclaims,pods,services,statefulsets Metric denylist : Suppose you have a metric called kube_deployment_spec_strategy_rollingupdate_max_surge which created 1000s of time series which is not-useful. In this case, you can add that metric to denylist of kube-state-metrics with flag --metric-denylist and that metric won\u0026rsquo;t be scraped or collected. --metric-denylist flag also accepts regex if you want to deny multiple matching metrics at once. For example: image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0 args: - --metric-denylist=kube_deployment_spec_.* Resources : https://github.com/kubernetes/kube-state-metrics‣ https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics https://github.com/prometheus/node_exporter https://github.com/google/cadvisor ","permalink":"https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/","summary":"\u003cp\u003eIn this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.\u003c/p\u003e\n\u003ch2 id=\"1-node-exporter\"\u003e1. Node exporter:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNode exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"node-exporter-total\" loading=\"lazy\" src=\"/node-exporter-total.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAlthough 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like \u003ca href=\"https://grafana.com/products/cloud/\"\u003eGrafana cloud\u003c/a\u003e, \u003ca href=\"https://aws.amazon.com/prometheus/\"\u003eAWS Prometheus\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud for Prometheus\u003c/a\u003e, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent \u0026amp; stored.\u003c/li\u003e\n\u003cli\u003eIt’s not necessary that you should cut down on metric scraping if you’re sending metrics to any of the vendors mentioned above.\u003c/li\u003e\n\u003cli\u003eThis also implies to local storage of Prometheus data, since too many time series over time can hinder Prometheus performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"whats-a-collector-\"\u003eWhat’s a collector ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMain components of a node are referred to as collector, for example CPU, file-system, memory etc.\u003c/li\u003e\n\u003cli\u003eEach collector exposes a set of metrics about the component it covers. \u003ca href=\"https://github.com/prometheus/node_exporter\"\u003eHere’s\u003c/a\u003e the list of collectors that are enabled by default.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eDisable collectors\u003c/strong\u003e :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eA collector can be disabled by providing the flag : \u003ccode\u003e--no-collector.\u0026lt;collector-name\u0026gt;\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#the command will look like this : \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enode_exporter --no-collector.nfs\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003e\u003cstrong\u003eDisable self metrics of node-exporter :\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eNode-exporter exposes \u003cem\u003e~ 80\u003c/em\u003e metrics about itself at \u003ccode\u003e/metrics\u003c/code\u003e along with node metrics.\u003c/li\u003e\n\u003cli\u003eThe metrics about node-exporter starts with prefix \u003ccode\u003epromhttp_*, process_*, go_*\u003c/code\u003e. Below is the list of some of them :\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_cpu_seconds_total\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_max_fds\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_open_fds\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_resident_memory_bytes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_start_time_seconds\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_virtual_memory_bytes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprocess_virtual_memory_max_bytes\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epromhttp_metric_handler_errors_total\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epromhttp_metric_handler_requests_in_flight\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epromhttp_metric_handler_requests_total\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026amp; around \u003cspan style=\"color:#ae81ff\"\u003e68\u003c/span\u003e go based metrics.\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eYou can disable all the above ~80 metrics by running node-exporter with flag \u003ccode\u003e-web.disable-exporter-metrics\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#the command will look like this : \u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enode_exporter --web.disable-exporter-metrics\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003e\u003cstrong\u003eEnable only the collectors required\u003c/strong\u003e :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eOpposite to disabling certain collectors, Node exporter has a flag \u003ccode\u003e--collector.disable-defaults\u003c/code\u003e which disables all collectors at once.\u003c/li\u003e\n\u003cli\u003eCombining that flag with the collector of your choice will only collect the metrics of the collectors you enabled and discard everything else.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eFor example\u003c/em\u003e, If you want to collect only the CPU and Memory metrics of a node, you can run the below command :\u003c/p\u003e","title":"How to prevent metrics explosion in Prometheus"},{"content":"Keeping your Prometheus optimized can be a tedious task over time, but it\u0026rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.\nIdentifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage \u0026amp; performance intact.\nIn this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.\nIdentifying the costly metrics : There are 3 ways in which you can get the top 10 costly metrics which are consuming your TSDB :\nVia promtool: Promtool is bundled with Prometheus, if you’re running Prometheus in Kubernetes, then you can just exec into the pod and run the below command. If you’re running Prometheus in VMs, the Promtool binary will be in the same directory as Prometheus. Command : promtool tsdb analyze path-to-data-directory/.\n/prometheus $ promtool tsdb analyze /data/ Block ID: 01FYV026P5AYM47XSSFT4WVG6X Duration: 1h59m59.999s Series: 173756 Label names: 321 Postings (unique label pairs): 18485 Postings entries (total label pairs): 3000877 Label pairs most involved in churning: 32894 namespace=default 14131 job=kubernetes-nodes-cadvisor 13606 job=kubernetes-service-endpoints Label names most involved in churning: 35344 __name__ 35022 instance 35022 job 34345 namespace 28503 pod 15626 container Most common label pairs: 82253 namespace=default 54338 job=linkerd-proxy 54338 control_plane_ns=linkerd 52074 job=kubernetes-service-endpoints 51637 app_kubernetes_io_managed_by=Helm 41292 kubernetes_io_os=linux Label names with highest cumulative label value length: 187875 id 116231 name 83663 __name__ 67642 path 67233 container_id 41271 image 31047 image_id 25053 filename 23436 uid 21800 pod Highest cardinality labels: 2227 __name__ 1846 id 1369 name 921 container_id 844 replicaset 733 owner_name Highest cardinality metric names: 20280 response_latency_ms_bucket 10140 route_response_latency_ms_bucket 5076 etcd_request_duration_seconds_bucket 3380 control_response_latency_ms_bucket 2079 stack_poll_total 1893 container_memory_usage_bytes 1893 container_memory_working_set_bytes 1893 container_cpu_user_seconds_total 1893 container_memory_max_usage_bytes 1893 container_memory_rss 1893 container_cpu_system_seconds_total 1745 tcp_open_total 1745 tcp_read_bytes_total 1745 tcp_open_connections 1745 tcp_write_bytes_total 1700 container_cpu_usage_seconds_total 1474 rest_client_request_latency_seconds_bucket 1424 route_request_total 1295 kube_pod_container_resource_requests 1260 http_client_request_latency_seconds_bucket Via /tsdb-status endpoint: Prometheus already provides outputs of the above tsdb analyze command in easy to understand manner at prometheus-example.com/tsdb-status endpoint.\nThere are more details about the label pairs etc. on that endpoint, but for the scope of this article, we’ll focus on the top 10 series count by metric name. Via PromQL query: If you’re more fond of PromQL to get things done, the above result can also be achieved by running the PromQL query : topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;})) From the above pointers, now we know what are our costly metrics which we need to be aware of. Let’s assume we decided that we want to drop prometheus_http_request_duration_seconds_bucket \u0026amp; prometheus_http_response_size_bytes_bucket metric, as it\u0026rsquo;s of no practical use to us.\n1. Drop In order to drop the above-mentioned metrics, we need to add metric_relabel_configs in Prometheus scrape config with the metric name we need to drop :\nscrape_configs: - job_name: \u0026#34;prometheus\u0026#34; scrape_interval: 15s static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: \u0026#39;(prometheus_http_request_duration_seconds_bucket|prometheus_http_response_size_bytes_bucket)\u0026#39; action: drop complete flow will be like :\nmetric_relabel_configs : metric relabeling process starts once the metrics is scraped. The reason for that is, the main label __name__ will be generated post scraping.\nsource_labels: we’re utilizing the label __name__ to get the desired metric, since this itself is a label which has value of metric name. i.e __name__=prometheus_http_request_duration_seconds_bucket\nIf you have lots of metrics which you need to drop, a better approach would be to use the action “keep” and mention the metric names you need to keep and everything else will be dropped.\n2. Keep As mentioned above, let’s try the permissive approach of Prometheus. Here we will be only keeping metric prometheus_http_requests_total and drop everything else. The config looks like this : scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; scrape_interval: 5s static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: prometheus_http_requests_total action: keep Gotchas Once you have dropped the metric of choice, new samples won\u0026rsquo;t get stored for that metric. When a metric is not receiving any samples for ~5 min, it will be considered as a stale metric. You can read more about staleness here. Depending on your retention period, the old time series will be removed once it reaches the retention, by default it\u0026rsquo;s 15 days. However, if you want to delete the stored metrics to clear up space, you can follow the below steps to achieve that. Deletion of single metric: In order to delete a series based on label, first you need to enable Admin API. You can enable the flag -web.enable-admin-api to do that. The complete command will look like this : ./prometheus --web.enable-admin-api Send a POST request with the label selectors of your choice to delete the resulting time series from starting date till today, i.e, everything for that time series. For example, the below will delete prometheus_http_requests_total from all scrape job (if there were multiple).\ncurl -X POST -g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=prometheus_http_requests_total\u0026#39; In case you need to delete a metric from a specific scrape job, you can mention the job label.\n-g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=prometheus_http_requests_total{job=\u0026#34;prometheus\u0026#34;}\u0026#39; The above step will mark the time series for deletion, but won\u0026rsquo;t remove them from the disk at that moment. They will get flushed in future compaction. If you need to remove them from the disk to clear the space instantly, you can hit the tombstone endpoint to purge the time series from the disk : curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones Deletion of multiple metrics: If you’re cleaning / optimizing your Prometheus TSDB, deleting a single metric won\u0026rsquo;t do anything. Hence, deletion of 100s if not 1000s of unnecessary time series is evident. At of the time writing this article, you can\u0026rsquo;t delete n metrics which are matched by regular expression. I found a workaround for that via shell scripting for multiple metric deletion at a single shot. Example : There are around 66 go based metrics which are available at Prometheus /metrics endpoint.\nIf you want to delete all of them, then you can perform the below steps, since regex based deletion is not possible as of now. The below command will grab all the metrics which starts with go_ . # get all metric name with go_* into a txt file. curl http://localhost:9090/api/v1/label/__name__/values | tr \u0026#39;,\u0026#39; \u0026#39;\\n\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | grep \u0026#39;^go_.*\u0026#39; \u0026gt; prom_go_metrics.txt The metrics collected in file will look like : cat prom_go_metrics.txt | head -n 5 go_gc_cycles_forced_gc_cycles_total go_gc_cycles_total_gc_cycles_total go_gc_duration_seconds go_gc_duration_seconds_count go_gc_duration_seconds_sum Create a bash script delete-metrics.sh, append the below contents and run it : #shell script to go over each metric in the file and request the API for deletion recursively. #!/bin/bash for i in `cat prom_go_metrics.txt`; do curl -X POST -g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=\u0026#39;$i\u0026#39;\u0026#39; done If the deletion is successful, the response will be empty. Once the deletion is successful, use Promtool to query metrics via CLI or check via web UI to see if the metric returns any value. The response should be empty. promtool query series http://localhost:9090 --match=go_gc_heap_allocs_by_size_bytes_total References :\nhttps://prometheus.io/docs/prometheus/latest/querying/api/#delete-series https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs https://relabeler.promlabs.com/ ","permalink":"https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/","summary":"\u003cp\u003eKeeping your Prometheus optimized can be a tedious task over time, but it\u0026rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.\u003c/p\u003e\n\u003cp\u003eIdentifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage \u0026amp; performance intact.\u003c/p\u003e\n\u003cp\u003eIn this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.\u003c/p\u003e","title":"How to drop and delete metrics in Prometheus"},{"content":"In this article, let\u0026rsquo;s try to estimate the Prometheus storage required for an environment.\nPrometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests \u0026amp; stores will increase too leading to disk space issues.\nFrom the docs:\nPrometheus stores an average of only 1-2 bytes per sample. Thus, to plan the capacity of a Prometheus server, you can use the rough formula :\nneeded_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample By default, the metrics retention period is 15 days. In this case, let’s assume you want to keep the metrics for 1 Month. To figure out the above formula, let\u0026rsquo;s check our Prometheus to understand the parameters which are mentioned above.\nHow many samples are stored/ingested This metric is exposed by the Prometheus server which represents a total number of appended samples i.e stored metric samples.\nTo get an average number of samples stored in Prometheus, you can run :\n(rate(prometheus_tsdb_head_samples_appended_total[1d]) Which looks like this:\nWe can see that around 15k metrics are stored in the Prometheus.\nCalculating byte per sample : To calculate, what’s the average size of each sample ingested, we can run the below query :\n(rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d]) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d])) Which is around 1.7 byte / sample in our case :\nPutting it all together : 2592000* (rate(prometheus_tsdb_head_samples_appended_total[1d]) * (rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d]) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]))) 2592000 : the retention period in seconds, in this case 30 days.\nThis gives us the value: 47335001801 bytes = 47.33GB\nTo lower the rate of ingested samples, you can either reduce the number of time series you scrape (fewer targets or fewer series per target), or you can increase the scrape interval. However, reducing the number of series is likely more effective, due to the compression of samples within a series. In case you want to play this around a bit, Grafana is generous enough to give a playground, you can run the queries there.\nReference :\nhttps://prometheus.io/docs/prometheus/latest/storage/#storage\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/","summary":"\u003cp\u003eIn this article, let\u0026rsquo;s try to estimate the Prometheus storage required for an environment.\u003c/p\u003e\n\u003cp\u003ePrometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests \u0026amp; stores will increase too leading to disk space issues.\u003c/p\u003e\n\u003cp\u003eFrom the docs:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePrometheus stores an average of only 1-2 bytes per sample. Thus, to\nplan the capacity of a Prometheus server, you can use the rough formula :\u003c/p\u003e","title":"How to calculate the storage space required for Prometheus server"},{"content":"This article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.\nDefinition : Readiness Probe in Kubernetes is a probing mechanism to detect health (ready status) of a pod and if the health is intact, then allow the traffic to the pod.\nFrom the official doc,\nSometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don\u0026rsquo;t want to kill the application, but you don\u0026rsquo;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.\nHence, to detect readiness probe failure of our apps, lets configure an alert in Prometheus.\nPrerequisites : Prometheus\nkube-state-metrics\n1. Constructing PromQL expression: All Prometheus queries / expressions are written in a query language called PromQL. You can read more about how to write promQL queries here.\nThe main metrics which gives the status of readiness probe of a pod is : kube_pod_status_ready. The metric has a condition label whose value can be false or true. For the testing of this metric, let\u0026rsquo;s create a Kubernetes deployment with a incorrect readiness probe :\napiVersion: apps/v1 kind: Deployment metadata: name: readiness-test labels: app: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 readinessProbe: httpGet: host: scheme: HTTP path: / httpHeaders: - name: Host value: example.com port: 50 The complete promQL expression will look like this :\nsum by(pod)( kube_pod_info{created_by_kind!=\u0026#34;Job\u0026#34;} AND ON (pod, namespace) kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1) Result :\nExplanation : The below query will filter out all the pods which has failed readiness status. kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1 However, the above also returns the pod names which are in completed state, i.e. from a CronJob. To filter them out, we can use kube_pod_info metrics which got a label created_by_kind!=\u0026quot;Job\u0026quot;, we\u0026rsquo;ll use that to filter in all pods which are not from a cronJob. finally we\u0026rsquo;re aggregating the filtered values with label pod \u0026amp; namespace using function sum(). Alerting: To create a new alert in Prometheus, update the below config values in Prometheus.yml file.\n- alert: KubePodReadinessFailure annotations: description: Readiness probe for the Pod {{ $labels.pod }} is failing for last 10 minutes expr: sum by(pod)( kube_pod_info{created_by_kind!=\u0026#34;Job\u0026#34;} AND ON (pod, namespace) kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1) \u0026gt; 0 for: 10m labels: severity: warning The above alert will wait for 10 min to trigger to filter out misfire and returns the name of the pod which is facing the failure. References :\npod-metrics alerting_rules ","permalink":"https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/","summary":"\u003cp\u003eThis article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.\u003c/p\u003e\n\u003ch3 id=\"definition-\"\u003eDefinition :\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eReadiness Probe\u003c/strong\u003e in Kubernetes is a probing mechanism to detect health (ready status)  of a pod and if the health is intact, then allow the traffic to the pod.\u003c/p\u003e\n\u003cp\u003eFrom the official doc,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don\u0026rsquo;t want to kill the application, but you don\u0026rsquo;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.\u003c/p\u003e","title":"How to configure Readiness Probe alert in Prometheus"},{"content":"Who will monitor the monitoring system ? Itself\u0026hellip;\u0026hellip;\u0026hellip;sounds a bit magical.\nSince Prometheus monitors everything, it\u0026rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.\nIf Prometheus goes down, you won\u0026rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!\nConfiguring Prometheus to monitor itself Prometheus exposes metrics about itself at /metrics endpoint, hence it can scrape and monitor its own health.\nAdd a Prometheus scrape job in prometheus.yml config file : scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # prometheus endpoint address Restart the Prometheus \u0026amp; curl the /metrics endpoint of Prometheus server to verify: curl http://localhost:1256/metrics # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 7.3633e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 9.2295e-05 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000100231 go_gc_duration_seconds{quantile=\u0026#34;0.75\u0026#34;} 0.000110334 go_gc_duration_seconds{quantile=\u0026#34;1\u0026#34;} 0.001204485 go_gc_duration_seconds_sum 43.914716791000004 go_gc_duration_seconds_count 191769 Looks good, let’s move ahead.\nTime Series Prometheus fundamentally stores all data as time series streams of timestamped values belonging to the same metric and the same set of labeled dimensions.\nUnderstanding valuable metrics 1. Active time Series count : prometheus_tsdb_head_series metric type : Gauge\nThis metric shows total number of active time series in the head block. A time series is considered active if new data points have been generated within the last 15 to 30 minutes. A head block in TSDB is an in-memory part of database where time series are stored for a shorter period and then later flushed to persistent disk storage. Read more about how head block works in Prometheus TSDB here. Total Available series in head block can be calculated using the below expression : max_over_time(prometheus_tsdb_head_series[1d]) 2. Time Series Created : prometheus_tsdb_head_series_created_total metric type: Counter\nThis metrics displays total number of time series created in the head block. Since it’s a counter, we can calculate its rate with expression : rate(prometheus_tsdb_head_series_created_total[5m]) 3. Time Series Deleted : prometheus_tsdb_head_series_removed_total metric type: Counter\nThis metrics displays total number of time series removed in the head block. To calculate, rate of deletion of time series with per second average, you can run : rate(prometheus_tsdb_head_series_removed_total[5m]) As you can see from the above graph, the time series are removed from the head and are flushed to persistent storage around every 2 hours. Samples: A sample is a combination of timestamp, value of a time series metric. For example, memory used by container A at time 1:50 \u0026amp; 1:51. These are two samples for the same metric or same time series. 4. Samples ingested : prometheus_tsdb_head_samples_appended_total metric type: Counter\nThis metric shows total number of appended samples into the head block. This metrics is different from the metric scrape_samples_scraped because the appended metric shows the number of samples added / ingested into head block, whereas samples_scraped shows how many samples were scraped. The difference comes into play when you are dropping a lot of metrics via relabel config in your Prometheus config. Sample ingestion rate can be calculated with the expression : rate(prometheus_tsdb_head_samples_appended_total[5m]) 5. Sample size : metric type: Gauge\nThe below expression will give the size of the sample ingested by the Prometheus. In an ideal scenario, the size of each sample will be around 1-2 byte. Sample size monitoring is essential because if there’s anomaly and size goes beyond 3-4 byte, Storage of the server will wear out quickly, leading to disaster. Size of sample can be calculated using the below : (rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d])) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]) 3. Samples scraped per job : scrape_samples_scraped metric type: Counter\nOver time, you might need to keep an eye on which job is contributing to the highest samples getting scraped and ingested. For that, the above metric comes to the rescue. In an easier way, this metric displays how many metrics are scraped by a job. Hence, you can write a promQL expression to configure an alert too if it breaches a certain threshold. Total samples scraped / job can be calculated by using the below expression : sum by (job)(scrape_samples_scraped) 3. Scrape duration : prometheus_target_interval_length_seconds metric type: Gauge\nYou might also need to monitor the scrape duration heath of your Prometheus. If the duration goes beyond a certain threshold value, the samples will get out of order. More details \u0026amp; debugging out of order samples here. P99 of the scrape duration can be calculated using the below expression :\nprometheus_target_interval_length_seconds{quantile=\u0026#34;0.9\u0026#34;} I’ve listed above some important metrics you can look for, there’s lot more of them at /metrics endpoint of your Prometheus server.\nGrafana Dashboard For quicker insights, I’ve made a Grafana dashboard from all the above expressions mentioned.\nWhich looks like this :\nYou can import the JSON file from here to your Grafana instance to get started.\nReferences : https://www.omerlh.info/2019/03/04/keeping-prometheus-in-shape/\nhttps://prometheus.io/docs/prometheus/latest/getting_started/#configuring-prometheus-to-monitor-itself\nhttps://ganeshvernekar.com/blog/prometheus-tsdb-the-head-block/\n","permalink":"https://tanmay-bhat.github.io/posts/prometheus-self-metrics/","summary":"\u003cp\u003eWho will monitor the monitoring system ? \u003cem\u003eItself\u003c/em\u003e\u0026hellip;\u0026hellip;\u0026hellip;sounds a bit magical.\u003c/p\u003e\n\u003cp\u003eSince Prometheus monitors everything, it\u0026rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.\u003c/p\u003e\n\u003cp\u003eIf Prometheus goes down, you won\u0026rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!\u003c/p\u003e\n\u003ch3 id=\"configuring-prometheus-to-monitor-itself\"\u003e\u003cstrong\u003e\u003cstrong\u003eConfiguring Prometheus to monitor itself\u003c/strong\u003e\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003ePrometheus exposes metrics about itself  at \u003ccode\u003e/metrics\u003c/code\u003e endpoint, hence it can scrape and monitor its own health.\u003c/p\u003e","title":"Self monitoring Prometheus with Grafana"},{"content":"What\u0026rsquo;s on Image Updater A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.\nCapabilities : Argo CD Image Updater can check for new versions of the container images that are deployed with your Kubernetes workloads and automatically update them to their latest allowed version using Argo CD. It works by setting appropriate application parameters for Argo CD applications, i.e. similar to argocd app set --helm-set image.tag=v1.0.1 - but in a fully automated manner. Prerequisite : Kubernetes Cluster ArgoCD setup Steps Installation of ArgoCD Image Updater To install argocd image updater in your cluster ( same one as argocd), run the below command: kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml Once it\u0026rsquo;s installed, let’s check the logs of the pod: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-image-updater time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Starting image update cycle, considering 0 annotated application(s) for update\u0026#34; time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=0 images_considered=0 images_skipped=0 images_updated=0 errors=0\u0026#34; Looks clean, let\u0026rsquo;s move forward.\nConfigure Remote Repository Secret One of the main features of image updater is to write back / update the new image tag to the remote git repo i.e. it will update the image tag, for example, v1.2.3 in the manifest file every time a new image is pushed to image registry. I’ll be using Gitlab in this case. Feel free to use Github also. Obtain the Access token of your account from Gitlab . Once received, run the below command to create the secret : kubectl --namespace argocd \\ create secret generic gitlab-token \\ -from-literal=username=GITLAB_USERNAME \\ --from-literal=password=GITLAB_TOKEN Verify the secret that has been created, by running the below command : kubectl describe secret gitlab-token -n argocd Configure Container registry secret I’m using DigitalOcean Registry as my cloud provider for storing docker images. There are two ways to configure the secret that Kubernetes will use to authenticate to registry endpoint i.e registry.digitalocean.com. Automatic: Follow the on-screen instructions on the DigitalOcean console to auto-integrate the Kubernetes secret into every namespace. Manual: Download the Docker Credentials and follow steps on this link to manually create a Kubernetes secret and then patch to service account of your namespace, argocd in this scenario. Configure Image updater to watch for Image updates. Image updater needs access to container registry to watch the Image updates.\nOut of the box below Registries are supported :\nDocker Hub Registry Google Container Registry (gcr.io) RedHat Quay Registry (quay.io) GitHub Docker Packages (docker.pkg.github.com) GitHub Container Registry (ghcr.io) If you’re storing images in any other registry other than the above-mentioned, no need to worry, we can utilize the config map to add the registry configs.\nTo Digital Ocean configs, Add the below to configmap : argocd-image-updater-config in argocd namespace:\nregistries: - name: \u0026#39;Digital Ocean\u0026#39; api_url: https://registry.digitalocean.com ping: no credentials: pullsecret:argocd/SECRET_NAME ( configured from above step ) defaultns: library prefix: registry.digitalocean.com Here the credentials are in this order : credentials:pullsecret:namespace/secret-name\nThere are other forms of secret configuration as well. Please refer to them here.\nConfigure ArgoCD Application and Annotation Once the Gitlab secret is configured, let\u0026rsquo;s move to the application configuration part. Create the Application manifest file for your application. Here’s a sample definition for kube-ops-view application : Let’s understand the above file in detail :\nWe’re installing this resource in argocd namespace. ( this is mandatory for argocd resources) Annotations : image-list: tells the image updater, which docker image to watch \u0026amp; update. write-back-method: is updating the image tag back to manifest files via git commit git-branch: we’ll write back to the branch: main. server: we’re deploying this to the default cluster. namespace: the actual namespace in which the application will be deployed. path : the path of your helm chart inside the remote git repository. targetRevision: the branch name to which argocd syncs apps. Since we’re using helm, I’m updating to use values from values.yaml file. CreateNamespace: if the mentioned namespace is not found, argocd will create it. Once we’re good with the above configurations, to see the image updater in action\u0026hellip;\nPush a new image tag to your container registry : \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB \u0026gt; docker tag registry.digitalocean.com/tanmaybhat/kube-ops-view:latest registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.4 a645de6a07a3 21 months ago 253MB \u0026gt; docker push registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 Once the image is pushed, let\u0026rsquo;s check the logs of image updater pod logs, it should look like this:\ntime=\u0026#34;2022-01-18T13:09:11Z\u0026#34; level=info msg=\u0026#34;git push origin main\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 execID=mvIL8 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=Trace args=\u0026#34;[git push origin main]\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 operation_name=\u0026#34;exec git\u0026#34; time_ms=2058.666776 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=\u0026#34;Successfully updated the live application spec\u0026#34; application=kube-ops-view-demo time=\u0026#34;2022-01-18T13:09:14Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=1 images_considered=1 images_skipped=0 images_updated=1 errors=0 Here’s what image updater did :\nDetected a new image tag update in the registry Cloned the repository Made the image update in the manifest file Pushed a commit back to the repository main branch For further verification, let\u0026rsquo;s go to the repo where the helm chart exists, we should be seeing a new commit :\nLet’s see what changed in the file :\nNow, all we have to do is wait for 3 min ( default sync period of argocd), and argocd notices that a new image tag has been updated and the same will be applied to the application.\nLet’s check the image tag as well :\nkubectl describe deploy kube-ops-view -n kube-ops-view | grep Image Image: registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 ","permalink":"https://tanmay-bhat.github.io/posts/getting-started-with-argocd-image-updater/","summary":"\u003ch2 id=\"whats-on-image-updater\"\u003eWhat\u0026rsquo;s on Image Updater\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA tool to automatically update the container images of Kubernetes workloads\nthat are managed by \u003ca href=\"https://github.com/argoproj/argo-cd\"\u003eArgo CD\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"capabilities-\"\u003eCapabilities :\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eArgo CD Image Updater can check for new versions of the container images\nthat are deployed with your Kubernetes workloads and automatically update them\nto their latest allowed version using Argo CD.\u003c/li\u003e\n\u003cli\u003eIt works by setting appropriate\napplication parameters for Argo CD applications, i.e. similar to\n\u003ccode\u003eargocd app set --helm-set image.tag=v1.0.1\u003c/code\u003e - but in a fully automated\nmanner.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"prerequisite-\"\u003ePrerequisite :\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eKubernetes Cluster\u003c/li\u003e\n\u003cli\u003eArgoCD setup\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\n\u003ch3 id=\"installation-of-argocd-image-updater\"\u003eInstallation of ArgoCD Image Updater\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTo install argocd image updater in your cluster ( same one as argocd), run the below command:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eOnce it\u0026rsquo;s installed, let’s check the logs of the pod:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ekubectl logs -n argocd -l app.kubernetes.io/name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eargocd-image-updater\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etime\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;2022-01-16T06:21:00Z\u0026#34;\u003c/span\u003e level\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003einfo msg\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Starting image update cycle, considering 0 annotated application(s) for update\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etime\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;2022-01-16T06:21:00Z\u0026#34;\u003c/span\u003e level\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003einfo msg\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Processing results: applications=0 images_considered=0 images_skipped=0 images_updated=0 errors=0\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLooks clean, let\u0026rsquo;s move forward.\u003c/p\u003e","title":"ArgoCD Image Updater with Digital Ocean Container Registry"},{"content":"Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\nThere are other solutions that grew out of Prometheus for ex Thanos or Cortex.\nI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\nBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring. Here’s how they explain:\nPulling over HTTP offers several of advantages:\nYou can run your monitoring on your laptop when developing changes. You can more easily tell if a target is down. You can manually go to a target and inspect its health with a web browser. Overall, we believe that pulling is slightly better than pushing, but it should not be considered a major point when considering a monitoring system.\nOne of the main problem where pull based mechanism won\u0026rsquo;t work is when\nPrometheus cannot directly reach the server to scrape metrics from it.\nFor this problem, push based mechanism comes to play. A bit about it :\nWith a pull model, it is straightforward to determine whether a node is available using an up metric with a value of 1 when the target is reachable and 0 when it is not. With the push model, the up metric has a value of 1 when the server is running and no value at all when it is not. This distinction is important when monitoring whether your monitoring system is running as expected. Solution : Grafana agent + Remote write 1. Configuring Prometheus as remote receiver endpoint As of Prometheus v2.33.3, this feature is supported, and you can pass flag -web.enable-remote-write-receiver and your server endpoint example.com/api/v1/write will accept remote metrics. Here’s how the config looks if you\u0026rsquo;re running Prometheus in Kubernetes : - name: prometheus-server image: quay.io/prometheus/prometheus:v2.33.3 args: - \u0026#39;--storage.tsdb.retention.time=15d\u0026#39; - \u0026#39;--config.file=/etc/config/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/data\u0026#39; - \u0026#39;--web.console.libraries=/etc/prometheus/console_libraries\u0026#39; - \u0026#39;--web.console.templates=/etc/prometheus/consoles\u0026#39; - \u0026#39;--web.enable-lifecycle\u0026#39; - \u0026#39;--web.enable-remote-write-receiver\u0026#39; 2. Configuring Grafana Agent Grafana Agent is actually Prometheus lite 😛. It just collects and pushes metrics to a remote server.\nNote: I’ll be running Grafana Agent in docker, but there wont be much change to running it via systemd.\nFirst would be to create the agent configuration file ( agent.yaml) : server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 30s external_labels: environment: test-server configs: - name: default scrape_configs: - job_name: agent static_configs: - targets: [\u0026#39;grafana-agent:12345\u0026#39;] - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;node_exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] remote_write: - url: https://example.com/api/v1/write basic_auth: username: admin password: secret-password This config is almost identical to a regular Prometheus configuration file. scrape_interval : Time interval at which agent should scrape / collect the metrics. external_labels : Label which you can add for all metrics agent sends to Prometheus for easier identification and analysis later ( key-pair) . We have 3 scrape Jobs in scrape_configs . Each Job tells Prometheus what \u0026amp; where to scrape. remote_write is where the magic happens, its the location where agent should send the metrics it collected. Basic_auth : Basic Authentication for authenticating with the /api/v1/write endpoint. Basic Auth is not a Mandatory Option, but it sure is necessary, else you’re endpoint will be open to public !!! Prometheus needs to be configured with Basic auth initially. Please follow this doc to set it up. Once the config file is completed, you can use the below Docker-compose file to get started with the Grafana agent + cadvisor ( container metrics) + node-exporter ( machine metrics) :\nversion: \u0026#34;3\u0026#34; services: grafana-agent: image: grafana/agent:v0.23.0 container_name: grafana-agent volumes: - /path/to/data/:/etc/agent/data - /path/to/agent.yaml:/etc/agent/agent.yaml node-exporter: image: prom/node-exporter:v1.3.1 container_name: node_exporter command: - \u0026#39;--path.rootfs=/host\u0026#39; network_mode: host pid: host volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; cadvisor: image: gcr.io/cadvisor/cadvisor container_name: cadvisor volumes: - /sys:/sys:ro - /:/rootfs:ro - /var/run:/var/run:rw - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro For Grafana Agent, the volume folder needs to be created before running the docker-compose up command. Grafana agent needs data folder because in-case the agent container restarts, it can resend the metrics which couldn\u0026rsquo;t be sent earlier. It stores metrics up-to last 1 hour. in-case you think it’s too much you can configure metrics DROP for the time series which are not needed in Grafana config file itself Doc. Once both of the above steps are complete, you can check if metrics are being pushed to your Prometheus server by running the below queries :\nFor Node metrics, run the PromQL query :\nrate(node_cpu_seconds_total{environment=\u0026#34;test-server\u0026#34;}[5m]) which should look like : For Container metrics, run the PromQL query :\ncontainer_memory_usage_bytes{environment=\u0026#39;test-server\u0026#39;,name=\u0026#39;grafana-agent\u0026#39;} Now it\u0026rsquo;s up to the DevOps engineer to write useful PromQL queries, Create Grafana dashboards and configure alerts for the same to make use of these metrics.\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/","summary":"\u003ch2 id=\"push-vs-pull\"\u003ePush vs Pull\u003c/h2\u003e\n\u003cp\u003ePrometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\u003c/p\u003e\n\u003cp\u003eThere are other solutions that grew out of Prometheus for ex \u003ca href=\"https://thanos.io/\"\u003eThanos\u003c/a\u003e or \u003ca href=\"https://cortexmetrics.io/\"\u003eCortex\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\u003c/p\u003e\n\u003cp\u003eBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring. Here’s how they explain:\u003c/p\u003e","title":"How to configure Prometheus server as a remote receiver"},{"content":"What\u0026rsquo;s Apps of Apps or Cluster bootstrapping ? The App of Apps Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.\nArgoCD will watch the root application and synchronize any applications that it generates.\nRequirement You may have this question as to why is this even needed, where as you can manaully create the application via argo cli or via argo UI. But once the number of application you manage increases, automation needs to be done. For example, if you have a new app that needs to be created \u0026amp; deployed in your cluster via ArgoCD, apart from argo CLI and UI ( both are manual ) we really dont have any automation on creation of apps except custom scripting. Behold : Apps of apps Actually, it took me a while to understand what exactly it is because the official doc is not well written.\nSince the concept is pretty new and as far as I know, not much of argocd users are utilising this.\nNow that we know why, what lets go over to how.\nStructure Components Root app : the single app you need to deploy to your cluster ( parent/root) child app : your actual microservice applications ( child) child app template : argocd Application kind template describing your application application manifests : the actual micro-service definitions. High level flow\nGit repo → root app → application templates → actual applications\nPrerequisites\nKubernetes cluster Helm v3 ArgoCD installed Demonstration For the purpose of demonstration, here are the components described above :\nroot app template : root.yaml application templates : manifests application definitions : charts/monitoring Let’s take a look at root.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: root-app namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: default project: default source: path: ./apps-of-apps/manifests/ repoURL: https://gitlab.com/Tanmay-Bhat/argocd.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true Name : suggests the name of my root application i.e root-app namespace : all argocd based resources should reside in argocd namespace itself Finaliser : its the kubernetes CRD finalizer which help in protection from accidental deletion of resources. Destination : the cluster onto which the resources should deploy. If you have multiple clusters , mention the required one here. namespace: the actual namespace in which you want to deploy your app path : path inside your Git Repo where the application manifests are. repoURL : the GIT repo address. targetRevision: if its HEAD, argocd will always sync to master branch, if you need to be a different one, update that here. For example, staging branch for Dev environment etc. prune : this enables argocd to auto remove your application and its created resources when you delete the manifest file from the Application templates directory. selfHeal : this enables argocd to auto patch the configs same as latest master branch changes even though new config updates has been done via kubectl. Let’s look at one of the actual application template inside manifests, kube-state-metrics.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: kube-state-metrics namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: kube-system project: default source: path: ./Applicationset/charts/monitoring/kube-state-metrics repoURL: https://github.com/tanmay-bhat/ArgoCD.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true name: name of my application namespace : I’m deploying the app to kube-system namespace. path: the path inside GIT repo where hem charts are for the app targetRevision: I’m synching the changes to master branch. repoURL : repo where the helm charts / manifests of app is stored. Once, we understand the above, we can proceed to try them out :\nInstall the root app by running :\nkubectl apply -f root.yaml -n argocd\nOnce installed, go to your ArgoCD UI and you should be able to see:\nNow that our root app is created and healthy, give it a minute and any application template you create inside manifests folder (in my case), will be automatically synced and created for you in the specified namespace.\nIn the above picture, you can see root app is managing multiple apps and auto-syncs when you make changes to the helm charts (github repo in my case)\nThere’s a little arrow in each of the app, if you click on it, it will take you to the applciation page of it in argocd : If you need to add a new application, the flow goes like this, add the actual helm chart/manifest file to your GIT repo → add an argocd template to your templates directory → sit back and enjoy !!\nI’ve added the argocd template and bam !! app is automatically created : Problems Synching apps across multiple clusters. Lets say you want to have node-exporter in multiple clusters, its not possible via apps-of-apps directly. For that you can create a application template and in destination update your cluster name as registred in argocd. This problem is solved by Aplicationset from ArgoCD. Support for multiple values.yaml file for helm charts. Let’s say you want to apply node-selector to 2 clusters dev and prod each with their own customized values which is not possible at the moment. ","permalink":"https://tanmay-bhat.github.io/posts/2022-01-11-introduction-to-argocd-apps-of-apps/","summary":"\u003ch3 id=\"whats-apps-of-apps--or-cluster-bootstrapping-\"\u003eWhat\u0026rsquo;s Apps of Apps  or Cluster bootstrapping ?\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/\"\u003eThe App of Apps\u003c/a\u003e Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder  which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.\u003c/p\u003e","title":"Introduction to ArgoCD : apps of apps"},{"content":"You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster. Before we begin, here are the prerequisites :\nKubernetes cluster Cluster autoscaler Bit of patience Usecase : One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra. You maybe hosting workload in Kubernetes where you wont get traffic post business hours. Or in weekends, you just want to scale down as no traffic flows to your apps during that time. The cost to keep those worker nodes at off hours are pretty high if you calculate for a quarter or for a year. Solution : Though there isn\u0026rsquo;t any one click solution, Kubernetes finds a way or always Kubernetes Admin does !!\nStrangely, there isn\u0026rsquo;t any tool out of the BOX from AWS side, heck not even a blog on how can customers achieve that. Ahem, GCP aced in this scenario.\nBehold \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nKube downscaler : Kube downscaler is a FOSS project by Henning Jacobs who’s the creator of famous project kube-ops-view.\nThis project fits exactly to our requirement as it can scale down below resources in specified timeframe :\nStatefulsets Deployments (HPA too) Cronjobs Installation : Clone the repository: git clone https://codeberg.org/hjacobs/kube-downscaler Update the configmap file deploy/config.yaml to your Cluster TIMEZONE and desired uptime, here’s mine : apiVersion: v1 kind: ConfigMap metadata: name: kube-downscaler data: # timeframe in which your resources should be up DEFAULT_UPTIME: \u0026#34;Mon-Fri 09:30-06:30 Asia/Kolkata\u0026#34; Apply the manifest files : kubectl apply -f deploy/ Working and configuration: As soon as the downscaler pod runs, you can check the logs of it, it should look like ‘Scale down deployment/myapp to replicas 0 (dry-run)`\nAs a safety-plug no scaling operations will happen when the pod starts as --dry-run argument is enabled. Remove that by patching the deployment to start the scaling activity:\nkubectl patch deployment kube-downscaler --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/args\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;--interval=60\u0026#34; ]}]\u0026#39; Once, dry-run argument is removed, all the resources ( deployment, Statefulset \u0026amp; cronjob) wil be scaled down to 0 ( default replica) if current_time ≠ default_uptime mentioned in above mentioned configmap.\nIncase you need to exclude any app from being scaled down, you can annotate that depoyment/statefulset/cronjob with :\nkubectl annotate deploy myapp \u0026#39;downscaler/exclude=true\u0026#39; If you want to have minimum 1 replica after scale down activity, you can annotate the resource : kubectl annotate deploy myapp \u0026#39;downscaler/downtime-replicas=1\u0026#39; Note : No need to annotate uptime value in each deployment or statefulset since by default all pods will be scaled down.\nAdditional tunings like namespace based annotation etc are available at the readme Here.\nAchieving node scale down : Once the pods are scaled down, assuming you have cluster autoscaler configured, it should automatically remove the nodes that are unused or empty from your nodegroup.\nNote: Cluster autoscaler is mandatory since at the end of the day that’s what removes worker nodes to save your bill. ","permalink":"https://tanmay-bhat.github.io/posts/how-to-scale-down-kubernetes-cluster-workloads-during-off-hours/","summary":"\u003ch3 id=\"you-heard-it-right-everyone-needs-to-rest-once-a-while-even-our-little-kubernetes-cluster\"\u003eYou heard it right, everyone needs to rest once a while, even our little Kubernetes cluster.\u003c/h3\u003e\n\u003cp\u003eBefore we begin, here are the prerequisites :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKubernetes cluster\u003c/li\u003e\n\u003cli\u003eCluster autoscaler\u003c/li\u003e\n\u003cli\u003eBit of patience\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"usecase-\"\u003eUsecase :\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOne of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra.\u003c/li\u003e\n\u003cli\u003eYou maybe hosting workload in Kubernetes where you wont get traffic post business hours.\u003c/li\u003e\n\u003cli\u003eOr in weekends, you just want to scale down as no traffic flows to your apps during that time.\u003c/li\u003e\n\u003cli\u003eThe cost to keep those worker nodes at off hours are pretty high if you calculate for a quarter or for a year.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"solution-\"\u003eSolution :\u003c/h2\u003e\n\u003cp\u003eThough there isn\u0026rsquo;t any one click solution, Kubernetes finds a way or always Kubernetes Admin does !!\u003c/p\u003e","title":"How to scale down Kubernetes cluster workloads during off hours"},{"content":"One Loadbalancer to rule them all ? you heard it true, Its achievable !\nFor AWS LoadBalancer Controller Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money. Hence we thought how can we use a **single loadbalancer ** across all the namespaces.\nHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 Lets take a look at ArgoCD ingress :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Now, the problem with the above configuration is, by default AWS loadbalancer contoller will create ALB for each namespace. The cost of creating ALB for each namespace is $17.99 and data transfter charges are $0.02 per GB. If you got a lot of namespaces, the cost will be alot over the year. This is not Loadbalancer are supoosed to be used as ALB can handle alot of load in a single unit.\nAfter hunting for a while, we found that the solution lies in using a ALB annotation called group.name on the Ingress object.\nThe flow Assuming you got 5 different ingress with 5 ALB in backend in different namespaces, choose one ingress and add the annotation : alb.ingress.kubernetes.io/group.name: \u0026lt;ANYTHING_MEANINGFULL\u0026gt; Now, apply the same group name annotation to all the ingress objects in all namespaces. AWS LoadBalancer controller will use the first ingress Loadbalcner for all ingress objects and all other 4 ALB in this case will be removed. Below is an example for 2 different namespaces with single ALB :\n#default namespace ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 #argocd namespace ingerss apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Notice how both ingress objects are using the same annotion with same Group name. Whenever next time you need to create a new ingress in differnt namesapce, you can just add the same *group.name * annotation and it works flawlessly.\nFor NGINX Ingress Controller If you are in any other cloud, lets say DigitalOcean, its cloud provider controller doesn\u0026rsquo;t have any custom LB controller. But dont get sad there buddy, NGINX to the rescue.\nYou can leverage NGINX ingress controller to create \u0026amp; manage LB for you.\nFor installing NGINX ingress controller, you can refer thier documentation which has all the steps to install it.\nLets say you got 2 different namespaces with 2 different ingress objects same as above one for default and one for ArgoCD.\nThe solution is to add ingressClassName: nginx in the ingress object **Spec ** section in each ingress file.\nSo the final ingress file looks like this for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default spec: ingressClassName: nginx rules: - host: chartmuseum.example.com http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Argocd Ingress namespace ingress looks like this:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd spec: ingressClassName: nginx rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 Happy Loadbalancing :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-19-using-single-load-balancer-across-multiple-namespaces-in-kubernetes/","summary":"\u003cp\u003eOne Loadbalancer to rule them all ? you heard it true, Its achievable !\u003c/p\u003e\n\u003ch2 id=\"for-aws-loadbalancer-controller\"\u003eFor AWS LoadBalancer Controller\u003c/h2\u003e\n\u003cp\u003eUntil couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money.\nHence we thought how can we use a **single loadbalancer ** across all the namespaces.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:\u003c/p\u003e","title":"Using Single Load Balancer across multiple namespaces in Kubernetes"},{"content":"\nMost DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\nWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\nHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces. I\u0026rsquo;ll list down why :\nSuper easy to set up. Damn cheap. $5 for 250GB storage. One-click public/private button. Easy to integrate CDN if you have any. That being said, Let\u0026rsquo;s look at how we can utilize Spaces as an AWS S3 replacement to host our helm charts.\nSetup \u0026amp; Configuration 1. Create Spaces Log in to your console and click on Spaces and select Create spaces for $5 The name has to be unique and make the permission (File Listing) private. Once done, you should have the endpoint of the spaces created like : Now, go to API settings and create Access keys for Spaces. Please note that the secret key will be displayed only once and hence keep it safe copied somewhere. With that said, you\u0026rsquo;re ready to move to the next step. 2. Install Chartmuseum Add helm repo : helm repo add chartmuseum https://chartmuseum.github.io/charts Update the repo : helm repo update Here, we can\u0026rsquo;t just run helm install since we need to tell chartmuseum to use Space as a holy place to store charts instead of local PVC ( default). Download the chart to your local system by running : helm pull chartmuseum/chartmuseum —untar Update the below fields in values.yaml file :\nSTORAGE: \u0026#34;amazon\u0026#34; STORAGE_AMAZON_BUCKET: \u0026lt; your space name \u0026gt; STORAGE_AMAZON_REGION: \u0026lt; REGION in which your space is created\u0026gt; STORAGE_AMAZON_ENDPOINT: \u0026#34;https://REGION_NAME.digitaloceanspaces.com\u0026#34; #enable API to interact with endpoint DISABLE_API: false #secret section BASIC_AUTH_USER: admin # password for basic http authentication BASIC_AUTH_PASS: secret_password123 #Spaces access key AWS_ACCESS_KEY_ID: \u0026#34;YOUR KEY\u0026#34; #spacess secret key AWS_SECRET_ACCESS_KEY: \u0026#34;YOUR SECRET KEY\u0026#34; That\u0026rsquo;s it, run the below command to install the chart :\nhelm install chartmuseum chartmuseum/ -f chartmuseum/values.yaml Next, add an entry in ingress to point to your Repository URL. For example :\nhost: chartmuseum.tanmaybhat.tk http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Please note that you need to set False for Key Disable_API, else you cant send any data to your endpoint.\nThat\u0026rsquo;s it, you can now add your repo to your helm by typing :\nhelm repo add chartmuseum chartmuseum.example.com -u USERNAME -p PASSWORD If you want to push chart to this repo, you can do that by installing the helm push plugin:\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0 Once installed, push the chart by running :\nhelm push chart_directory chartmuseum Happy Helming !\nReferences Artifact Hub\nChart Museum\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-hosting-the-chartmuseum-in-digital-ocean-space/","summary":"\u003cp\u003e\u003cimg alt=\"image info\" loading=\"lazy\" src=\"/vlz-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eMost DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\u003c/p\u003e\n\u003cp\u003eWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\u003c/p\u003e\n\u003cp\u003eHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces. I\u0026rsquo;ll list down why :\u003c/p\u003e","title":"Hosting the Chartmuseum in DigitalOcean Spaces"},{"content":"\nIntroduction Though this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\nUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today.\nI learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.\nFrom Kaniko\u0026rsquo;s doc :\nkaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster.\nkaniko doesn\u0026rsquo;t depend on a Docker daemon and executes each command within a Dockerfile completely in userspace.\nThis enables building container images in environments that can\u0026rsquo;t easily or securely run a Docker daemon, such as a standard Kubernetes cluster.\nLet\u0026rsquo;s see how to achieve this in our pipeline. For simplicity, I\u0026rsquo;ll be using Gitlab CI in this example. You can use circle CI or GitHub actions or anything you like (a little bit of modification required ).\nPrerequisites\nAWS IAM credential ( Access-key and Secret-key) with ECR full access. Bit of time to implement the below 😀 Setup of AWS credentials Go to CI-CD settings of your project and set the below variables with appropriate values:\nAWS_ACCESS_KEY_ID = \u0026lt;your access key\u0026gt; AWS_SECRET_ACCESS_KEY = \u0026lt;your secret key\u0026gt; Gitlab-CI Create a Gitlab CI file that looks like :\nimage: alpine stages: - build_and_push build and push docker image: stage: build_and_push only: variables: - $CI_COMMIT_TAG =~ /^v[0-9]+\\.[0-9]+\\.[0-9]+$/ variables: AWS_DEFAULT_REGION: REGION_NAME CI_REGISTRY_IMAGE: YOUR_ID.dkr.ecr.REGION_NAME.amazonaws.com/REPO_NAME image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\u0026#34;\u0026#34;] script: - mkdir -p /kaniko/.docker - echo \u0026#34;{\\\u0026#34;credsStore\\\u0026#34;:\\\u0026#34;ecr-login\\\u0026#34;}\u0026#34; \u0026gt; /kaniko/.docker/config.json - /kaniko/executor --context \u0026#34;${CI_PROJECT_DIR}\u0026#34; --dockerfile \u0026#34;${CI_PROJECT_DIR}/Dockerfile\u0026#34; --destination \u0026#34;${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}\u0026#34; Let\u0026rsquo;s look at the above pipeline in detail :\nWe\u0026rsquo;re using the base image as alpine. We have one stage which is will build the Dockerfile and push to ECR using Kaniko As we have mentioned only constraint, the pipeline will only trigger when a new tag is pushed to Master branch. Next, we have 2 variables, in which we\u0026rsquo;re defining the default AWS region and our Registry address of ECR. ( please update with your values) Next, we\u0026rsquo;re using the Kaniko base image to build run the scripts mentioned and build our image. Then we\u0026rsquo;re making a docker folder that will have the registry to push credentials. Note that ECR regular login is a bit different than other container registries like Quay or GCR. You won\u0026rsquo;t get the regular username and password for this repo from AWS side. You must know that to log in to ECR, you need to run aws ecr get-login command which will give an authentication token that has a TTL of 12 hours, which doesn\u0026rsquo;t work in our case. Luckily was has created a new ECR login provider extension that will work through IAM permissions. Kaniko has built-in support for that provider, so you just need to add the variable of AWS creds in GitLab CI and Kaniko will take care of the rest. ( magic !) Then we provide Kaniko the path to Dockerfile which will be inside our current project, hence the use of CI_PROJECT_DIR which is a pre-defined variable from GitLab CI points to the current project context. Then I\u0026rsquo;m tagging the image with the latest tag from the repository and pushing to the ECR. Happy CI-CDing !!!\nReference :\nhttps://github.com/GoogleContainerTools/kaniko\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-using-kaniko-to-build-and-push-images-through-gitlab-ci-to-ecr/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/Kaniko-Logo.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThough this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\u003c/p\u003e\n\u003cp\u003eUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today.\u003c/p\u003e\n\u003cp\u003eI learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.\u003c/p\u003e","title":"Using Kaniko to build and push images to ECR from Gitlab CI"},{"content":"This happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the us-east-1 region. I asked him about the error message and he said ssh is giving a time-out error.\nSo, I tried connecting to the server via EC2 connect feature (web shell) that AWS provides, and even that said connection timed out.\nTried telnet to the endpoint and was the same also.\nI thought maybe the server may be struck due to overload and restarted it. But the state was still the same once it came up. I saw the metrics of the server via Cloudwatch and everything was fine, saw system logs also and even that looked good.\nCuriously opened status.aws.amazon.com and saw that us-east-1 Region is having an outage.\nBeing a Reddit fan, opened r/sysadmin and I could see people all over the world complaining about AWS being down in that region and 1000\u0026rsquo;s of memes on that topic. I told myself this could be mostly due to the AWS outage and I\u0026rsquo;ll see once they fix it.\nCut to the next day because the outage took 19 long hours to fix the outage. long live SLA!\nI still was not able to connect to the instance post AWS fix. After digging for X time, turns out, the issue was with the subnet in which EC2 was launched. Someone mistakenly attached NAT gateway to the public subnet instead of the Internet gateway. Updated the correct config in the Route-table of the subnet and it worked.\nOne tiny missing detail can totally mess your mind up.\nAnother day of learning :D\nHappy DevOpsing !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-a-tale-of-ec2-connectivity-issue/","summary":"\u003cp\u003eThis happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the \u003cem\u003eus-east-1\u003c/em\u003e region. I asked him about the error message and he said ssh is giving a time-out error.\u003c/p\u003e\n\u003cp\u003eSo, I tried connecting to the server via EC2 connect feature  \u003cem\u003e(web shell)\u003c/em\u003e that AWS provides, and even that said connection timed out.\u003c/p\u003e\n\u003cp\u003eTried telnet to the endpoint and was the same also.\u003c/p\u003e","title":"A tale of EC2 connectivity issue"},{"content":"Hey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a Kubernetes challenge and guess what they\u0026rsquo;re giving away free credits of $120 to try it out free!!!\nThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD. Let\u0026rsquo;s get started!\nChallenge Details Link of the challenge page Here Pick one challenge from the list mention in above link based on your knowledge. Create a GitHub or GitLab repo for your project Fill out the code challenge form to get DigitalOcean credits for your project Join the #kubernetes-challenge channel in the DigitalOcean Deploy Discord Complete your challenge Write about what you’ve built and share it on a blog or in your project README. Make a pull request against the Kubernetes Challenge Github Repo with information about your project Let them know you’ve completed your challenge by filling out this form Now that we\u0026rsquo;ve applied let\u0026rsquo;s take a look at one of the challenges I chose :\nDeploy a GitOps CI/CD implementation GitOps is the (only) way automate deployment pipelines for Kubernetes environments in 2022, and ArgoCD is currently one of the leading player. Install it to create a CI/CD solution, using GH Actions for actual image building.\nCluster Creation Sign in to your DO console. Click on NEW button and create a Kubernetes cluster with default values. You can customize the location of cluster nearest to your location to avoid altency issues to API server. Once you submit, it\u0026rsquo;ll take around 10-15 min for the worker nodes and API server to become ready. Click on the Actions button and download the kubeconfig file. Once you download, install kubectl binary by following steps in the Getting started section of *overview *tab. Once, kubectl in installed in your local, you can save / move the config file downloaded to your ~/.kube/config location. Now, you can connect to your API server, test it by running : kubectl get node -o wide Project setup Clone this repository using below command : git clone https://github.com/tanmay-bhat/DigitalOcean-Kubernetes-Challenge-argoCD This project contains below:\ngo app Dockerfile github actions file ( CI) Kubernetes manifest files Let\u0026rsquo;s Look mainly kustomize/base :\nHere, Deployment.yaml file contains the deployment resource YAML file. containers: - image: registry.digitalocean.com/tanmaybhat/saymyname name: saymyname ports: - name: http containerPort: 8080 imagePullSecrets: - name: tanmaybhat Notice the image registry I\u0026rsquo;m using is the Digital Ocean registry itself and not the mostly used Docker Hub.\nThe ImagePullSecrets has a name: Tanmay Bhat. This is the Kubernetes secret which has the Digital Ocean registry credentials which we will use to pull the image.\nNow let\u0026rsquo;s look at our Github actions config file :\nname: Go on: push: branches: [ main ] tags: - \u0026#39;v*.*.*\u0026#39; jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.x uses: actions/setup-go@v2 with: go-version: ^1.14 - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Login to Digitalocean uses: docker/login-action@v1 with: registry: registry.digitalocean.com username: ${{ secrets.DIGITAL_OCEAN_TOKEN }} password: ${{ secrets.DIGITAL_OCEAN_TOKEN }} - name: push image to digitalocean run: | docker build -t registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} . docker push registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} deploy: name: Deploy runs-on: ubuntu-latest needs: build steps: - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: update image tag in manifest uses: imranismail/setup-kustomize@v1 - run: | cd kustomize/base kustomize edit set image registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} - name: Commit files run: | git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -am \u0026#34;update image tag to ${{ env.GIT_TAG }}\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} I\u0026rsquo;m gonna explain the above section since the main goal of this article is to do CI/CD :\nThe on section says trigger this piepline if the chnages has been pushed to **Main branch with a tag in format : vx.x.x ( i.e v1.0.0 etc) On each tag push, pipeline will run 2 jobs. Build and Deploy. In Build section, the follwing steps will run on ubuntu image. Check out code step uses pre-build action actions/checkout@v2 to clone current repository into the piepline container i.e ubuntu. Extract Git Tag is used to get the latest tag pusued to main branch and store it in th environmental variable GIT_TAG. In Login to Digitalocean, since we need to push our build docker images to a private registry like Digital Ocean, I\u0026rsquo;m using docker login action to auttenticate to the DO registry. In push image to digitalocean, I\u0026rsquo;m buidling the docker image and tagging it to latest pushed tag version and pushing to my registry. Next comes, the Deploy section. here again I\u0026rsquo;m using ubuntu as base image and again getting the repository from main branch and extracting tag version from the repositiry. Once that is done, I\u0026rsquo;ll use a tool called Kustomize to update my manifest file\u0026rsquo;s docker image tag to the latest tag version. If you\u0026rsquo;re using helm charts only but not kustomize with Helm, you need to use Sed command and update the image tag in manifest file ( deployment.yaml). Later, I\u0026rsquo;m doing the commit of latest tag edit and pushing the changes back to my repo. To sum up, what the exact pipeline does whenever a new tag is pushed to main branch :\nclone the repository, build the docker image, tag it and push it to registry. update the tag in manifest file and push it back to gthe repository. You might have this question, Tanmay, this is just CI, where\u0026rsquo;s CD ? well, that\u0026rsquo;s the magic ArgoCD solves for us.\nArgoCD Setup ArgoCD by running the below commands : kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Once done, you can verify its running status by running the command Next step is to retrieve the password of argocd. For that, run : kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d By default argocd service type will be ClusterIP. That means you cant access argocd outside of your cluster. So, Let\u0026rsquo;s change that to LoadBalancer by running : kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Now, wait for couple more minutes for LoadBalancer to start in Digital Ocean and get the endpoint of it by running : Open the external IP in your browser and voila, you should see argocd UI login page. Login with username: admin password got from step 3. ArgoCD Configuration Once logged in to ArgoCD UI, click on new app and set the below values: application name : demo-argocd Project : default Sync Policy : Automatic Repository URL : \u0026lt;GITHUB REPO URL OF YOUR PROJECT\u0026gt; Rivision : HEAD Path : kustomize/base Destination Cluster : https://kubernetes.default.svc Namespace : default Click create and see your app glowing in your cluster.\nthe ArgoCD magic here is that, it watches for any new changes to your repo every 3 minutes ( default ) and new changes will be auto-applied in your cluster.\nSee the comment that says: update the tag to v1.0.12 ? that was my last commit. If a new tag is committed, here\u0026rsquo;s how it\u0026rsquo;s gonna update and look\nConclusion I found DOKS to be extreme easy to set up and straightforward. From click to integrate Registry to a Kubernetes cluster, easy cluster creation and scaling up.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-journey-to-the-kubernetes-world-with-digital-ocean/","summary":"\u003cp\u003eHey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a \u003cstrong\u003eKubernetes challenge\u003c/strong\u003e and guess what they\u0026rsquo;re giving away free credits of \u003cem\u003e$120\u003c/em\u003e to try it out free!!!\u003c/p\u003e\n\u003cp\u003eThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD. Let\u0026rsquo;s get started!\u003c/p\u003e","title":"Journey to the Kubernetes world with Digital Ocean"},{"content":"History You may have faced this scenario where you wanna keep scaling up apps nodes but also under-keeping costs at a limit. Spot Instance is the way for that task. Now, how do we do that? let\u0026rsquo;s see.\nAs you know there are mainly 2 types of instances in AWS, called On-demand and Spot. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.\nSpot instances are a bit different. Spot instance is the unused capacity in AWS cloud sitting idle and AWS gives that to you at an extremely low price for like 80-90% cheaper than on-demand. The difference being whenever AWS needs the capacity to handle on-demand, it takes back the spot instances with ~2 min notice via notification to you.\nNode groups Now that we\u0026rsquo;ve learned about what spot offers, it makes total sense to include that in your workload to save quite a lot of money. So let\u0026rsquo;s learn about the node groups for on-demand and spot.\nDesigning node groups : Since spot can go down anytime, you should always run your critical workloads in on-demand instances. All stateful- sets should run in on-demand instances. Have multiple nodegroups for spot so that you can maximize chance of getting spot instances. Use CA for scaling up / down. Real-world scenario Now, let\u0026rsquo;s say you have critical apps running in on-demand NG and other cronjobs or monitoring stacks are in spot NG. If you wanted to schedule pods with the below architecture I got the answer.\nRequired architecture: If your app has 8 replicas, 4/5 of them should run in on-demand NG and 3/4 of them in spot NG such that even if the spot goes down, ondemand can handle the load until the new spot comes in and takes the load. In this way, you\u0026rsquo;ll have \u0026lsquo;Zero Downtime\u0026rsquo;.\nNow for the above problem, there isn\u0026rsquo;t a straightforward or clear-cut solution out of the box. But I\u0026rsquo;ll explain the way I\u0026rsquo;ve implemented it.\nSolution Once both the NG are created, let\u0026rsquo;s take a look at the label of a spot node.\nkubectl describe no ip-100-45-51-226.ap-south-1.compute.internal | grep SPOT eks.amazonaws.com/capacityType=SPOT Any node created by spot will have the above label and any node with ondemand will have label :\neks.amazonaws.com/capacityType=ON_DEMAND Once, they are done, you can create a sample Nginx deployment with the below configs:\nAll the other configs are pretty easy to understand and come under basic Kubernetes concepts. Since nodes created by spot and on-demand NG will have the above-mentioned labels, we can utilize that and request scheduler to try its best effort to schedule 40% of pods in this deployment to SPOT and 60% to ON_DEMAND.\nYou can change the above weight as per your needs. Once the above YAML is deployed, let\u0026rsquo;s take a look at the way pods are scheduled.\nIn my case, nodes with names 25 and 226 are Spot instances. If calculated correctly, 6 pods are running in on-demand and 4 pods are running in spot NG which is exactly as we expected.\nNOTE: This may not be always exactly the ratio you need since the scheduler gives pods to nodes on a best effort basis. But it\u0026rsquo;ll be almost a similar result.\nHappy Kuberneting !!!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-11-19-scheduling-pods-in-both-spot-and-on-demand-nodes-in-eks/","summary":"\u003ch3 id=\"history\"\u003eHistory\u003c/h3\u003e\n\u003cp\u003eYou may have faced this scenario where you wanna keep scaling up apps  nodes but also under-keeping costs at a limit.  Spot Instance is the way for that task. Now,  how do we do that? let\u0026rsquo;s see.\u003c/p\u003e\n\u003cp\u003eAs you know there are mainly 2 types of instances in AWS, called \u003cstrong\u003eOn-demand\u003c/strong\u003e and \u003cstrong\u003eSpot\u003c/strong\u003e. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.\u003c/p\u003e","title":"Scheduling pods in both Spot and On-demand nodes in EKS"},{"content":"If you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in Skit.ai as a DevOps Engineer.\nOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\nDefinition Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:\nThere are pods that failed to run in the cluster due to insufficient resources. There are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes. Documents If you\u0026rsquo;re going to implement autoscaler in your EKS cluster, please read the FAQ .\nThe setting up of autoscaler in EKS is perfectly written by AWS document here\nOnce, things are set up, the logs should look like below :\nNow if you\u0026rsquo;re getting this, then it means the setup is clean. If we take a closer look at logs, it says node minimum size reached and cant scale down anymore.\nLet\u0026rsquo;s understand scaling up and scale down criteria and it\u0026rsquo;s working.\nScale-down flow Every 10 seconds (configurable by --scan-interval flag),Cluster Autoscaler checks which nodes are unneeded. A node is considered for removal when all below conditions hold:\nThe sum of cpu and memory requests of all pods running on this node is smaller than 50% of the node\u0026rsquo;s allocatable. All pods running on the node (except these that run on all nodes by default, like manifest-run pods or pods created by daemonsets) can be moved to other nodes. It doesn\u0026rsquo;t have scale-down disabled annotation (see How can I prevent Cluster Autoscaler from scaling down a particular node?) If a node is unneeded for more than 10 minutes, it will be terminated.\nCluster Autoscaler terminates one non-empty node at a time to reduce the risk of creating new unschedulable pods.\nThe next node may possibly be terminated just after the first one, if it was also unneeded for more than 10 min and didn\u0026rsquo;t rely on the same nodes in simulation (see below example scenario), but not together. Empty nodes, on the other hand, can be terminated in bulk, up to 10 nodes at a time.\nWhat happens when a non-empty node is terminated? As mentioned above, all pods should be migrated elsewhere.\nCluster Autoscaler does this by evicting them and tainting the node, so they aren\u0026rsquo;t scheduled there again.\nAlso, you should consider the below point :\nIf there\u0026rsquo;s a node which is under-utilized but that node counts towards minimum node group size, then CA wont terminate that node and the logs will be similar to above screenshot. Scale-up flow Scale-up creates a watch on the API server looking for all pods. It checks for any unschedulable pods every 10 seconds (configurable by --scan-interval flag). A pod is unschedulable when the Kubernetes scheduler is unable to find a node that can accommodate the pod. For example, a pod can request more CPU that is available on any of the cluster nodes. Unschedulable pods are recognized by their PodCondition. Whenever a Kubernetes scheduler fails to find a place to run a pod, it sets \u0026ldquo;schedulable\u0026rdquo; PodCondition to false and reason to \u0026ldquo;unschedulable\u0026rdquo;. If there are any items in the unschedulable pods list, Cluster Autoscaler tries to find a new place to run them. Testing the CA Let\u0026rsquo;s assume you got 2 t3.medium node and the min value of nodegroup is 2 with max value set to 5.\u0026lt; Run a nginx deployment with 500 replicas to see if cluster autoscaler scales up the nodes.. The command would be : kubectl create deployment cluster-killer --image=nginx --replicas=500 2 nodes of that size can\u0026rsquo;t handle 500 pods of nginx, so they should be in pending state and CA scans for pending state pods every 10 seconds which should start couple of nodes within minutes. You can verify from command : kubectl get node Once all pods are scheduled, to test scale down, you can either delete the deployment using : kubectl delete deployment cluster-killer Or scale down the replicas to zero with command : kubectl scale deployment cluster-killer --replicas=0 If you refer the logs of cluster autoscaler now, it will mention that X node is uneeded for X min etc. The cool down period by default is 10 min so, after that time, it\u0026rsquo;ll apply taint on that node with name DeletionCandidateOfClusterAutoscaler and ToBeDeletedByClusterAutoscaler and removes the nodes. It looks like below: ","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-a-closer-look-at-cluster-autoscaler-for-eks/","summary":"\u003cp\u003eIf you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in \u003ca href=\"https://skit.ai\"\u003eSkit.ai\u003c/a\u003e as a DevOps Engineer.\u003c/p\u003e\n\u003cp\u003eOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\u003c/p\u003e\n\u003ch2 id=\"definition\"\u003eDefinition\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCluster Autoscaler\u003c/strong\u003e is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThere are pods that failed to run in the cluster due to insufficient resources.\u003c/li\u003e\n\u003cli\u003eThere are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"documents\"\u003eDocuments\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIf you\u0026rsquo;re going to implement autoscaler in your EKS cluster, please read the \u003ca href=\"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md\"\u003eFAQ\u003c/a\u003e .\u003c/p\u003e","title":"A closer look at Cluster Autoscaler for EKS"},{"content":"Definition Let\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\nIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\nProblem statement Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like : kubectl edit pv your_PV and just change the size, it used to work since you have storage class of kubernetes.io/aws-ebs.\nNow, You can\u0026rsquo;t resize your PV just by changing the size in the manifest file (\u0026gt; EKS v1.17)\nwhat should you do as a Kubernetes admin if you wanna resize your PV with above mentioned version ?\nPrerequisites: AWS EKS cluster Eleveated IAM permissions Understanding of PV in Kubernetes Solution Simple, Kubernetes team has a new tool called ebs-csi controller. What does it do? The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.\nYou can install the ebs-csi driver by referring to AWS document . Once you installation is done, you should see the pods similar to : And the ebs-csi-controller pod logs should look like : Looks good, now, for a test, lets edit a PV and increase its size. In my example, I\u0026rsquo;ll just increase the alert-manager PV to 3GB, Initial size was 2GB. If you are thinking how did I beautify Kubernetes editing ? all thanks to Lens IDE.\nLets verify the PV size now :\nHere comes the real test to see if the actual EBS volume is resized or not. For that let\u0026rsquo;s copy the volume id and search that volume size in AWS console or via AWS cli to verify the disk size.\nTo get the volume id of a PV, run the below command :\nkubectl describe pv PV_NAME | grep Volume Now if you prefer AWS CLI, you can use the following command, else can be verified in AWS console :\n","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-dynamic-pv-in-kubernetes-feat-eks-ebs/","summary":"\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003eLet\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\u003c/p\u003e\n\u003cp\u003eIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\u003c/p\u003e\n\u003ch4 id=\"problem-statement\"\u003eProblem statement\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eUp until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like\u003c/strong\u003e : \u003ccode\u003ekubectl edit pv your_PV\u003c/code\u003e \u003cstrong\u003eand just change the size, it used to work since you have storage class  of\u003c/strong\u003e \u003ccode\u003ekubernetes.io/aws-ebs\u003c/code\u003e.\u003c/p\u003e","title":"Dynamic PV in Kubernetes feat. EKS (EBS)"},{"content":"Definition what\u0026rsquo;s kubewatch ?\nkubewatch is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\nLets see how we can deploy it to our cluster.\nPre-requisites :\nKubernetes 1.12+ cluster Helm v3 A slack app and a channel to integrate kubewatch Steps Add Bitnami repo to your helm : helm repo add bitnami https://charts.bitnami.com/bitnami Verify that kubewatch chart is available in the repo : demo\u0026gt; helm search repo kubewatch NAME CHART VERSION APP VERSION DESCRIPTION bitnami/kubewatch 3.2.16 0.1.0 Kubewatch Customize the values like slack integration and enabling RBAC. If you directly do helm install chart-name you wont get any event notification as RBAC is set to false by default kin the helm chart.\nRun the below command to get the values.yaml to local :\nhelm show values bitnami/kubewatch \u0026gt; updated-values.yaml Now edit the yaml file as per your requirement. Here\u0026rsquo;s what I\u0026rsquo;ve changed :\nSlack Integration :\nslack: enabled: true channel: \u0026#34;kubewatch\u0026#34; #your slack channel name ## Create using: https://my.slack.com/services/new/bot and invite the bot to your channel using: /join @botname ## token: \u0026#34;your slack bot token here\u0026#34; Enable RBAC:\n## @section RBAC parameters ## @param rbac.create Whether to create use RBAC resources or not ## rbac: create: true Now lets deploy using the below command : helm install kubewatch bitnami/kubewatch -f ./updated-values.yaml Verify that kubewatch pod is running : demo\u0026gt; kubectl get pod NAME READY STATUS RESTARTS AGE kubewatch-c86656645-8znwk 1/1 Running 0 2m19s To test it out, lets create a nginx deployment with command : kubectl create deploy nginx --image=nginx Check your slack channel for notifications : The indication is as follows :\nGreen : for resources created Yellow : for resources updated Red : for resources deleted You can customize the notification a lot, for example, which namespace to monitor to ( default value is all namespace) , which resource to monitor to like deployment, pod, PV, service etc.\nYou can just edit the configmap : kubewatch-config and change the resources to monitor.\nHappy monitoring !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-15-monitoring-k8s-resource-changes-in-cluster-with-kubewatch/","summary":"\u003ch2 id=\"definition\"\u003eDefinition\u003c/h2\u003e\n\u003cp\u003ewhat\u0026rsquo;s kubewatch ?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/bitnami-labs/kubewatch%22%3E\"\u003ekubewatch\u003c/a\u003e is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLets see how we can deploy it to our cluster.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePre-requisites :\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKubernetes 1.12+ cluster\u003c/li\u003e\n\u003cli\u003eHelm v3\u003c/li\u003e\n\u003cli\u003eA slack app and a channel to integrate kubewatch\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdd Bitnami repo to your helm :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehelm repo add bitnami https://charts.bitnami.com/bitnami\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eVerify that kubewatch chart is available in the repo :\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edemo\u0026gt; helm search repo kubewatch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNAME                    CHART VERSION   APP VERSION     DESCRIPTION\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ebitnami/kubewatch       3.2.16          0.1.0           Kubewatch\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003e\n\u003cp\u003eCustomize the values like slack integration and enabling RBAC. If you directly do \u003cstrong\u003ehelm install chart-name\u003c/strong\u003e you wont get any event notification as RBAC is set to \u003cstrong\u003efalse\u003c/strong\u003e by default kin the helm chart.\u003c/p\u003e","title":"Monitoring K8S resource changes with kubewatch"},{"content":"Previously I have written https://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article about how AWS pushed broken image to Docker hub and we got screwed as we were using latest as image tag.\nWelp, this happened again in our CI/CD pipeline as we were using https://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push plugin from helm and using that to push charts to https://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum .\nSo we were using the below line to pull the helm push plugin :\nhelm plugin install https://github.com/chartmuseum/helm-push.git And were pushing to Chartmuseum via command :\nhelm push app-name repo-name\nIt turns out that command is not valid and as per their latest (v0.10.0) changes to the plugin, its been renamed to cm-push and we gotta use like helm cm-push app-name repo-name. Else we can use the same command with old version of plugin.\nHence our pipeline got screwed and I\u0026rsquo;ve fixed by pulling specific version from their repo by using -version argument. It goes like this :\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0\nThe better solution to this is to replace the hard-coded version above to GitLab CI variable and update the version from there later.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-13-it-happened-again-in-production/","summary":"\u003cp\u003ePreviously I have written     \u003ca href=\"https://wp.me/pcknFJ-2F%22%3Earticle\"\u003ehttps://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article\u003c/a\u003e  about how AWS pushed broken image to Docker hub and we got screwed as we were using \u003cem\u003elatest\u003c/em\u003e as image tag.\u003c/p\u003e\n\u003cp\u003eWelp, this happened again in our CI/CD pipeline as we were using     \u003ca href=\"https://github.com/chartmuseum/helm-push%22%3Epush\"\u003ehttps://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push\u003c/a\u003e  plugin from helm and using that to push charts to     \u003ca href=\"https://chartmuseum.com/%22%3Echartmuseum\"\u003ehttps://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum\u003c/a\u003e .\u003c/p\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003cp\u003eSo we were using the below line to pull the helm push plugin :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehelm plugin install https://github.com/chartmuseum/helm-push.git\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd were pushing to Chartmuseum via command :\u003c/p\u003e","title":"It happened again in production !!"},{"content":"Question After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\nThere\u0026rsquo;s a reason at least why I had this requirement, Cost optimization in AWS.\nBackground When you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region. Now for staging / testing clusters the Inter Availability Zone data transfer fees we were getting was a hefty one, which was unnecessary as HA is not needed for the testing environment.\nI couldn\u0026rsquo;t find this anywhere else, so with an outage at staging cluster :D ( shhhhh!) I found out that the solution is to create a new node group with AZ hard-coded while creating it and any node you spawn in that node group using ASG (Auto Scaling Group) will be in that single AZ only keeping your inter AZ data transfer cost to 0.\nSolution eksctl create nodegroup --cluster=staging_cluster \\ --region=ap-south-1 \\ --node-zones=ap-south-1a \\ --name=M5.2xlarge_NG \\ --node-type=m5.2xlarge In the above snippet, I\u0026rsquo;m creating NG in Mumbai Region in AZ ap-south-1a with m5.2xlarge instance.\nIf you want to go with GUI way, then :\nGo to your cluster in EKS and then click on Add Node Group : Go with usual flow of giving it a name, taint if required, and IAM role.\nSelect AMI, disk size, instance family etc.\nIn Networking section, by default 3 subnets will be selected, untick 2 of them and keep 1 ( any desired AZ \u0026lsquo;a/b/c\u0026rsquo;). If you\u0026rsquo;re unsure about the name and AZ, you can verify that by going to VPC -\u0026gt; subnets.\nThat\u0026rsquo;s it, create the node group and all the instances will be spawned in that AZ only.\nYou can also do this in a hackish way by editing the ASG corresponding to the node group and removing 2 subnets from there.\nIt works but your node group will become Unhealthy and AWS wont do anything to the node groups which is having health issue. So better to create a new node group.\neksctl There\u0026rsquo;s one more way which I found out i.e to use ekctl command line and create from config file.\nYou can read more about eksctl and configure it by referring here .\nLet\u0026rsquo;s create a config file to create the node group : ap-south-la-NG.yaml :\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: Your_Cluster_Name region: ap-south-1 managedNodeGroups: - name: demo-nodegroup labels: { role: worker-nodes } instanceType: m5.xlarge desiredCapacity: 1 volumeSize: 50 availabilityZones: \u0026amp;#091; ap-south-1a ] minSize: 1 maxSize: 2 volumeType: gp3 privateNetworking: true Then you can apply using the below command :\neksctl create nodegroup --config-file ap-south-la-NG.yaml Finally, once the new Node groups is created, you can scale down your existing node group to 0 so that AWS will drain the nodes gracefully and all your workloads will be moved to newly created node groups.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-11-how-to-migrate-all-your-worker-nodes-from-multiple-az-to-single-az-in-aws-eks/","summary":"\u003ch3 id=\"question\"\u003eQuestion\u003c/h3\u003e\n\u003cp\u003eAfter reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s a reason at least why I had this requirement, \u003cstrong\u003eCost optimization in AWS\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eWhen you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region. Now for staging / testing clusters the Inter Availability Zone data transfer fees we were getting was a hefty one, which was unnecessary as HA is not needed for the testing environment.\u003c/p\u003e","title":"How to migrate a Node-Group from Multi AZ to single AZ in AWS EKS"},{"content":"History If you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful amusing for you.\nOn Thursday, I started receiving alerts that our CI pipeline is failing.\nI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\nInstalling docker Installation failed. Check that you have permissions to install. Cleaning up file based variables ERROR: Job failed: command terminated with exit code 1 After scratching the head for sometime, I found that the latest aws-cli image from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.\nSo I went to Docker hub and I saw on that day there was a new version pushed which was 2.2.39 tagged as latest. Since in our CI file, we didn\u0026rsquo;t mention specific image version to pull so it always assumes the tag to pull is latest.\nAs a temporary fix, I changed the image version to older one which was 2.2.38 and it worked fine.\nIf you ask me for a better a better solution, it would be always good to use a specific version in production since you know it will work for sure instead of using latest tag which could change every single day.\nElse push that image to your private container repositories like ECR and pull from there.\nI\u0026rsquo;m pretty sure AWS broke few thousand CI pipelines over the world whoever used latest as the image tag :D\nTo give an idea about how to install docker inside aws-cli image, you can just run the below command which should install docker from AWS hosted repo for a faster install :\namazon-linux-extras install docker DevOps story ends here. I\u0026rsquo;ll update more stories like this in future :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-19-story-of-keeping-ci-pipeline-from-getting-screwed-when-aws-pushes-broken-docker-image-to-docker-hub/","summary":"\u003ch2 id=\"history\"\u003eHistory\u003c/h2\u003e\n\u003cp\u003eIf you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful   amusing for you.\u003c/p\u003e\n\u003cp\u003eOn Thursday, I started receiving alerts that our CI pipeline is failing.\u003c/p\u003e\n\u003cp\u003eI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eInstalling docker\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eInstallation failed. Check that you have permissions to install.\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eCleaning up file based variables\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eERROR: Job failed: command terminated with exit code 1\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter scratching the head for sometime, I found that the latest \u003ccode\u003eaws-cli \u003c/code\u003eimage from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.\u003c/p\u003e","title":"Story of keeping CI pipeline from getting screwed when AWS pushes broken docker image to Docker hub"},{"content":"Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\nI wanted to add a user to my EKS, hence while adding the user to aws-auth configmap of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !!!\nStraight forward solution which I found out :\nFind out who created the EKS cluster ( owner) and ask them to edit the aws-auth configmap to correct your mistakes.\nThe user who created the cluster is the root user for entity. Hence regardless of aws-auth configmap mess, he/she can login via kubectl anytime.\nRead more here on solution by AWS.\nI wrote this because I made this mistake in my company and spent hours searching for answer before finding this info.\nOnce I found out the creator, she corrected it in 1 min. :D\nLong term solution :\nYou might be saying \u0026rsquo; Thats one solution to save my job, how do I make sure I dont do this mistake again ?'\nAlright, so here\u0026rsquo;s what you can follow from next time :\nFirst get the configmap yaml file by typing :\nkubectl get configmap aws-auth -n kube-system -o yaml \u0026gt; aws-auth-configmap.yml Once you get the yaml file, edit the file using your favorite text editor and update your changes.\nNow, update the configmap with your new updated file by typing :\nkubectl apply -n kube-system -f aws-auth-configmap.yml Remember, live editing is never a good option !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-01-how-to-access-aws-eks-cluster-when-you-mess-up-the-aws-auth-configmap/","summary":"\u003cp\u003eHey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI wanted to add a user to my EKS, hence while adding the user to \u003ccode\u003eaws-auth configmap\u003c/code\u003e of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !!!\u003c/p\u003e","title":"How to access AWS EKS cluster when you mess up the aws-auth configmap"},{"content":"Hey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of writing this since I spent hours searching and fixing this :/\nLets look at the problem or the requirement. It goes like this :\nI have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository. how do I pull from that repo ?\nGitlab CI Lets consider the below gitlab-ci.yml file :\nimage: \u0026#34;python:3.6\u0026#34; stages: - publish_image build and push docker image: stage: publish_image only: variables: - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+-\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/ - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/ variables: DOCKER_HOST: tcp://docker:2375 image: name: amazon/aws-cli entrypoint: \u0026#34;\u0026#34; services: - docker:dind before_script: - echo \u0026#34;$CI_COMMIT_TAG\u0026#34; - amazon-linux-extras install docker - docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; -p \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; $CI_REGISTRY script: - docker build -t $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG . - aws ecr get-login-password | docker login --username AWS --password-stdin $DOCKER_REGISTRY - docker push $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_TAG - docker push $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG Here\u0026rsquo;s how the above CI file works :\nUses base image python on which the stages will run. has a single stage which will build and push images to ECR only section tells gitlab to run the stage only if the git tag is done and it matched the regex mentioned. in before_script section, we\u0026rsquo;re displaying the commit tag and installing docker in aws-cli image since that image doesn\u0026rsquo;t come preinstalled with docker. finally we\u0026rsquo;re doing docker login to with our dockerhub account before building Dockerfile. Later we build the Dockerfile and then push it to ECR Configure login to Docker hub in GitLab CI To configure the Dockerhub credentials, go to your GitLab project -\u0026gt; settings -\u0026gt; CI/CD In Variables section, add the below Key and their value : Key : CI_REGISTRY || Value : docker.io Key : CI_REGISTRY_USER || Value : your_dockerhub_username Key : CI_REGISTRY_PASSWORD || Value : your_dockerhub_password Now, to setup AWS credentials, configure the below values :\nKey : AWS_ACCESS_KEY_ID || Value : your_aws_accesskey Key : AWS_SECRET_ACCESS_KEY || Value : your_aws_secretkey That\u0026rsquo;s it, voila !! Now GitLab runner should get your docker credentials from variables and pull the image seamlessly.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-08-23-pulling-private-image-from-docker-hub-in-gitlab-ci/","summary":"\u003cp\u003eHey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of  writing this since I spent hours searching and fixing this :/\u003c/p\u003e\n\u003cp\u003eLets look at the problem or the requirement. It goes like this :\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository. how do I pull from that repo ?\u003c/p\u003e","title":"Pulling private image from Docker hub in GitLab CI"},{"content":"Hey people, in this article, we\u0026rsquo;ll see how to configure TP -Link TL-WR740N (preferably old one) as repeater to extend your main WI-FI signal in your house.\nLets get into basics real quick.\nWhat\u0026rsquo;s a repeater ?\nDefinition : A WiFi repeater or extender is used to extend the coverage area of your Wi-Fi network. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.\nSteps on secondary router :\nDo a factory reset of your secondary router. You can refer https://youtu.be/4AkkPRE9ZBM\u0026quot;\u0026gt;this video for how-to steps. Once the router is up and running, connect to it wirelessly / through LAN cable. Go to admin console by typing this IP address in browser URL : 192.168.0.1 with credentials , username : admin password : admin ( super secure :D ) Lets first change the IP address of this router to something else rather than the default one as later this IP can cause IP allocation conflict due to DHCP set in primary router. To to that , lets go to Network -\u0026gt; LAN -\u0026gt; IP address and change it to something like 192.168.1.100 .and hit Save. ( you can change it to almost any IP you like in this subnet) Do a reboot of the router and connect back to router console using the new IP in browser URL i.e. in my case 192.168.1.100 or the IP given by you.\nLets configure the repeater mode. To do that, go to Wireless -\u0026gt; Enable WDS Bridging.\nClick on Survey and select the WIFI name which you want to repeat.\nType the password for that in Password field and hit Save. Later you may get alert on switching the repeater to be in same Wi-Fi channel as main router, select ok to that pop-up.\nNext thing would be to setup DHCP of the router. I\u0026rsquo;ll explain a bit here regarding the problem I faced. According to YouTube tutorials and articles out there, we need to disable the DHCP option in secondary router.\nWhat I faced after that is I cant connect any device to that router later as DHCP is disabled, the router wont be able to assign any IP address to any device asking for connection. So your device will be struck in \u0026ldquo;Obtaining IP address\u0026rdquo;.\nSo I found out the below trick and its working brilliantly for me.\nOk, lets through the settings one by one, DHCP Server : Keep it Enable\nStart IP Address: Enter : 192.168.1.101 OR the +1 IP of the assigned IP to your router. i.e\nIf you gave 192.168.1.10 to your router, mention here 192.168.1.11\nEnd IP Address: : Enter 192.168.1.199 or the IP range limit you need. I mentioned here 98 (199 - 101) Address limit assuming my number of devices wont exceed 98 devices :D\n[ Follow the start IP address logic if you mentioned any alternate IP address to router. ]\nAddress Lease Time: Keep the default value.\nDefault Gateway: Here, enter the IP address of your primary router. You can mostly find out by seeing the backside of your primary router.\nElse, you can run the below command via cmd to get the value ( after connecting to primary router) :\nipconfig /all | findstr Gateway\u0026lt;br /\u0026gt;Default Gateway . . . . . . . . . : 192.168.1.1\nDefault Domain: Keep the default value i.e. empty.\nPrimary DNS: You can mention the DNS resolver address. This is optional and same for below one also. if none is mentioned, DNS resolver given by ISP is used. which is not a good solution from privacy perspective. You can use Google Public DNS ( 8.8.8.8) , Quad DNS,(9.9.9.9) Cloud flare DNS (1.1.1.1) here.\nSecondary DNS: This value corresponds to what resolver to use if the request is not resolved by the first DNS. Its good to mention different service to ensure high reliability.\nThat\u0026rsquo;s it. hit Save and do a reboot of the server to get new changes.\nNote : there\u0026rsquo;s a high change you wont be able to connect to your repeater later if you\u0026rsquo;re in a Wi-Fi crowded place i.e. you are surrounded by lot of WIFI. When there are lot of Wifi nearby the router tries to get to channel which is less crowded.\nBut in repeater mode, both repeater and main router needs to be in same Wi-Fi channel. So I would highly advice you to go to your primary router set the Wi-Fi to a particular channel and keep the same channel in repeater also. rather than the default setting : Auto.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-08-11-configuring-tp-link-tl-wr740n-as-wi-fi-repeater/","summary":"\u003cp\u003eHey people, in this article, we\u0026rsquo;ll see how to configure TP -Link \u003ca href=\"https://www.tp-link.com/in/home-networking/wifi-router/tl-wr740n/\"\u003eTL-WR740N\u003c/a\u003e (preferably old one) as repeater to extend your main WI-FI signal in your house.\u003c/p\u003e\n\u003cp\u003eLets get into basics real quick.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u0026rsquo;s a repeater ?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDefinition : A WiFi repeater or extender is used \u003cstrong\u003eto extend the coverage area of your Wi-Fi network\u003c/strong\u003e. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.\u003c/p\u003e","title":"Configuring TP -Link TL-WR740N as WI-FI repeater"},{"content":"Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\nSteps Open the router login page, which is mostly : 192.168.1.1 in your case. After logging in, navigate to Network page, LAN IP Address tab. Change the Lan Dns Mode to : static Set the primary and secondary DNS address and click on Save/Apply. Perform a reboot of router to apply the changes. There are a lot of DNS providers out there most of them for free. However, please be wise while choosing them.\nI have chosen 1.1.1.1 DNS as my primary server which is provided by Cloudflare.\nI have set the secondary server to 8.8.8.8 which is provided by Google so that if one of the service is down, it will fallback to another.\nList of some of the best DNS providers list in r/sysadmin Psst\u0026hellip;\u0026hellip; Feeling Geeky ?\nPerform DNS benchmark tests : https://www.grc.com/dns/benchmark.htm\nCloudflare DNS validation test : https://1.1.1.1/help\nNeed more? Read the detailed guide for BSNL FTTH : (Fiber optimization\nWorth reading about 3rd party DNS resolvers\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-27-how-to-change-dns-server-for-syrotech-router-bsnl-ftth/","summary":"\u003cp\u003eOk, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\u003c/p\u003e\n\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eOpen the router login page, which is mostly : \u003ca href=\"http://192.168.1.1\"\u003e192.168.1.1\u003c/a\u003e in your case.\u003c/li\u003e\n\u003cli\u003eAfter logging in, navigate to Network page,  LAN IP Address tab.\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003eChange the \u003cem\u003eLan Dns Mode\u003c/em\u003e to : \u003cstrong\u003estatic\u003c/strong\u003e\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003eSet the primary and secondary DNS address and click on Save/Apply.\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003cli\u003ePerform a reboot of router to apply the changes.\u003c!-- raw HTML omitted --\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/syro-dns.png\"\u003e\u003c/p\u003e","title":"How to change DNS server for Syrotech Router [BSNL FTTH]"},{"content":"Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to Microsoft Events.\nThen you can register for the training of an exam of your choice by searching it in the page.\nSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\nAzure Data Fundamentals\nAzure Fundamentals\nAzure AI fundamentals.\nOnce you register the training and attend it, you can go to that certification website, schedule it through Pearson VUE. If the email used to register for the event is the same as the MSA (Microsoft Account) email used for certification, You should see a banner to claim 100% free voucher, click *claim *to get that, like below.\nThen, you can schedule the exam at your day of choice from home and get certified.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-26-how-to-claim-free-azure-certification-vouchers-after-attending-microsoft-events/","summary":"\u003cp\u003eMicrosoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to \u003ca href=\"https://events.microsoft.com\"\u003eMicrosoft Events\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/ms-event.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThen you can register for the training of an exam of your choice by searching it in the page.\u003c/p\u003e\n\u003cp\u003eSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/learn/certifications/azure-data-fundamentals/\"\u003eAzure Data Fundamentals\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/learn/certifications/azure-fundamentals/#certification-exams\"\u003eAzure Fundamentals\u003c/a\u003e\u003c/p\u003e","title":"How to claim free Azure certification  vouchers after attending Microsoft Events"},{"content":"Even though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\nActually we should be using **ss **tool installed of netstat !\nAll common network related tools are bundled with package net-tools.\nnwlab:/etc # rpm -qa | grep net-tools net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64 However in openSUSE 15, the team decided to knock it off from net tools package!\nSo, the solution ?\nsudo zypper install net-tools-deprecated\nnwlab:/etc # rpm -qa | grep net-tools net-tools-deprecated-2.0+git20170221.479bb4a-lp152.5.5.x86_64 Once installed, netstat should work totally fine now !\nnwlab:/etc # netstat -ano | grep 9000 tcp6 0 0 :::9000 :::* LISTEN off (0.00/0/0) ","permalink":"https://tanmay-bhat.github.io/posts/2021-01-25-how-to-install-netstat-tool-in-opensuse-15/","summary":"\u003cp\u003eEven though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\u003c/p\u003e\n\u003cp\u003eActually we should be using **ss **tool installed of netstat !\u003c/p\u003e\n\u003cp\u003eAll common network related tools are bundled with package net-tools.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enwlab:/etc # rpm -qa | grep net-tools\nnet-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHowever in openSUSE 15, the team decided to knock it off from net tools package!\u003c/p\u003e","title":"How to install netstat tool in openSUSE 15"},{"content":"Hey all ! For those of you who don\u0026rsquo;t know what PWD is below is short explanation :\nSo, PWD stands for Play With Docker. You can deploy learn docker at free with time limit of each instance up-to 10 Hrs!\nFor more info, go to : Docker Labs\nThere are two ways you can access the docker instance.\nUse the web based console. SSH into that instance. I always love to do ssh as it gives me more freedom.\nIf you go straight away and do ssh from your terminal, you will get :\nlab-suse:~/.ssh # ssh ip-x-x-x-@direct.labs.play-with-docker.com ip-x-x-x-@direct.labs.play-with-docker.com: Permission denied (publickey). Why are we getting this ? because there is no fresh key generated in your host.\nLets create a fresh key, run the below command and use default values :\nlab-suse:~/# ssh-keygen After you complete the above command, try ssh again, it should work:\nlab-suse:~/.ssh# ssh ip-x-x-x-@direct.labs.play-with-docker.com The authenticity of host \u0026#39;direct.labs.play-with-docker.com (40.76.55.146)\u0026#39; can\u0026#39;t be established. RSA key fingerprint is SHA256:UyqFRi42lglohSOPKn6Hh9M83Y5Ic9IQn1PTHYqOjEA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;direct.labs.play-with-docker.com,40.76.55.146\u0026#39; (RSA) to the list of known hosts. Connecting to Ip-x-x-x:8022 ########################### # WARNING!!!! # This is a sandbox environment. Using personal credentials # is HIGHLY! discouraged. Any consequences of doing so are completely # the user\u0026#39;s responsibilites. # The PWD team node1 root@192.168.0.28 Note : if you are using any ssh applications, save that key you generated to a file and load that file in authentication section.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-01-22-how-to-ssh-into-docker-in-pwd-play-with-docker/","summary":"\u003cp\u003eHey all ! For those of you who don\u0026rsquo;t  know what PWD is below is short explanation :\u003c/p\u003e\n\u003cp\u003eSo, PWD stands for Play With Docker. You can deploy   learn docker at free with time limit of each instance up-to 10 Hrs!\u003c/p\u003e\n\u003cp\u003eFor more info, go to : \u003ca href=\"https://labs.play-with-docker.com/\"\u003eDocker Labs\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eThere are two ways you can access the docker instance.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUse the web based console.\u003c/li\u003e\n\u003cli\u003eSSH into that instance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI always love to do ssh as it gives me more freedom.\u003c/p\u003e","title":"How to SSH into docker in PWD (Play With Docker)"},{"content":"Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\nThere are 2 ways you can install mhvtl :\nInstall the rpm package. Directly compile the source code yourself. I have tried using the first method as its easy and fast :D.\nNote: I have tested this install in openSUSE 15.2\nSteps 1.First update the packages to latest version available by typing :\nsudo zypper up\nOnce the packages are up to date, install the below supporting packages :\nsudo zypper install gcc gcc-c++ kernel-devel zlib-devel mt-st mtx lzo-devel perl Now add the repository and install the package : zypper addrepo https://download.opensuse.org/repositories/openSUSE:Leap:15.2:Update/standard/openSUSE:Leap:15.2:Update.repo zypper refresh zypper install mhVTL start the mhvtl service ;\nservice mhvtl start check if mhvtl service is running :\nservice mhvtl status test-machine:/home/azureuser # service mhvtl status ● mhvtl.target - mhvtl service allowing to start/stop all vtltape@.service and vtllibrary@.service instances at once Loaded: loaded (/usr/lib/systemd/system/mhvtl.target; disabled; vendor preset: disabled) Active: active since Thu 2021-01-14 17:13:36 UTC; 50min ago verify if you are able to see the tape library and the drives configured( by default) : test-machine:/home/azureuser # lsscsi -g [1:0:0:0] cd/dvd Msft Virtual CD/ROM 1.0 /dev/sr0 /dev/sg3 [2:0:0:0] disk Msft Virtual Disk 1.0 /dev/sda /dev/sg0 [3:0:1:0] disk Msft Virtual Disk 1.0 /dev/sdb /dev/sg1 [5:0:0:0] disk Msft Virtual Disk 1.0 /dev/sdc /dev/sg2 [6:0:0:0] mediumx STK L700 0162 /dev/sch0 /dev/sg12 [6:0:1:0] tape IBM ULT3580-TD5 0162 /dev/st0 /dev/sg4 [6:0:2:0] tape IBM ULT3580-TD5 0162 /dev/st7 /dev/sg11 [6:0:3:0] tape IBM ULT3580-TD4 0162 /dev/st3 /dev/sg7 [6:0:4:0] tape IBM ULT3580-TD4 0162 /dev/st4 /dev/sg8 [6:0:8:0] mediumx STK L80 0162 /dev/sch1 /dev/sg13 [6:0:9:0] tape STK T10000B 0162 /dev/st2 /dev/sg6 [6:0:10:0] tape STK T10000B 0162 /dev/st5 /dev/sg9 [6:0:11:0] tape STK T10000B 0162 /dev/st1 /dev/sg5 [6:0:12:0] tape STK T10000B 0162 /dev/st6 /dev/sg10 For more details about mhvtl, please refer the below link :\nhttps://sites.google.com/site/linuxvtl2/\n","permalink":"https://tanmay-bhat.github.io/posts/2021-01-14-how-to-install-mhvtl-in-opensuse/","summary":"\u003cp\u003eLets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\u003cbr\u003e\nThere are 2 ways you can install mhvtl :\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInstall the rpm package.\u003c/li\u003e\n\u003cli\u003eDirectly compile the source code yourself.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI have tried using the first method as its easy and fast :D.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote: I have tested this install in openSUSE 15.2\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"steps\"\u003eSteps\u003c/h2\u003e\n\u003cp\u003e1.First update the packages to latest version available by typing :\u003c/p\u003e","title":"How to install mhVTL in openSUSE"}]