[{"content":"What\u0026rsquo;s on Image Updater  A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.\n Capabilities :  Argo CD Image Updater can check for new versions of the container images that are deployed with your Kubernetes workloads and automatically update them to their latest allowed version using Argo CD. It works by setting appropriate application parameters for Argo CD applications, i.e. similar to argocd app set --helm-set image.tag=v1.0.1 - but in a fully automated manner.  Prerequisite :  Kubernetes Cluster ArgoCD setup  Steps Installation of ArgoCD Image Updater  To install argocd image updater in your cluster ( same one as argocd), run the below command:  kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml  Once it\u0026rsquo;s installed, let’s check the logs of the pod:  kubectl logs -n argocd -l app.kubernetes.io/name=argocd-image-updater time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Starting image update cycle, considering 0 annotated application(s) for update\u0026#34; time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=0 images_considered=0 images_skipped=0 images_updated=0 errors=0\u0026#34; Looks clean, let\u0026rsquo;s move forward.\nConfigure Remote Repository Secret  One of the main features of image updater is to write back / update the new image tag to the remote git repo i.e. it will update the image tag, for example, v1.2.3 in the manifest file every time a new image is pushed to image registry. I’ll be using Gitlab in this case. Feel free to use Github also.   Obtain the Access token of your account from Gitlab . Once received, run the below command to create the secret :  kubectl --namespace argocd \\ create secret generic gitlab-token \\ -from-literal=username=GITLAB_USERNAME \\ --from-literal=password=GITLAB_TOKEN  Verify the secret that has been created, by running the below command :  kubectl describe secret gitlab-token -n argocd Configure Container registry secret  I’m using DigitalOcean Registry as my cloud provider for storing docker images. There are two ways to configure the secret that Kubernetes will use to authenticate to registry endpoint i.e registry.digitalocean.com.   Automatic: Follow the on-screen instructions on the DigitalOcean console to auto-integrate the Kubernetes secret into every namespace.  Manual: Download the Docker Credentials and follow steps on this link to manually create a Kubernetes secret and then patch to service account of your namespace, argocd in this scenario.   Configure Image updater to watch for Image updates.   Image updater needs access to container registry to watch the Image updates.\n  Out of the box below Registries are supported :\n Docker Hub Registry Google Container Registry (gcr.io) RedHat Quay Registry (quay.io) GitHub Docker Packages (docker.pkg.github.com) GitHub Container Registry (ghcr.io)    If you’re storing images in any other registry other than the above-mentioned, no need to worry, we can utilize the config map to add the registry configs.\n  To Digital Ocean configs, Add the below to configmap : argocd-image-updater-config in argocd namespace:\n  registries: - name: \u0026#39;Digital Ocean\u0026#39; api_url: https://registry.digitalocean.com ping: no credentials: pullsecret:argocd/SECRET_NAME ( configured from above step ) defaultns: library prefix: registry.digitalocean.com Here the credentials are in this order : credentials:pullsecret:namespace/secret-name\nThere are other forms of secret configuration as well. Please refer to them here.\n Configure ArgoCD Application and Annotation  Once the Gitlab secret is configured, let\u0026rsquo;s move to the application configuration part. Create the Application manifest file for your application. Here’s a sample definition for kube-ops-view application :   Let’s understand the above file in detail :\n We’re installing this resource in argocd namespace. ( this is mandatory for argocd resources) Annotations :  image-list: tells the image updater, which docker image to watch \u0026amp; update. write-back-method: is updating the image tag back to manifest files via git commit git-branch: we’ll write back to the branch: main.   server: we’re deploying this to the default cluster. namespace: the actual namespace in which the application will be deployed. path : the path of your helm chart inside the remote git repository. targetRevision: the branch name to which argocd syncs apps. Since we’re using helm, I’m updating to use values from values.yaml file. CreateNamespace: if the mentioned namespace is not found, argocd will create it.  Once we’re good with the above configurations, to see the image updater in action\u0026hellip;\n Push a new image tag to your container registry :  \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB \u0026gt; docker tag registry.digitalocean.com/tanmaybhat/kube-ops-view:latest registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.4 a645de6a07a3 21 months ago 253MB \u0026gt; docker push registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 Once the image is pushed, let\u0026rsquo;s check the logs of image updater pod logs, it should look like this:\ntime=\u0026#34;2022-01-18T13:09:11Z\u0026#34; level=info msg=\u0026#34;git push origin main\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 execID=mvIL8 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=Trace args=\u0026#34;[git push origin main]\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 operation_name=\u0026#34;exec git\u0026#34; time_ms=2058.666776 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=\u0026#34;Successfully updated the live application spec\u0026#34; application=kube-ops-view-demo time=\u0026#34;2022-01-18T13:09:14Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=1 images_considered=1 images_skipped=0 images_updated=1 errors=0 Here’s what image updater did :\n Detected a new image tag update in the registry Cloned the repository Made the image update in the manifest file Pushed a commit back to the repository main branch  For further verification, let\u0026rsquo;s go to the repo where the helm chart exists, we should be seeing a new commit :\nLet’s see what changed in the file :\nNow, all we have to do is wait for 3 min ( default sync period of argocd), and argocd notices that a new image tag has been updated and the same will be applied to the application.\nLet’s check the image tag as well :\nkubectl describe deploy kube-ops-view -n kube-ops-view | grep Image Image: registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 ","permalink":"https://tanmay-bhat.github.io/posts/argocd-image-updater/","summary":"What\u0026rsquo;s on Image Updater  A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.\n Capabilities :  Argo CD Image Updater can check for new versions of the container images that are deployed with your Kubernetes workloads and automatically update them to their latest allowed version using Argo CD. It works by setting appropriate application parameters for Argo CD applications, i.e. similar to argocd app set --helm-set image.","title":"ArgoCD Image Updater with Digital Ocean Container Registry"},{"content":"Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\nThere are other solutions that grew out of Prometheus for ex Thanos or Cortex.\nI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\nBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring. Here’s how they explain:\nPulling over HTTP offers several of advantages:\n You can run your monitoring on your laptop when developing changes. You can more easily tell if a target is down. You can manually go to a target and inspect its health with a web browser.  Overall, we believe that pulling is slightly better than pushing, but it should not be considered a major point when considering a monitoring system.\nOne of the main problem where pull based mechanism won\u0026rsquo;t work is when\nPrometheus cannot directly reach the server to scrape metrics from it.\nFor this problem, push based mechanism comes to play. A bit about it :\n With a pull model, it is straightforward to determine whether a node is available using an up metric with a value of 1 when the target is reachable and 0 when it is not. With the push model, the up metric has a value of 1 when the server is running and no value at all when it is not. This distinction is important when monitoring whether your monitoring system is running as expected.  Solution : Grafana agent + Remote write 1. Configuring Prometheus as remote receiver endpoint  As of Prometheus v2.33.3, this feature is supported, and you can pass flag -web.enable-remote-write-receiver and your server endpoint example.com/api/v1/write will accept remote metrics. Here’s how the config looks if you\u0026rsquo;re running Prometheus in Kubernetes :  - name: prometheus-server image: quay.io/prometheus/prometheus:v2.33.3 args: - \u0026#39;--storage.tsdb.retention.time=15d\u0026#39; - \u0026#39;--config.file=/etc/config/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/data\u0026#39; - \u0026#39;--web.console.libraries=/etc/prometheus/console_libraries\u0026#39; - \u0026#39;--web.console.templates=/etc/prometheus/consoles\u0026#39; - \u0026#39;--web.enable-lifecycle\u0026#39; - \u0026#39;--web.enable-remote-write-receiver\u0026#39; 2. Configuring Grafana Agent Grafana Agent is actually Prometheus lite 😛. It just collects and pushes metrics to a remote server.\nNote: I’ll be running Grafana Agent in docker, but there wont be much change to running it via systemd.\n First would be to create the agent configuration file ( agent.yaml) :  server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 30s external_labels: environment: test-server  configs: - name: default scrape_configs: - job_name: agent static_configs: - targets: [\u0026#39;grafana-agent:12345\u0026#39;] - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;node_exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] remote_write: - url: https://example.com/api/v1/write basic_auth: username: admin password: secret-password  This config is almost identical to a regular Prometheus configuration file. scrape_interval : Time interval at which agent should scrape / collect the metrics. external_labels : Label which you can add for all metrics agent sends to Prometheus for easier identification and analysis later ( key-pair) . We have 3 scrape Jobs in scrape_configs . Each Job tells Prometheus what \u0026amp; where to scrape. remote_write is where the magic happens, its the location where agent should send the metrics it collected. Basic_auth : Basic Authentication for authenticating with the /api/v1/write endpoint.  Basic Auth is not a Mandatory Option, but it sure is necessary, else you’re endpoint will be open to public !!! Prometheus needs to be configured with Basic auth initially. Please follow this doc to set it up.    Once the config file is completed, you can use the below Docker-compose file to get started with the Grafana agent + cadvisor ( container metrics) + node-exporter ( machine metrics) :\nversion: \u0026#34;3\u0026#34;services:  grafana-agent:  image: grafana/agent:v0.23.0  container_name: grafana-agent  volumes: - /path/to/data/:/etc/agent/data - /path/to/agent.yaml:/etc/agent/agent.yaml node-exporter: image: prom/node-exporter:v1.3.1 container_name: node_exporter command: - \u0026#39;--path.rootfs=/host\u0026#39; network_mode: host pid: host volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; cadvisor: image: gcr.io/cadvisor/cadvisor container_name: cadvisor volumes: - /sys:/sys:ro - /:/rootfs:ro - /var/run:/var/run:rw - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro For Grafana Agent, the volume folder needs to be created before running the docker-compose up command. Grafana agent needs data folder because in-case the agent container restarts, it can resend the metrics which couldn\u0026rsquo;t be sent earlier. It stores metrics up-to last 1 hour. in-case you think it’s too much you can configure metrics DROP for the time series which are not needed in Grafana config file itself Doc.  Once both of the above steps are complete, you can check if metrics are being pushed to your Prometheus server by running the below queries :\nFor Node metrics, run the PromQL query :\nrate(node_cpu_seconds_total{environment=\u0026#34;test-server\u0026#34;}[5m]) which should look like : For Container metrics, run the PromQL query :\ncontainer_memory_usage_bytes{environment=\u0026#39;test-server\u0026#39;,name=\u0026#39;grafana-agent\u0026#39;} Now it\u0026rsquo;s up to the DevOps engineer to write useful PromQL queries, Create Grafana dashboards and configure alerts for the same to make use of these metrics.\n","permalink":"https://tanmay-bhat.github.io/posts/prometheus-remote-reciever/","summary":"Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\nThere are other solutions that grew out of Prometheus for ex Thanos or Cortex.\nI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\nBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring.","title":"How to configure Prometheus server as a remote receiver"},{"content":"What\u0026rsquo;s Apps of Apps or Cluster bootstrapping ?  The App of Apps Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.\n ArgoCD will watch the root application and synchronize any applications that it generates.\nRequirement  You may have this question as to why is this even needed, where as you can manaully create the application via argo cli or via argo UI. But once the number of application you manage increases, automation needs to be done. For example, if you have a new app that needs to be created \u0026amp; deployed in your cluster via ArgoCD, apart from argo CLI and UI ( both are manual ) we really dont have any automation on creation of apps except custom scripting.  Behold : Apps of apps Actually, it took me a while to understand what exactly it is because the official doc is not well written.\nSince the concept is pretty new and as far as I know, not much of argocd users are utilising this.\nNow that we know why, what lets go over to how.\nStructure  Components  Root app : the single app you need to deploy to your cluster ( parent/root) child app : your actual microservice applications ( child) child app template : argocd Application kind template describing your application application manifests : the actual micro-service definitions.    High level flow\nGit repo → root app → application templates → actual applications\nPrerequisites\n Kubernetes cluster Helm v3 ArgoCD installed  Demonstration For the purpose of demonstration, here are the components described above :\n root app template : root.yaml application templates : manifests application definitions : charts/monitoring  Let’s take a look at root.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: root-app namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: default project: default source: path: ./apps-of-apps/manifests/ repoURL: https://gitlab.com/Tanmay-Bhat/argocd.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true  Name : suggests the name of my root application i.e root-app namespace : all argocd based resources should reside in argocd namespace itself Finaliser : its the kubernetes CRD finalizer which help in protection from accidental deletion of resources. Destination : the cluster onto which the resources should deploy. If you have multiple clusters , mention the required one here. namespace: the actual namespace in which you want to deploy your app path : path inside your Git Repo where the application manifests are. repoURL : the GIT repo address. targetRevision: if its HEAD, argocd will always sync to master branch, if you need to be a different one, update that here. For example, staging branch for Dev environment etc. prune : this enables argocd to auto remove your application and its created resources when you delete the manifest file from the Application templates directory. selfHeal : this enables argocd to auto patch the configs same as latest master branch changes even though new config updates has been done via kubectl.  Let’s look at one of the actual application template inside manifests, kube-state-metrics.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: kube-state-metrics namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: kube-system project: default source: path: ./Applicationset/charts/monitoring/kube-state-metrics repoURL: https://github.com/tanmay-bhat/ArgoCD.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true  name: name of my application namespace : I’m deploying the app to kube-system namespace. path: the path inside GIT repo where hem charts are for the app targetRevision: I’m synching the changes to master branch. repoURL : repo where the helm charts / manifests of app is stored.  Once, we understand the above, we can proceed to try them out :\n  Install the root app by running :\nkubectl apply -f root.yaml -n argocd\n  Once installed, go to your ArgoCD UI and you should be able to see:\n  Now that our root app is created and healthy, give it a minute and any application template you create inside manifests folder (in my case), will be automatically synced and created for you in the specified namespace.\n  In the above picture, you can see root app is managing multiple apps and auto-syncs when you make changes to the helm charts (github repo in my case)\n  There’s a little arrow in each of the app, if you click on it, it will take you to the applciation page of it in argocd :   If you need to add a new application, the flow goes like this, add the actual helm chart/manifest file to your GIT repo → add an argocd template to your templates directory → sit back and enjoy !!\n  I’ve added the argocd template and bam !! app is automatically created :    Problems  Synching apps across multiple clusters.  Lets say you want to have node-exporter in multiple clusters, its not possible via apps-of-apps directly. For that you can create a application template and in destination update your cluster name as registred in argocd. This problem is solved by Aplicationset from ArgoCD.   Support for multiple values.yaml file for helm charts.  Let’s say you want to apply node-selector to 2 clusters dev and prod each with their own customized values which is not possible at the moment.    ","permalink":"https://tanmay-bhat.github.io/posts/2022-01-11-introduction-to-argocd-apps-of-apps/","summary":"What\u0026rsquo;s Apps of Apps or Cluster bootstrapping ?  The App of Apps Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.","title":"Introduction to ArgoCD : apps of apps"},{"content":"You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster. Before we begin, here are the prerequisites :\n Kubernetes cluster Cluster autoscaler Bit of patience  Usecase :  One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra. You maybe hosting workload in Kubernetes where you wont get traffic post business hours. Or in weekends, you just want to scale down as no traffic flows to your apps during that time. The cost to keep those worker nodes at off hours are pretty high if you calculate for a quarter or for a year.  Solution : Though there isn\u0026rsquo;t any one click solution, Kubernetes finds a way or always Kubernetes Admin does !!\nStrangely, there isn\u0026rsquo;t any tool out of the BOX from AWS side, heck not even a blog on how can customers achieve that. Ahem, GCP aced in this scenario.\nBehold \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nKube downscaler : Kube downscaler is a FOSS project by Henning Jacobs who’s the creator of famous project kube-ops-view.\nThis project fits exactly to our requirement as it can scale down below resources in specified timeframe :\n Statefulsets Deployments (HPA too) Cronjobs  Installation :  Clone the repository:  git clone https://codeberg.org/hjacobs/kube-downscaler  Update the configmap file deploy/config.yaml to your Cluster TIMEZONE and desired uptime, here’s mine :  apiVersion: v1 kind: ConfigMap metadata: name: kube-downscaler data: # timeframe in which your resources should be up DEFAULT_UPTIME: \u0026#34;Mon-Fri 09:30-06:30 Asia/Kolkata\u0026#34;  Apply the manifest files :  kubectl apply -f deploy/ Working and configuration:   As soon as the downscaler pod runs, you can check the logs of it, it should look like ‘Scale down deployment/myapp to replicas 0 (dry-run)`\n  As a safety-plug no scaling operations will happen when the pod starts as --dry-run argument is enabled. Remove that by patching the deployment to start the scaling activity:\n  kubectl patch deployment kube-downscaler --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/args\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;--interval=60\u0026#34; ]}]\u0026#39;  Once, dry-run argument is removed, all the resources ( deployment, Statefulset \u0026amp; cronjob) wil be scaled down to 0 ( default replica) if current_time ≠ default_uptime mentioned in above mentioned configmap.\n  Incase you need to exclude any app from being scaled down, you can annotate that depoyment/statefulset/cronjob with :\n  kubectl annotate deploy myapp \u0026#39;downscaler/exclude=true\u0026#39; If you want to have minimum 1 replica after scale down activity, you can annotate the resource :  kubectl annotate deploy myapp \u0026#39;downscaler/downtime-replicas=1\u0026#39; Note : No need to annotate uptime value in each deployment or statefulset since by default all pods will be scaled down.\nAdditional tunings like namespace based annotation etc are available at the readme Here.\nAchieving node scale down : Once the pods are scaled down, assuming you have cluster autoscaler configured, it should automatically remove the nodes that are unused or empty from your nodegroup.\nNote: Cluster autoscaler is mandatory since at the end of the day that’s what removes worker nodes to save your bill.  ","permalink":"https://tanmay-bhat.github.io/posts/how-to-scale-down-kubernetes-cluster-workloads-during-off-hours/","summary":"You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster. Before we begin, here are the prerequisites :\n Kubernetes cluster Cluster autoscaler Bit of patience  Usecase :  One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra. You maybe hosting workload in Kubernetes where you wont get traffic post business hours.","title":"How to scale down Kubernetes cluster workloads during off hours"},{"content":"One Loadbalancer to rule them all ? you heard it true, Its achievable !\nFor AWS LoadBalancer Controller Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money. Hence we thought how can we use a **single loadbalancer ** across all the namespaces.\nHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 Lets take a look at ArgoCD ingress :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Now, the problem with the above configuration is, by default AWS loadbalancer contoller will create ALB for each namespace. The cost of creating ALB for each namespace is $17.99 and data transfter charges are $0.02 per GB. If you got a lot of namespaces, the cost will be alot over the year. This is not Loadbalancer are supoosed to be used as ALB can handle alot of load in a single unit.\nAfter hunting for a while, we found that the solution lies in using a ALB annotation called group.name on the Ingress object.\nThe flow  Assuming you got 5 different ingress with 5 ALB in backend in different namespaces, choose one ingress and add the annotation : alb.ingress.kubernetes.io/group.name: \u0026lt;ANYTHING_MEANINGFULL\u0026gt;  Now, apply the same group name annotation to all the ingress objects in all namespaces. AWS LoadBalancer controller will use the first ingress Loadbalcner for all ingress objects and all other 4 ALB in this case will be removed.  Below is an example for 2 different namespaces with single ALB :\n#default namespace ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 #argocd namespace ingerss apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Notice how both ingress objects are using the same annotion with same Group name.  Whenever next time you need to create a new ingress in differnt namesapce, you can just add the same *group.name * annotation and it works flawlessly.\n For NGINX Ingress Controller If you are in any other cloud, lets say DigitalOcean, its cloud provider controller doesn\u0026rsquo;t have any custom LB controller. But dont get sad there buddy, NGINX to the rescue.\n  You can leverage NGINX ingress controller to create \u0026amp; manage LB for you.\n  For installing NGINX ingress controller, you can refer thier documentation which has all the steps to install it.\n  Lets say you got 2 different namespaces with 2 different ingress objects same as above one for default and one for ArgoCD.\n  The solution is to add ingressClassName: nginx in the ingress object **Spec ** section in each ingress file.\n  So the final ingress file looks like this for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default spec: ingressClassName: nginx rules: - host: chartmuseum.example.com http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Argocd Ingress namespace ingress looks like this:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd spec: ingressClassName: nginx rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 Happy Loadbalancing :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-19-using-single-load-balancer-across-multiple-namespaces-in-kubernetes/","summary":"One Loadbalancer to rule them all ? you heard it true, Its achievable !\nFor AWS LoadBalancer Controller Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money. Hence we thought how can we use a **single loadbalancer ** across all the namespaces.\nHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:","title":"Using Single Load Balancer across multiple namespaces in Kubernetes"},{"content":"Most DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\nWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\nHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces. I\u0026rsquo;ll list down why :\n Super easy to set up. Damn cheap. $5 for 250GB storage. One-click public/private button. Easy to integrate CDN if you have any.  That being said, Let\u0026rsquo;s look at how we can utilize Spaces as an AWS S3 replacement to host our helm charts.\nSetup \u0026amp; Configuration 1. Create Spaces  Log in to your console and click on Spaces and select Create spaces for $5 The name has to be unique and make the permission (File Listing) private.   Once done, you should have the endpoint of the spaces created like :   Now, go to API settings and create Access keys for Spaces. Please note that the secret key will be displayed only once and hence keep it safe copied somewhere. With that said, you\u0026rsquo;re ready to move to the next step.  2. Install Chartmuseum  Add helm repo :  helm repo add chartmuseum https://chartmuseum.github.io/charts  Update the repo :  helm repo update  Here, we can just run helm install since we need to tell chartmuseum to use Space as a holy place to store charts instead of local PVC ( default). Download the chart to your local system by running :  helm pull chartmuseum/chartmuseum —untar Update the below fields in values.yaml file :\nSTORAGE: \u0026#34;amazon\u0026#34; STORAGE_AMAZON_BUCKET: \u0026lt; your space name \u0026gt; STORAGE_AMAZON_REGION: \u0026lt; REGION in which your space is created\u0026gt; STORAGE_AMAZON_ENDPOINT: \u0026#34;https://REGION_NAME.digitaloceanspaces.com\u0026#34; #enable API to interact with endpoint  DISABLE_API: false #secret section BASIC_AUTH_USER: admin  # password for basic http authentication BASIC_AUTH_PASS: secret_password123 #Spaces access key AWS_ACCESS_KEY_ID: \u0026#34;YOUR KEY\u0026#34; #spacess secret key AWS_SECRET_ACCESS_KEY: \u0026#34;YOUR SECRET KEY\u0026#34; That\u0026rsquo;s it, run the below command to install the chart :\nhelm install chartmuseum chartmuseum/ -f chartmuseum/values.yaml Next, add an entry in ingress to point to your Repository URL. For example :\nhost: chartmuseum.tanmaybhat.tk http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Please note that you need to set False for Key Disable_API, else you cant send any data to your endpoint.\nThat\u0026rsquo;s it, you can now add your repo to your helm by typing :\nhelm repo add chartmuseum chartmuseum.example.com -u USERNAME -p PASSWORD If you want to push chart to this repo, you can do that by installing the helm push plugin:\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0 Once installed, push the chart by running :\nhelm push chart_directory chartmuseum Happy Helming !\n References Artifact Hub\nChart Museum\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-hosting-the-chartmuseum-in-digital-ocean-space/","summary":"Most DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\nWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\nHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces.","title":"Hosting the Chartmuseum in DigitalOcean Spaces"},{"content":"Introduction Though this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\nUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today. I learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.\nFrom Kaniko\u0026rsquo;s doc :\n kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster.\n kaniko doesn\u0026rsquo;t depend on a Docker daemon and executes each command within a Dockerfile completely in userspace. This enables building container images in environments that can\u0026rsquo;t easily or securely run a Docker daemon, such as a standard Kubernetes cluster.\nLet\u0026rsquo;s see how to achieve this in our pipeline. For simplicity, I\u0026rsquo;ll be using Gitlab CI in this example. You can use circle CI or GitHub actions or anything you like (a little bit of modification required ).\nPrerequisites\n AWS IAM credential ( Access-key and Secret-key) with ECR full access. Bit of time to implement the below 😀  Setup of AWS credentials Go to CI-CD settings of your project and set the below variables with appropriate values:\nAWS_ACCESS_KEY_ID = \u0026lt;your access key\u0026gt; AWS_SECRET_ACCESS_KEY = \u0026lt;your secret key\u0026gt; Gitlab-CI The GitLab CI file should look like below :\nimage: alpine stages: - build_and_push build and push docker image: stage: build_and_push only: variables: - $CI_COMMIT_TAG =~ /^v[0-9]+\\.[0-9]+\\.[0-9]+$/ variables: AWS_DEFAULT_REGION: REGION_NAME CI_REGISTRY_IMAGE: YOUR_ID.dkr.ecr.REGION_NAME.amazonaws.com/REPO_NAME image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\u0026#34;\u0026#34;] script: - mkdir -p /kaniko/.docker - echo \u0026#34;{\\\u0026#34;credsStore\\\u0026#34;:\\\u0026#34;ecr-login\\\u0026#34;}\u0026#34; \u0026gt; /kaniko/.docker/config.json - /kaniko/executor --context \u0026#34;${CI_PROJECT_DIR}\u0026#34; --dockerfile \u0026#34;${CI_PROJECT_DIR}/Dockerfile\u0026#34; --destination \u0026#34;${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}\u0026#34; Let\u0026rsquo;s look at the above pipeline in detail :\n We\u0026rsquo;re using the base image as alpine. We have one stage which is will build the Dockerfile and push to ECR using Kaniko As we have mentioned only constraint, the pipeline will only trigger when a new tag is pushed to Master branch. Next, we have 2 variables, in which we\u0026rsquo;re defining the default AWS region and our Registry address of ECR. ( please update with your values) Next, we\u0026rsquo;re using the Kaniko base image to build run the scripts mentioned and build our image. Then we\u0026rsquo;re making a docker folder that will have the registry to push credentials. Note that ECR regular login is a bit different than other container registries like Quay or GCR. You won\u0026rsquo;t get the regular username and password for this repo from AWS side. You must know that to log in to ECR, you need to run aws ecr get-login command which will give an authentication token that has a TTL of 12 hours, which doesn\u0026rsquo;t work in our case. Luckily was has created a new ECR login provider extension that will work through IAM permissions. Kaniko has built-in support for that provider, so you just need to add the variable of AWS creds in GitLab CI and Kaniko will take care of the rest. ( magic !) Then we provide Kaniko the path to Dockerfile which will be inside our current project, hence the use of CI_PROJECT_DIR which is a pre-defined variable from GitLab CI points to the current project context. Then I\u0026rsquo;m tagging the image with the latest tag from the repository and pushing to the ECR.  Happy CI-CDing !!!\n Reference :\nhttps://github.com/GoogleContainerTools/kaniko\u0026quot;\u0026gt;https://github.com/GoogleContainerTools/kaniko https://gist.github.com/tanmay-bhat/6bc6d6034644ef010d841ea8373a41d6\u0026quot;\u0026gt;https://gist.github.com/tanmay-bhat/6bc6d6034644ef010d841ea8373a41d6  ","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-using-kaniko-to-build-and-push-images-through-gitlab-ci-to-ecr/","summary":"Introduction Though this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\nUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today. I learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.","title":"Using Kaniko to build and push images to Gitlab-CI to ECR"},{"content":"This happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the us-east-1 region. I asked him about the error message and he said ssh is giving a time-out error.\nSo, I tried connecting to the server via EC2 connect feature (web shell) that AWS provides, and even that said connection timed out.\nTried telnet to the endpoint and was the same also.\nI thought maybe the server may be struck due to overload and restarted it. But the state was still the same once it came up. I saw the metrics of the server via Cloudwatch and everything was fine, saw system logs also and even that looked good.\nCuriously opened status.aws.amazon.com and saw that us-east-1 Region is having an outage.\nBeing a Reddit fan, opened r/sysadmin and I could see people all over the world complaining about AWS being down in that region and 1000\u0026rsquo;s of memes on that topic. I told myself this could be mostly due to the AWS outage and I\u0026rsquo;ll see once they fix it.\nCut to the next day because the outage took 19 long hours to fix the outage. long live SLA!\nI still was not able to connect to the instance post AWS fix. After digging for X time, turns out, the issue was with the subnet in which EC2 was launched. Someone mistakenly attached NAT gateway to the public subnet instead of the Internet gateway. Updated the correct config in the Route-table of the subnet and it worked.\nOne tiny missing detail can totally mess your mind up.\nAnother day of learning :D\nHappy DevOpsing !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-a-tale-of-ec2-connectivity-issue/","summary":"This happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the us-east-1 region. I asked him about the error message and he said ssh is giving a time-out error.\nSo, I tried connecting to the server via EC2 connect feature (web shell) that AWS provides, and even that said connection timed out.\nTried telnet to the endpoint and was the same also.","title":"A tale of EC2 connectivity issue"},{"content":"Hey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a Kubernetes challenge and guess what they\u0026rsquo;re giving away free credits of $120 to try it out free!!!\nThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD. Let\u0026rsquo;s get started!\nChallenge Details  Link of the challenge page Here Pick one challenge from the list mention in above link based on your knowledge. Create a GitHub or GitLab repo for your project Fill out the code challenge form to get DigitalOcean credits for your project Join the #kubernetes-challenge channel in the DigitalOcean Deploy Discord Complete your challenge Write about what you’ve built and share it on a blog or in your project README. Make a pull request against the Kubernetes Challenge Github Repo with information about your project Let them know you’ve completed your challenge by filling out this form  Now that we\u0026rsquo;ve applied let\u0026rsquo;s take a look at one of the challenges I chose :\nDeploy a GitOps CI/CD implementation GitOps is the (only) way automate deployment pipelines for Kubernetes environments in 2022, and ArgoCD is currently one of the leading player. Install it to create a CI/CD solution, using GH Actions for actual image building.\nCluster Creation  Sign in to your DO console. Click on NEW button and create a Kubernetes cluster with default values. You can customize the location of cluster nearest to your location to avoid altency issues to API server. Once you submit, it\u0026rsquo;ll take around 10-15 min for the worker nodes and API server to become ready.   Click on the Actions button and download the kubeconfig file.   Once you download, install kubectl binary by following steps in the Getting started section of *overview *tab. Once, kubectl in installed in your local, you can save / move the config file downloaded to your ~/.kube/config location. Now, you can connect to your API server, test it by running :  kubectl get node -o wide Project setup  Clone this repository using below command :  git clone https://github.com/tanmay-bhat/DigitalOcean-Kubernetes-Challenge-argoCD   This project contains below:\n go app Dockerfile github actions file ( CI) Kubernetes manifest files    Let\u0026rsquo;s Look mainly kustomize/base :\n Here, Deployment.yaml file contains the deployment resource YAML file.  containers: - image: registry.digitalocean.com/tanmaybhat/saymyname name: saymyname ports: - name: http containerPort: 8080 imagePullSecrets: - name: tanmaybhat   Notice the image registry I\u0026rsquo;m using is the Digital Ocean registry itself and not the mostly used Docker Hub.\n  The ImagePullSecrets has a name: Tanmay Bhat. This is the Kubernetes secret which has the Digital Ocean registry credentials which we will use to pull the image.\n  Now let\u0026rsquo;s look at our Github actions config file :\n  name: Go on: push: branches: [ main ] tags: - \u0026#39;v*.*.*\u0026#39; jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.x uses: actions/setup-go@v2 with: go-version: ^1.14 - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Login to Digitalocean uses: docker/login-action@v1 with: registry: registry.digitalocean.com username: ${{ secrets.DIGITAL_OCEAN_TOKEN }} password: ${{ secrets.DIGITAL_OCEAN_TOKEN }} - name: push image to digitalocean run: |docker build -t registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} . docker push registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} deploy: name: Deploy runs-on: ubuntu-latest needs: build steps: - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: update image tag in manifest uses: imranismail/setup-kustomize@v1 - run: |cd kustomize/base kustomize edit set image registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} - name: Commit files run: |git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -am \u0026#34;update image tag to ${{ env.GIT_TAG }}\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }}  I\u0026rsquo;m gonna explain the above section since the main goal of this article is to do CI/CD :\n The on section says trigger this piepline if the chnages has been pushed to **Main branch with a tag in format : vx.x.x ( i.e v1.0.0 etc) On each tag push, pipeline will run 2 jobs. Build and Deploy. In Build section, the follwing steps will run on ubuntu image. Check out code step uses pre-build action actions/checkout@v2 to clone current repository into the piepline container i.e ubuntu. Extract Git Tag is used to get the latest tag pusued to main branch and store it in th environmental variable GIT_TAG. In Login to Digitalocean, since we need to push our build docker images to a private registry like Digital Ocean, I\u0026rsquo;m using docker login action to auttenticate to the DO registry. In push image to digitalocean, I\u0026rsquo;m buidling the docker image and tagging it to latest pushed tag version and pushing to my registry. Next comes, the Deploy section. here again I\u0026rsquo;m using ubuntu as base image and again getting the repository from main branch and extracting tag version from the repositiry. Once that is done, I\u0026rsquo;ll use a tool called Kustomize to update my manifest file\u0026rsquo;s docker image tag to the latest tag version. If you\u0026rsquo;re using helm charts only but not kustomize with Helm, you need to use Sed command and update the image tag in manifest file ( deployment.yaml). Later, I\u0026rsquo;m doing the commit of latest tag edit and pushing the changes back to my repo.   To sum up, what the exact pipeline does whenever a new tag is pushed to main branch :\n clone the repository, build the docker image, tag it and push it to registry. update the tag in manifest file and push it back to gthe repository.  You might have this question, Tanmay, this is just CI, where\u0026rsquo;s CD ? well, that\u0026rsquo;s the magic ArgoCD solves for us.\nArgoCD  Setup ArgoCD by running the below commands :  kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Once done, you can verify its running status by running the command  Next step is to retrieve the password of argocd. For that, run :  kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d By default argocd service type will be ClusterIP. That means you cant access argocd outside of your cluster. So, Let\u0026rsquo;s change that to LoadBalancer by running :  kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Now, wait for couple more minutes for LoadBalancer to start in Digital Ocean and get the endpoint of it by running :  Open the external IP in your browser and voila, you should see argocd UI login page. Login with username: admin password got from step 3.  ArgoCD Configuration  Once logged in to ArgoCD UI, click on new app and set the below values:  application name : demo-argocd Project : default Sync Policy : Automatic Repository URL : \u0026lt;GITHUB REPO URL OF YOUR PROJECT\u0026gt; Rivision : HEAD Path : kustomize/base Destination Cluster : https://kubernetes.default.svc Namespace : default Click create and see your app glowing in your cluster.\nthe ArgoCD magic here is that, it watches for any new changes to your repo every 3 minutes ( default ) and new changes will be auto-applied in your cluster.\nSee the comment that says: update the tag to v1.0.12 ? that was my last commit. If a new tag is committed, here\u0026rsquo;s how it\u0026rsquo;s gonna update and look\nConclusion I found DOKS to be extreme easy to set up and straightforward. From click to integrate Registry to a Kubernetes cluster, easy cluster creation and scaling up.\n ","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-journey-to-the-kubernetes-world-with-digital-ocean/","summary":"Hey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a Kubernetes challenge and guess what they\u0026rsquo;re giving away free credits of $120 to try it out free!!!\nThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD.","title":"Journey to the Kubernetes world with Digital Ocean"},{"content":"History You may have faced this scenario where you wanna keep scaling up apps nodes but also under-keeping costs at a limit. Spot Instance is the way for that task. Now, how do we do that? let\u0026rsquo;s see.\nAs you know there are mainly 2 types of instances in AWS, called On-demand and Spot. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.\nSpot instances are a bit different. Spot instance is the unused capacity in AWS cloud sitting idle and AWS gives that to you at an extremely low price for like 80-90% cheaper than on-demand. The difference being whenever AWS needs the capacity to handle on-demand, it takes back the spot instances with ~2 min notice via notification to you.\nNode groups Now that we\u0026rsquo;ve learned about what spot offers, it makes total sense to include that in your workload to save quite a lot of money. So let\u0026rsquo;s learn about the node groups for on-demand and spot.\nDesigning node groups :  Since spot can go down anytime, you should always run your critical workloads in on-demand instances. All stateful- sets should run in on-demand instances. Have multiple nodegroups for spot so that you can maximize chance of getting spot instances. Use CA for scaling up / down.  Real-world scenario  Now, let\u0026rsquo;s say you have critical apps running in on-demand NG and other cronjobs or monitoring stacks are in spot NG. If you wanted to schedule pods with the below architecture I got the answer.\nRequired architecture: If your app has 8 replicas, 4/5 of them should run in on-demand NG and 3/4 of them in spot NG such that even if the spot goes down, ondemand can handle the load until the new spot comes in and takes the load. In this way, you\u0026rsquo;ll have \u0026lsquo;Zero Downtime\u0026rsquo;.\nNow for the above problem, there isn\u0026rsquo;t a straightforward or clear-cut solution out of the box. But I\u0026rsquo;ll explain the way I\u0026rsquo;ve implemented it.\nSolution Once both the NG are created, let\u0026rsquo;s take a look at the label of a spot node.\nkubectl describe no ip-100-45-51-226.ap-south-1.compute.internal | grep SPOT eks.amazonaws.com/capacityType=SPOT Any node created by spot will have the above label and any node with ondemand will have label :\neks.amazonaws.com/capacityType=ON_DEMAND Once, they are done, you can create a sample Nginx deployment with the below configs:\n All the other configs are pretty easy to understand and come under basic Kubernetes concepts. Since nodes created by spot and on-demand NG will have the above-mentioned labels, we can utilize that and request scheduler to try its best effort to schedule 40% of pods in this deployment to SPOT and 60% to ON_DEMAND.\nYou can change the above weight as per your needs. Once the above YAML is deployed, let\u0026rsquo;s take a look at the way pods are scheduled.\nIn my case, nodes with names 25 and 226 are Spot instances. If calculated correctly, 6 pods are running in on-demand and 4 pods are running in spot NG which is exactly as we expected.\nNOTE: This may not be always exactly the ratio you need since the scheduler gives pods to nodes on a best effort basis. But it\u0026rsquo;ll be almost a similar result.\nHappy Kuberneting !!!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-11-19-scheduling-pods-in-both-spot-and-on-demand-nodes-in-eks/","summary":"History You may have faced this scenario where you wanna keep scaling up apps nodes but also under-keeping costs at a limit. Spot Instance is the way for that task. Now, how do we do that? let\u0026rsquo;s see.\nAs you know there are mainly 2 types of instances in AWS, called On-demand and Spot. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.","title":"Scheduling pods in both Spot and On-demand nodes in EKS"},{"content":"If you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in Skit.ai as a DevOps Engineer.\nOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\nDefinition Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:\n There are pods that failed to run in the cluster due to insufficient resources. There are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes.   Documents   If you\u0026rsquo;re going to implement autoscaler in your EKS cluster, please read the FAQ .\n  The setting up of autoscaler in EKS is perfectly written by AWS document here\n   Once, things are set up, the logs should look like below :\nNow if you\u0026rsquo;re getting this, then it means the setup is clean. If we take a closer look at logs, it says node minimum size reached and cant scale down anymore.\nLet\u0026rsquo;s understand scaling up and scale down criteria and it\u0026rsquo;s working.\nScale-down flow   Every 10 seconds (configurable by --scan-interval flag),Cluster Autoscaler checks which nodes are unneeded. A node is considered for removal when all below conditions hold: The sum of cpu and memory requests of all pods running on this node is smaller than 50% of the node\u0026rsquo;s allocatable.  All pods running on the node (except these that run on all nodes by default, like manifest-run pods or pods created by daemonsets) can be moved to other nodes.  It doesn\u0026rsquo;t have scale-down disabled annotation (see How can I prevent Cluster Autoscaler from scaling down a particular node?)    If a node is unneeded for more than 10 minutes, it will be terminated.\n  Cluster Autoscaler terminates one non-empty node at a time to reduce the risk of creating new unschedulable pods.\n  The next node may possibly be terminated just after the first one, if it was also unneeded for more than 10 min and didn\u0026rsquo;t rely on the same nodes in simulation (see below example scenario), but not together. Empty nodes, on the other hand, can be terminated in bulk, up to 10 nodes at a time.\n  What happens when a non-empty node is terminated? As mentioned above, all pods should be migrated elsewhere.\n  Cluster Autoscaler does this by evicting them and tainting the node, so they aren\u0026rsquo;t scheduled there again.\n  Also, you should consider the below point :\n If there\u0026rsquo;s a node which is under-utilized but that node counts towards minimum node group size, then CA wont terminate that node and the logs will be similar to above screenshot.  Scale-up flow  Scale-up creates a watch on the API server looking for all pods. It checks for any unschedulable pods every 10 seconds (configurable by --scan-interval flag). A pod is unschedulable when the Kubernetes scheduler is unable to find a node that can accommodate the pod. For example, a pod can request more CPU that is available on any of the cluster nodes. Unschedulable pods are recognized by their PodCondition. Whenever a Kubernetes scheduler fails to find a place to run a pod, it sets \u0026ldquo;schedulable\u0026rdquo; PodCondition to false and reason to \u0026ldquo;unschedulable\u0026rdquo;. If there are any items in the unschedulable pods list, Cluster Autoscaler tries to find a new place to run them.  Testing the CA  Let\u0026rsquo;s assume you got 2 t3.medium node and the min value of nodegroup is 2 with max value set to 5.\u0026lt; Run a nginx deployment with 500 replicas to see if cluster autoscaler scales up the nodes.. The command would be :  kubectl create deployment cluster-killer --image=nginx --replicas=500  2 nodes of that size can\u0026rsquo;t handle 500 pods of nginx, so they should be in pending state and CA scans for pending state pods every 10 seconds which should start couple of nodes within minutes. You can verify from command : kubectl get node Once all pods are scheduled, to test scale down, you can either delete the deployment using :  kubectl delete deployment cluster-killer  Or scale down the replicas to zero with command :  kubectl scale deployment cluster-killer --replicas=0  If you refer the logs of cluster autoscaler now, it will mention that X node is uneeded for X min etc. The cool down period by default is 10 min so, after that time, it\u0026rsquo;ll apply taint on that node with name DeletionCandidateOfClusterAutoscaler and ToBeDeletedByClusterAutoscaler and removes the nodes. It looks like below:   ","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-a-closer-look-at-cluster-autoscaler-for-eks/","summary":"If you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in Skit.ai as a DevOps Engineer.\nOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\nDefinition Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:","title":"A closer look at Cluster Autoscaler for EKS"},{"content":"Definition Let\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\nIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\nProblem statement Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like : kubectl edit pv your_PV and just change the size, it used to work since you have storage class of kubernetes.io/aws-ebs.\nNow, You can\u0026rsquo;t resize your PV just by changing the size in the manifest file (\u0026gt; EKS v1.17)\nwhat should you do as a Kubernetes admin if you wanna resize your PV with above mentioned version ?\n Prerequisites:  AWS EKS cluster Eleveated IAM permissions Understanding of PV in Kubernetes  Solution  Simple, Kubernetes team has a new tool called ebs-csi controller. What does it do?  The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.\n You can install the ebs-csi driver by referring to AWS document .    Once you installation is done, you should see the pods similar to :   And the ebs-csi-controller pod logs should look like :   Looks good, now, for a test, lets edit a PV and increase its size. In my example, I\u0026rsquo;ll just increase the alert-manager PV to 3GB, Initial size was 2GB.    If you are thinking how did I beautify Kubernetes editing ? all thanks to Lens IDE.\n  Lets verify the PV size now :\n    Here comes the real test to see if the actual EBS volume is resized or not. For that let\u0026rsquo;s copy the volume id and search that volume size in AWS console or via AWS cli to verify the disk size.\n  To get the volume id of a PV, run the below command :\nkubectl describe pv PV_NAME | grep Volume   Now if you prefer AWS CLI, you can use the following command, else can be verified in AWS console :\n  ","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-dynamic-pv-in-kubernetes-feat-eks-ebs/","summary":"Definition Let\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\nIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\nProblem statement Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like : kubectl edit pv your_PV and just change the size, it used to work since you have storage class of kubernetes.","title":"Dynamic PV in Kubernetes feat. EKS (EBS)"},{"content":"Definition what\u0026rsquo;s kubewatch ?\n kubewatch is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\n Lets see how we can deploy it to our cluster.\nPre-requisites :\n Kubernetes 1.12+ cluster Helm v3 A slack app and a channel to integrate kubewatch   Steps  Add Bitnami repo to your helm :  helm repo add bitnami https://charts.bitnami.com/bitnami Verify that kubewatch chart is available in the repo :  demo\u0026gt; helm search repo kubewatch NAME CHART VERSION APP VERSION DESCRIPTION bitnami/kubewatch 3.2.16 0.1.0 Kubewatch  Customize the values like slack integration and enabling RBAC. If you directly do helm install chart-name you wont get any event notification as RBAC is set to false by default kin the helm chart.\n  Run the below command to get the values.yaml to local :\n  helm show values bitnami/kubewatch \u0026gt; updated-values.yaml Now edit the yaml file as per your requirement. Here\u0026rsquo;s what I\u0026rsquo;ve changed :\nSlack Integration :\nslack: enabled: true channel: \u0026#34;kubewatch\u0026#34; #your slack channel name ## Create using: https://my.slack.com/services/new/bot and invite the bot to your channel using: /join @botname ## token: \u0026#34;your slack bot token here\u0026#34; Enable RBAC:\n## @section RBAC parameters ## @param rbac.create Whether to create use RBAC resources or not ## rbac: create: true Now lets deploy using the below command :  helm install kubewatch bitnami/kubewatch -f ./updated-values.yaml Verify that kubewatch pod is running :  demo\u0026gt; kubectl get pod NAME READY STATUS RESTARTS AGE kubewatch-c86656645-8znwk 1/1 Running 0 2m19s To test it out, lets create a nginx deployment with command :  kubectl create deploy nginx --image=nginx Check your slack channel for notifications :  The indication is as follows :\n Green : for resources created Yellow : for resources updated Red : for resources deleted   You can customize the notification a lot, for example, which namespace to monitor to ( default value is all namespace) , which resource to monitor to like deployment, pod, PV, service etc.\nYou can just edit the configmap : kubewatch-config and change the resources to monitor.\nHappy monitoring !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-15-monitoring-k8s-resource-changes-in-cluster-with-kubewatch/","summary":"Definition what\u0026rsquo;s kubewatch ?\n kubewatch is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\n Lets see how we can deploy it to our cluster.\nPre-requisites :\n Kubernetes 1.12+ cluster Helm v3 A slack app and a channel to integrate kubewatch   Steps  Add Bitnami repo to your helm :  helm repo add bitnami https://charts.","title":"Monitoring K8S resource changes with kubewatch"},{"content":"Previously I have written https://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article about how AWS pushed broken image to Docker hub and we got screwed as we were using latest as image tag.\nWelp, this happened again in our CI/CD pipeline as we were using https://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push plugin from helm and using that to push charts to https://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum .\nSo we were using the below line to pull the helm push plugin :\nhelm plugin install https://github.com/chartmuseum/helm-push.git And were pushing to Chartmuseum via command :\nhelm push app-name repo-name\nIt turns out that command is not valid and as per their latest (v0.10.0) changes to the plugin, its been renamed to cm-push and we gotta use like helm cm-push app-name repo-name. Else we can use the same command with old version of plugin.\nHence our pipeline got screwed and I\u0026rsquo;ve fixed by pulling specific version from their repo by using -version argument. It goes like this :\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0\nThe better solution to this is to replace the hard-coded version above to GitLab CI variable and update the version from there later.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-13-it-happened-again-in-production/","summary":"Previously I have written https://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article about how AWS pushed broken image to Docker hub and we got screwed as we were using latest as image tag.\nWelp, this happened again in our CI/CD pipeline as we were using https://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push plugin from helm and using that to push charts to https://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum .\nSo we were using the below line to pull the helm push plugin :\nhelm plugin install https://github.com/chartmuseum/helm-push.git And were pushing to Chartmuseum via command :","title":"It happened again in production !!"},{"content":"Question After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\nThere\u0026rsquo;s a reason at least why I had this requirement, Cost optimization in AWS.\nBackground When you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region. Now for staging / testing clusters the Inter Availability Zone data transfer fees we were getting was a hefty one, which was unnecessary as HA is not needed for the testing environment.\nI couldn\u0026rsquo;t find this anywhere else, so with an outage at staging cluster :D ( shhhhh!) I found out that the solution is to create a new node group with AZ hard-coded while creating it and any node you spawn in that node group using ASG (Auto Scaling Group) will be in that single AZ only keeping your inter AZ data transfer cost to 0.\nSolution eksctl create nodegroup --cluster=staging_cluster \\  --region=ap-south-1 \\  --node-zones=ap-south-1a \\  --name=M5.2xlarge_NG \\  --node-type=m5.2xlarge In the above snippet, I\u0026rsquo;m creating NG in Mumbai Region in AZ ap-south-1a with m5.2xlarge instance.\nIf you want to go with GUI way, then :\n Go to your cluster in EKS and then click on Add Node Group :    Go with usual flow of giving it a name, taint if required, and IAM role.\n  Select AMI, disk size, instance family etc.\n  In Networking section, by default 3 subnets will be selected, untick 2 of them and keep 1 ( any desired AZ \u0026lsquo;a/b/c\u0026rsquo;). If you\u0026rsquo;re unsure about the name and AZ, you can verify that by going to VPC -\u0026gt; subnets.\n  That\u0026rsquo;s it, create the node group and all the instances will be spawned in that AZ only.\n   You can also do this in a hackish way by editing the ASG corresponding to the node group and removing 2 subnets from there.\n  It works but your node group will become Unhealthy and AWS wont do anything to the node groups which is having health issue. So better to create a new node group.\n  eksctl There\u0026rsquo;s one more way which I found out i.e to use ekctl command line and create from config file.\n  You can read more about eksctl and configure it by referring here .\n  Let\u0026rsquo;s create a config file to create the node group : ap-south-la-NG.yaml :\n  apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: Your_Cluster_Name region: ap-south-1 managedNodeGroups: - name: demo-nodegroup labels: { role: worker-nodes } instanceType: m5.xlarge desiredCapacity: 1 volumeSize: 50 availabilityZones: \u0026amp;#091; ap-south-1a ] minSize: 1 maxSize: 2 volumeType: gp3 privateNetworking: true Then you can apply using the below command :\neksctl create nodegroup --config-file ap-south-la-NG.yaml Finally, once the new Node groups is created, you can scale down your existing node group to 0 so that AWS will drain the nodes gracefully and all your workloads will be moved to newly created node groups.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-11-how-to-migrate-all-your-worker-nodes-from-multiple-az-to-single-az-in-aws-eks/","summary":"Question After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\nThere\u0026rsquo;s a reason at least why I had this requirement, Cost optimization in AWS.\nBackground When you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region.","title":"How to migrate a Node-Group from Multi AZ to single AZ in AWS EKS"},{"content":"History If you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful amusing for you.\nOn Thursday, I started receiving alerts that our CI pipeline is failing.\nI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\nInstalling docker Installation failed. Check that you have permissions to install. Cleaning up file based variables ERROR: Job failed: command terminated with exit code 1 After scratching the head for sometime, I found that the latest aws-cli image from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.\nSo I went to Docker hub and I saw on that day there was a new version pushed which was 2.2.39 tagged as latest. Since in our CI file, we didn\u0026rsquo;t mention specific image version to pull so it always assumes the tag to pull is latest.\nAs a temporary fix, I changed the image version to older one which was 2.2.38 and it worked fine.\n If you ask me for a better a better solution, it would be always good to use a specific version in production since you know it will work for sure instead of using latest tag which could change every single day.\nElse push that image to your private container repositories like ECR and pull from there.\nI\u0026rsquo;m pretty sure AWS broke few thousand CI pipelines over the world whoever used latest as the image tag :D\nTo give an idea about how to install docker inside aws-cli image, you can just run the below command which should install docker from AWS hosted repo for a faster install :\namazon-linux-extras install docker DevOps story ends here. I\u0026rsquo;ll update more stories like this in future :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-19-story-of-keeping-ci-pipeline-from-getting-screwed-when-aws-pushes-broken-docker-image-to-docker-hub/","summary":"History If you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful amusing for you.\nOn Thursday, I started receiving alerts that our CI pipeline is failing.\nI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\nInstalling docker Installation failed. Check that you have permissions to install. Cleaning up file based variables ERROR: Job failed: command terminated with exit code 1 After scratching the head for sometime, I found that the latest aws-cli image from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.","title":"Story of keeping CI pipeline from getting screwed when AWS pushes broken docker image to Docker hub"},{"content":"Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\n I wanted to add a user to my EKS, hence while adding the user to aws-auth configmap of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !!!\n Straight forward solution which I found out :\nFind out who created the EKS cluster ( owner) and ask them to edit the aws-auth configmap to correct your mistakes.\nThe user who created the cluster is the root user for entity. Hence regardless of aws-auth configmap mess, he/she can login via kubectl anytime.\nRead more here on solution by AWS.\n I wrote this because I made this mistake in my company and spent hours searching for answer before finding this info.\nOnce I found out the creator, she corrected it in 1 min. :D\n Long term solution :\nYou might be saying ' Thats one solution to save my job, how do I make sure I dont do this mistake again ?'\nAlright, so here\u0026rsquo;s what you can follow from next time :\n  First get the configmap yaml file by typing :\n  kubectl get configmap aws-auth -n kube-system -o yaml \u0026gt; aws-auth-configmap.yml   Once you get the yaml file, edit the file using your favorite text editor and update your changes.\n  Now, update the configmap with your new updated file by typing :\n  kubectl apply -n kube-system -f aws-auth-configmap.yml Remember, live editing is never a good option !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-01-how-to-access-aws-eks-cluster-when-you-mess-up-the-aws-auth-configmap/","summary":"Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\n I wanted to add a user to my EKS, hence while adding the user to aws-auth configmap of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !","title":"How to access AWS EKS cluster when you mess up the aws-auth configmap"},{"content":"Hey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of writing this since I spent hours searching and fixing this :/\nLets look at the problem or the requirement. It goes like this :\n I have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository. how do I pull from that repo ?\n Gitlab CI Lets consider the below gitlab-ci.yml file :\nimage: \u0026#34;python:3.6\u0026#34; stages: - publish_image  build and push docker image: stage: publish_image only: variables: - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+-\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/  - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/  variables: DOCKER_HOST: tcp://docker:2375 image: name: amazon/aws-cli entrypoint: \u0026#34;\u0026#34; services: - docker:dind  before_script: - echo \u0026#34;$CI_COMMIT_TAG\u0026#34; - amazon-linux-extras install docker - docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; -p \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; $CI_REGISTRY script: - docker build -t $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG . - aws ecr get-login-password | docker login --username AWS --password-stdin $DOCKER_REGISTRY  - docker push $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_TAG - docker push $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG Here\u0026rsquo;s how the above CI file works :\n Uses base image python on which the stages will run. has a single stage which will build and push images to ECR only section tells gitlab to run the stage only if the git tag is done and it matched the regex mentioned. in before_script section, we\u0026rsquo;re displaying the commit tag and installing docker in aws-cli image since that image doesn\u0026rsquo;t come preinstalled with docker. finally we\u0026rsquo;re doing docker login to with our dockerhub account before building Dockerfile. Later we build the Dockerfile and then push it to ECR  Configure login to Docker hub in GitLab CI  To configure the Dockerhub credentials, go to your GitLab project -\u0026gt; settings -\u0026gt; CI/CD In Variables section, add the below Key and their value :  Key : CI_REGISTRY || Value : docker.io Key : CI_REGISTRY_USER || Value : your_dockerhub_username Key : CI_REGISTRY_PASSWORD || Value : your_dockerhub_password Now, to setup AWS credentials, configure the below values :\nKey : AWS_ACCESS_KEY_ID || Value : your_aws_accesskey Key : AWS_SECRET_ACCESS_KEY || Value : your_aws_secretkey That\u0026rsquo;s it, voila !! Now GitLab runner should get your docker credentials from variables and pull the image seamlessly.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-08-23-pulling-private-image-from-docker-hub-in-gitlab-ci/","summary":"Hey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of writing this since I spent hours searching and fixing this :/\nLets look at the problem or the requirement. It goes like this :\n I have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository.","title":"Pulling private image from Docker hub in GitLab CI"},{"content":"Hey people, in this article, we\u0026rsquo;ll see how to configure TP -Link TL-WR740N (preferably old one) as repeater to extend your main WI-FI signal in your house.\nLets get into basics real quick.\nWhat\u0026rsquo;s a repeater ?\nDefinition : A WiFi repeater or extender is used to extend the coverage area of your Wi-Fi network. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.\nSteps on secondary router :\n Do a factory reset of your secondary router. You can refer https://youtu.be/4AkkPRE9ZBM\u0026quot;\u0026gt;this video for how-to steps. Once the router is up and running, connect to it wirelessly / through LAN cable. Go to admin console by typing this IP address in browser URL : 192.168.0.1 with credentials , username : admin  password : admin  ( super secure :D ) Lets first change the IP address of this router to something else rather than the default one as later this IP can cause IP allocation conflict due to DHCP set in primary router. To to that , lets go to Network -\u0026gt; LAN -\u0026gt; IP address and change it to something like 192.168.1.100 .and hit Save. ( you can change it to almost any IP you like in this subnet)   Do a reboot of the router and connect back to router console using the new IP in browser URL i.e. in my case 192.168.1.100 or the IP given by you.\n  Lets configure the repeater mode. To do that, go to Wireless -\u0026gt; Enable WDS Bridging.\n  Click on Survey and select the WIFI name which you want to repeat.\n  Type the password for that in Password field and hit Save. Later you may get alert on switching the repeater to be in same Wi-Fi channel as main router, select ok to that pop-up.\n  Next thing would be to setup DHCP of the router.  I\u0026rsquo;ll explain a bit here regarding the problem I faced. According to YouTube tutorials and articles out there, we need to disable the DHCP option in secondary router.\nWhat I faced after that is I cant connect any device to that router later as DHCP is disabled, the router wont be able to assign any IP address to any device asking for connection. So your device will be struck in \u0026ldquo;Obtaining IP address\u0026rdquo;.\nSo I found out the below trick and its working brilliantly for me.\nOk, lets through the settings one by one,  DHCP Server : Keep it Enable\nStart IP Address: Enter : 192.168.1.101 OR the +1 IP of the assigned IP to your router. i.e\nIf you gave 192.168.1.10 to your router, mention here 192.168.1.11\nEnd IP Address: : Enter 192.168.1.199 or the IP range limit you need. I mentioned here 98 (199 - 101) Address limit assuming my number of devices wont exceed 98 devices :D\n[ Follow the start IP address logic if you mentioned any alternate IP address to router. ]\nAddress Lease Time: Keep the default value.\nDefault Gateway: Here, enter the IP address of your primary router. You can mostly find out by seeing the backside of your primary router.\nElse, you can run the below command via cmd to get the value ( after connecting to primary router) :\nipconfig /all | findstr Gateway\u0026lt;br /\u0026gt;Default Gateway . . . . . . . . . : 192.168.1.1\nDefault Domain: Keep the default value i.e. empty.\nPrimary DNS: You can mention the DNS resolver address. This is optional and same for below one also. if none is mentioned, DNS resolver given by ISP is used. which is not a good solution from privacy perspective. You can use Google Public DNS ( 8.8.8.8) , Quad DNS,(9.9.9.9) Cloud flare DNS (1.1.1.1) here.\nSecondary DNS: This value corresponds to what resolver to use if the request is not resolved by the first DNS. Its good to mention different service to ensure high reliability.\nThat\u0026rsquo;s it. hit Save and do a reboot of the server to get new changes.\n Note : there\u0026rsquo;s a high change you wont be able to connect to your repeater later if you\u0026rsquo;re in a Wi-Fi crowded place i.e. you are surrounded by lot of WIFI. When there are lot of Wifi nearby the router tries to get to channel which is less crowded.\nBut in repeater mode, both repeater and main router needs to be in same Wi-Fi channel. So I would highly advice you to go to your primary router set the Wi-Fi to a particular channel and keep the same channel in repeater also. rather than the default setting : Auto.\n ","permalink":"https://tanmay-bhat.github.io/posts/2021-08-11-configuring-tp-link-tl-wr740n-as-wi-fi-repeater/","summary":"Hey people, in this article, we\u0026rsquo;ll see how to configure TP -Link TL-WR740N (preferably old one) as repeater to extend your main WI-FI signal in your house.\nLets get into basics real quick.\nWhat\u0026rsquo;s a repeater ?\nDefinition : A WiFi repeater or extender is used to extend the coverage area of your Wi-Fi network. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.","title":"Configuring TP -Link TL-WR740N as WI-FI repeater"},{"content":"Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\nSteps  Open the router login page, which is mostly : 192.168.1.1 in your case. After logging in, navigate to Network page, LAN IP Address tab. Change the Lan Dns Mode to : static Set the primary and secondary DNS address and click on Save/Apply. Perform a reboot of router to apply the changes.  There are a lot of DNS providers out there most of them for free. However, please be wise while choosing them.\nI have chosen 1.1.1.1 DNS as my primary server which is provided by Cloudflare.\nI have set the secondary server to 8.8.8.8 which is provided by Google so that if one of the service is down, it will fallback to another.\nList of some of the best DNS providers list in r/sysadmin \n Psst\u0026hellip;\u0026hellip; Feeling Geeky ?\nPerform DNS benchmark tests : https://www.grc.com/dns/benchmark.htm\nCloudflare DNS validation test : https://1.1.1.1/help\nNeed more? Read the detailed guide for BSNL FTTH : (Fiber optimization\nWorth reading about 3rd party DNS resolvers\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-27-how-to-change-dns-server-for-syrotech-router-bsnl-ftth/","summary":"Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\nSteps  Open the router login page, which is mostly : 192.168.1.1 in your case. After logging in, navigate to Network page, LAN IP Address tab. Change the Lan Dns Mode to : static Set the primary and secondary DNS address and click on Save/Apply.","title":"How to change DNS server for Syrotech Router [BSNL FTTH]"},{"content":"Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to Microsoft Events.\nThen you can register for the training of an exam of your choice by searching it in the page.\nSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\n  Azure Data Fundamentals\n  Azure Fundamentals\n  Azure AI fundamentals.\n  Once you register the training and attend it, you can go to that certification website, schedule it through Pearson VUE. If the email used to register for the event is the same as the MSA (Microsoft Account) email used for certification, You should see a banner to claim 100% free voucher, click *claim *to get that, like below.\nThen, you can schedule the exam at your day of choice from home and get certified.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-26-how-to-claim-free-azure-certification-vouchers-after-attending-microsoft-events/","summary":"Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to Microsoft Events.\nThen you can register for the training of an exam of your choice by searching it in the page.\nSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\n  Azure Data Fundamentals","title":"How to claim free Azure certification  vouchers after attending Microsoft Events"},{"content":"Even though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\nActually we should be using **ss **tool installed of netstat !\nAll common network related tools are bundled with package net-tools.\nnwlab:/etc # rpm -qa | grep net-tools net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64 However in openSUSE 15, the team decided to knock it off from net tools package!\nSo, the solution ?sudo zypper install net-tools-deprecated\nnwlab:/etc # rpm -qa | grep net-tools net-tools-deprecated-2.0+git20170221.479bb4a-lp152.5.5.x86_64 Once installed, netstat should work totally fine now !\nnwlab:/etc # netstat -ano | grep 9000 tcp6 0 0 :::9000 :::* LISTEN off (0.00/0/0) ","permalink":"https://tanmay-bhat.github.io/posts/2021-01-25-how-to-install-netstat-tool-in-opensuse-15/","summary":"Even though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\nActually we should be using **ss **tool installed of netstat !\nAll common network related tools are bundled with package net-tools.\nnwlab:/etc # rpm -qa | grep net-tools net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64 However in openSUSE 15, the team decided to knock it off from net tools package!","title":"How to install netstat tool in openSUSE 15"},{"content":"Hey all ! For those of you who don\u0026rsquo;t know what PWD is below is short explanation :\nSo, PWD stands for Play With Docker. You can deploy learn docker at free with time limit of each instance up-to 10 Hrs!\nFor more info, go to : Docker Labs\n There are two ways you can access the docker instance.\n Use the web based console. SSH into that instance.  I always love to do ssh as it gives me more freedom.\n If you go straight away and do ssh from your terminal, you will get :\nlab-suse:~/.ssh # ssh ip-x-x-x-@direct.labs.play-with-docker.com ip-x-x-x-@direct.labs.play-with-docker.com: Permission denied (publickey). Why are we getting this ? because there is no fresh key generated in your host.\nLets create a fresh key, run the below command and use default values :\nlab-suse:~/# ssh-keygen After you complete the above command, try ssh again, it should work:\nlab-suse:~/.ssh# ssh ip-x-x-x-@direct.labs.play-with-docker.com The authenticity of host \u0026#39;direct.labs.play-with-docker.com (40.76.55.146)\u0026#39; can\u0026#39;t be established. RSA key fingerprint is SHA256:UyqFRi42lglohSOPKn6Hh9M83Y5Ic9IQn1PTHYqOjEA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;direct.labs.play-with-docker.com,40.76.55.146\u0026#39; (RSA) to the list of known hosts. Connecting to Ip-x-x-x:8022 ########################### # WARNING!!!! # This is a sandbox environment. Using personal credentials # is HIGHLY! discouraged. Any consequences of doing so are completely # the user\u0026#39;s responsibilites. # The PWD team node1 root@192.168.0.28 Note : if you are using any ssh applications, save that key you generated to a file and load that file in authentication section.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-01-22-how-to-ssh-into-docker-in-pwd-play-with-docker/","summary":"Hey all ! For those of you who don\u0026rsquo;t know what PWD is below is short explanation :\nSo, PWD stands for Play With Docker. You can deploy learn docker at free with time limit of each instance up-to 10 Hrs!\nFor more info, go to : Docker Labs\n There are two ways you can access the docker instance.\n Use the web based console. SSH into that instance.  I always love to do ssh as it gives me more freedom.","title":"How to SSH into docker in PWD (Play With Docker)"},{"content":"Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\nThere are 2 ways you can install mhvtl :\n Install the rpm package. Directly compile the source code yourself.  I have tried using the first method as its easy and fast :D.\nNote: I have tested this install in openSUSE 15.2\nSteps 1.First update the packages to latest version available by typing :\nsudo zypper up\nOnce the packages are up to date, install the below supporting packages :\nsudo zypper install gcc gcc-c++ kernel-devel zlib-devel mt-st mtx lzo-devel perl Now add the repository and install the package :  zypper addrepo https://download.opensuse.org/repositories/openSUSE:Leap:15.2:Update/standard/openSUSE:Leap:15.2:Update.repo zypper refresh zypper install mhVTL start the mhvtl service ;\nservice mhvtl start  check if mhvtl service is running :\nservice mhvtl status  test-machine:/home/azureuser # service mhvtl status ● mhvtl.target - mhvtl service allowing to start/stop all vtltape@.service and vtllibrary@.service instances at once Loaded: loaded (/usr/lib/systemd/system/mhvtl.target; disabled; vendor preset: disabled) Active: active since Thu 2021-01-14 17:13:36 UTC; 50min ago verify if you are able to see the tape library and the drives configured( by default) :  test-machine:/home/azureuser # lsscsi -g [1:0:0:0] cd/dvd Msft Virtual CD/ROM 1.0 /dev/sr0 /dev/sg3 [2:0:0:0] disk Msft Virtual Disk 1.0 /dev/sda /dev/sg0 [3:0:1:0] disk Msft Virtual Disk 1.0 /dev/sdb /dev/sg1 [5:0:0:0] disk Msft Virtual Disk 1.0 /dev/sdc /dev/sg2 [6:0:0:0] mediumx STK L700 0162 /dev/sch0 /dev/sg12 [6:0:1:0] tape IBM ULT3580-TD5 0162 /dev/st0 /dev/sg4 [6:0:2:0] tape IBM ULT3580-TD5 0162 /dev/st7 /dev/sg11 [6:0:3:0] tape IBM ULT3580-TD4 0162 /dev/st3 /dev/sg7 [6:0:4:0] tape IBM ULT3580-TD4 0162 /dev/st4 /dev/sg8 [6:0:8:0] mediumx STK L80 0162 /dev/sch1 /dev/sg13 [6:0:9:0] tape STK T10000B 0162 /dev/st2 /dev/sg6 [6:0:10:0] tape STK T10000B 0162 /dev/st5 /dev/sg9 [6:0:11:0] tape STK T10000B 0162 /dev/st1 /dev/sg5 [6:0:12:0] tape STK T10000B 0162 /dev/st6 /dev/sg10 For more details about mhvtl, please refer the below link :\nhttps://sites.google.com/site/linuxvtl2/\n ","permalink":"https://tanmay-bhat.github.io/posts/2021-01-14-how-to-install-mhvtl-in-opensuse/","summary":"Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\nThere are 2 ways you can install mhvtl :\n Install the rpm package. Directly compile the source code yourself.  I have tried using the first method as its easy and fast :D.\nNote: I have tested this install in openSUSE 15.2\nSteps 1.First update the packages to latest version available by typing :","title":"How to install mhVTL in openSUSE"}]