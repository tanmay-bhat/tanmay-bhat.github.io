[{"content":" I’ve recently got a gaming laptop and monitoring the CPU \u0026amp; GPU temperature of it has been a tedious task, like install MSI Afterburner, configure statistics server, configure overlay etc.\nThat made me use Grafana’s product suite to configure monitoring of key components such as CPU, GPU, Network, Disk and alerting for my laptop such that I can game in peace and when my laptop’s temperature reaches a certain threshold limit, I’ll get a phone call.\nInstall and Configure OhmGraphite OhmGraphite is a Windows service that exposes hardware sensor data to a metric store, allowing one to observe health and status of one’s system over below components:\nPower consumption of the CPU and GPU CPU voltages and frequencies Load breakdown on individual GPU components CPU, GPU, disk, and motherboard temperature readings Disk activity, space overview Network consumption Lets install and configure OhmGraphite in our laptop :\nCreate a directory that will be the base directory for OhmGraphite (like C:\\Program Files\\OhmGraphite). Download the latest zip and extract to C:\\Program Files\\OhmGraphite . Next, we need to update the app configuration so that the app can scrape the laptop metrics and expose it at localhost:4445/metrics. In order to do that, edit the file OhmGraphite.exe.config and update it with below values : \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appSettings\u0026gt; \u0026lt;add key=\u0026#34;type\u0026#34; value=\u0026#34;prometheus\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;prometheus_port\u0026#34; value=\u0026#34;4445\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;prometheus_host\u0026#34; value=\u0026#34;localhost\u0026#34; /\u0026gt; \u0026lt;/appSettings\u0026gt; \u0026lt;/configuration\u0026gt; We need the above changes since the app can expose metrics in metric stores for example, Graphite, Prometheus \u0026amp; even push to TimescaleDB \u0026amp; InfluxdDB. Install the app by running the command .\\OhmGraphite.exe install. This command will install OhmGraphite as a Windows service. Now, we can start the OhmGraphite service by running the below command : #PowerShell start-Service OhmGraphite #CommandLine sc start OhmGraphite You can curl the endpoint http://localhost:4445/metrics and the scraped metrics are listed there. #Sample response curl http://localhost:4445/metrics # HELP ohm_gpunvidia_bytes Metric reported by open hardware sensor # TYPE ohm_gpunvidia_bytes gauge ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Used\u0026#34;,} 1072693248 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Free\u0026#34;\u0026#34;} 3221225472 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;GPU Memory Total\u0026#34;} 4294967296 ohm_gpunvidia_bytes{hardware=\u0026#34;NVIDIA GeForce RTX 3050 Laptop GPU\u0026#34;,sensor=\u0026#34;D3D Shared Memory Used\u0026#34;} 69447680 Install and Configure Grafana Agent Download and install Grafana Agent for Windows from this link (latest release). Once it’s installed, edit the file C:\\Program Files\\Grafana Agent\\agent-config.yaml and update the below values : metrics: global: scrape_interval: 60s wal_directory: /tmp/grafana-agent-wal configs: - name: laptop_exporter remote_write: - basic_auth: password: \u0026lt;your-grafana-cloud-api-key\u0026gt; username: \u0026lt;your-grafana-cloud-username-id\u0026gt; url: \u0026lt;your-grafana-cloud-prometheus-push-endpoint\u0026gt; scrape_configs: - job_name: laptop_exporter static_configs: - targets: [\u0026#34;localhost:4445\u0026#34;] I’ve kept the scrape_interval to 1 minute. You can update to lower value(like 30s) if you need the values to be updated quicker. Job name can be anything you want. Better to keep something identifiable and meaningful. If you already have a Grafana Cloud account, then you can get the API Key, User-ID, \u0026amp; the Prometheus metrics push endpoint at your Prometheus stack page : Restart the Grafana Agent to update your new config : #PowerShell Restart-Service \u0026#39;Grafana Agent\u0026#39; #CommandLine sc stop \u0026#39;Grafana Agent\u0026#39; sc start \u0026#39;Grafana Agent\u0026#39; Verify that Agent is able to recognize the target and scraping the metrics : curl -s http://localhost:12345/agent/api/v1/metrics/targets | jq { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: [ { ... \u0026#34;endpoint\u0026#34;: \u0026#34;http://localhost:4445/metrics\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;instance\u0026#34;: \u0026#34;localhost:4445\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;laptop_exporter\u0026#34; ... ] } Grafana Agent also has Health \u0026amp; ready status API. This might be handy when you’re debugging Agent’s health and scrape issues. You can check them by running :\ncurl -s http://localhost:12345/-/ready #sample output Agent is Ready. curl -s http://localhost:12345/-/healthy #sample output Agent is Healthy. Scrape metrics using self hosted Prometheus (optional) If you want to use your own Prometheus instead of Grafana Agent, then you can add a scrape job to your existing Prometheus config prometheus.yml like below :\nglobal: scrape_interval: 30s scrape_configs: - job_name: \u0026#39;laptop_exporter\u0026#39; static_configs: - targets: [\u0026#34;localhost:4445\u0026#34;] Import Grafana Dashboard Once everything is setup, you can Import the Dashboard-ID : 11587 in your Grafana to visualize the metrics collected.\nI’ve updated few panels from the imported dashboard. For example for CPU temperature, my PromQL query is :\navg_over_time(ohm_cpu_celsius{instance=\u0026#34;$instance\u0026#34;}[5m]) Here’s how my Grafana Dashboard looks after the edit :\nCPU :\nGPU :\nAlerting I don’t want to have a overlay of CPU and GPU temperatures on my screen while I’m gaming. Hence I thought, Instead of having an overlay, why not create alerts for high temperatures and integrate those alerts to Grafana Oncall such that when my laptop is hot, I’ll get a call because who checks messages while gaming :D.\nAlert for High CPU temperature\nI’ve configured the alert threshold to 80 degree. So, if my laptop CPU temperature stays above 80 degree for 5 minutes straight, I’ll get a phone call. Here’s how my alert config looks like.\nOnce I receive the call, I can choose to lower the game resolution and continue to play which should give a bit of breathing room for my laptop or close it so that laptop can cool itself.\nAlert for High GPU temperature\nSame config goes for GPU temperature alert as well. I’m averaging out the temperature value since I don’t want to be paged for a temperature spike. I’d like the alert to be fired only when the temperature is consistently higher than the threshold.\nAlerting via AlertManager There’s no specific requirement to use Grafana’s new alerting. If you’d like to use your own AlertManager, you can easily write an alert config by referring to an example from Awesome Prometheus alerts.\nFor notification configuration, use the \u0026lt;webhook_config\u0026gt; section and use the webhook from Grafana Oncall’s integration. Here’s the doc for webhook config options for AlertManager.\nGrafana Oncall Configuring Grafana Oncall is fairly simple for Phone call alerts.\nCreate or verify the user which phone number on the Users section. Set the Default notification Method to : Phone Create a default escalation policy like below : Integrate the escalation policy you created to the Grafana Integration : Click on the “How to connect” as shown in the above screenshot which will give you the Grafana Oncall Webhook. Copy that. Go to Alerting → Contact Points → New → Select type as : Webhook and past the URL you copied from above step. That’s it, whenever the alert’s threshold reaches its value, you should get a phone call.\nReferences https://github.com/nickbabcock/OhmGraphite\nhttps://prometheus.io/docs/alerting/latest/configuration\nhttps://awesome-prometheus-alerts.grep.to/\nhttps://grafana.com/docs/oncall/latest/\n","permalink":"https://tanmay-bhat.github.io/posts/monitor-gaming-laptop-using-grafana-ohmgraphite/","summary":"I’ve recently got a gaming laptop and monitoring the CPU \u0026amp; GPU temperature of it has been a tedious task, like install MSI Afterburner, configure statistics server, configure overlay etc.\nThat made me use Grafana’s product suite to configure monitoring of key components such as CPU, GPU, Network, Disk and alerting for my laptop such that I can game in peace and when my laptop’s temperature reaches a certain threshold limit, I’ll get a phone call.","title":"Monitor Gaming Laptop Using Grafana and OhmGraphite"},{"content":" In this article, we’ll see how to setup Grafana with a remote database so that we can scale Grafana to N instances.\nThe default SQLite database will not work with scaling beyond 1 instance since the SQLite3 DB is embedded inside Grafana container.\nCreate Remote PostgreSQL using fly.io (Optional) For this demonstration, I’ll be using fly.io’s PostgreSQL service. Its free (no need to add credit card) and it has 1 GB of storage for DB and should be enough to try out with Grafana. Note : If you already have a remote DB such as AWS RDS (or local DB) running, you can skip this step.\nFirst, download and install the flyctl by referring this doc. Create the account and sign in by referring to this doc. Create the DB cluster using the command : flyctl postgres create Set the name for the cluster : grafana Choose the region near to you, for example : India Select Development - Single node, 1x shared CPU, 256MB RAM, 1GB disk After a minute, the cluster should get created and the username, password and URL will be visible like below, save it somewhere. Postgres cluster grafana created Username: postgres Password: super-secret-pasword Hostname: grafana.internal Proxy Port: 5432 Postgres Port: 5433 Now, the cluster is only accessible inside fly.io’s network since it doesn\u0026rsquo;t have a public address to reach over the internet. Use the below command to port-forward the DB connection to localhost at 5432: flyctl proxy 5432 -a grafana #expected output Proxying local port 5432 to remote [grafana.internal]:5432 You can connect to the DB using below command : flyctl postgres connect -a grafana Create Grafana Database Since we’re using remote database, when the first time Grafana starts, it starts DB migration and that will fail if it can\u0026rsquo;t find a database called grafana.\nHence, let’s create the DB using the command below (via psql client) :\ncreate database grafana; Grafana with Docker-Compose Once the DB is up and running, let\u0026rsquo;s create a docker-compose.yaml file for our Grafana:\nversion: \u0026#39;3\u0026#39; services: grafana: image: grafana/grafana:9.0.7 container_name: grafana ports: - 3000:3000 environment: - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=postgres - GF_DATABASE_PASSWORD=super-secret-password - GF_DATABASE_TYPE=postgres - GF_DATABASE_HOST=host.docker.internal:5432 Please update the environment variables as per your DB details.\nStart Grafana container using the command :\ndocker-compose up -d Grafana should be running now and DB migration should start and logs will indicate that : logger=sqlstore t=2022-08-20T15:24:28.83593716Z level=info msg=\u0026#34;Connecting to DB\u0026#34; dbtype=postgres logger=migrator t=2022-08-20T15:24:29.231523787Z level=info msg=\u0026#34;Starting DB migrations\u0026#34; logger=migrator t=2022-08-20T15:24:29.352920106Z level=info msg=\u0026#34;Executing migration\u0026#34; id=\u0026#34;create migration_log table\u0026#34; logger=migrator t=2022-08-20T15:24:29.5825907Z level=info msg=\u0026#34;Executing migration\u0026#34; id=\u0026#34;create user table\u0026#34; Open Grafana by navigating to http://localhost:3000 Gotchas In the above example, I’ve mentioned DB host as host.docker.internal since my DB is accessible through localhost of the host machine. If it\u0026rsquo;s AWS RDS or similar managed DB, just mention the DB connection URL and enable SSL verification. Regarding DB user, for this example, I haven’t created a separate DB user, but if you’re running a similar setup in production, it is highly advised that you create one. Data Persistence in Docker Grafana v9 stores almost all data inside its database, including alert configurations. We can check that by listing the tables inside our Grafana database : Schema | Name | Type | Owner --------+----------------------------+-------+---------- public | alert | table | postgres public | alert_configuration | table | postgres public | alert_notification | table | postgres public | api_key | table | postgres public | dashboard | table | postgres public | dashboard_provisioning | table | postgres public | data_source | table | postgres public | org | table | postgres public | org_user | table | postgres public | permission | table | postgres public | preferences | table | postgres public | team | table | postgres public | team_member | table | postgres public | user | table | postgres public | user_auth | table | postgres public | user_auth_token | table | postgres public | user_role | table | postgres ... In case of docker, if the container restarts, none of your configurations or dashboards will be lost.\nTo test the persistence, let’s create a Prometheus data source :\nImport the node exporter dashboard, Dashboard ID : 1860 Now, you can test that by simply restarting the Grafana container : docker-compose restart grafana The logs should show something similar to below : logger=settings t=2022-08-20T15:47:49.880986785Z level=info msg=\u0026#34;App mode production\u0026#34; logger=sqlstore t=2022-08-20T15:47:49.881148929Z level=info msg=\u0026#34;Connecting to DB\u0026#34; dbtype=postgres logger=migrator t=2022-08-20T15:47:54.783450339Z level=info msg=\u0026#34;Starting DB migrations\u0026#34; logger=migrator t=2022-08-20T15:47:55.043328627Z level=info msg=\u0026#34;migrations completed\u0026#34; performed=0 skipped=426 duration=894.268µs High Availability Grafana Setup in Docker To try our HA for Grafana in Docker, let\u0026rsquo;s create 2 grafana replicas behind an nginx proxy : version: \u0026#39;3\u0026#39; services: grafana: image: grafana/grafana:9.0.7 expose: - \u0026#34;3000\u0026#34; environment: - GF_DATABASE_NAME=grafana - GF_DATABASE_USER=postgres - GF_DATABASE_PASSWORD=super-secret-password - GF_DATABASE_TYPE=postgres - GF_DATABASE_HOST=host.docker.internal:5432 deploy: replicas: 2 nginx: image: nginx:latest container_name: nginx volumes: - \u0026#39;./nginx.conf:/etc/nginx/nginx.conf\u0026#39; depends_on: - grafana ports: - \u0026#34;8000:8000\u0026#34; If you look closely, we’ve exposed 3000 port from Grafana inside docker network and added Nginx as a proxy which will serve at port 8000. Let’s create a file called nginx.conf so that it can forward the traffic to 3000 port of Grafana containers : user nginx; events { worker_connections 1000; } http { server { listen 8000; location / { proxy_pass http://grafana:3000; proxy_set_header Host $http_host; } } } Now, you can start this stack by running docker-compose up -d and you can access Grafana by going to http://localhost:8000 Data Persistence \u0026amp; HA in Kubernetes For this demo, I’ll be using minikube as my Kubernetes cluster.\nFirst, let’s install Grafana using helm.\nRun the below commands to add the Grafana helm repository to your cluster :\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update Once added, create a file called custom-values.yaml and add the values : grafana.ini: database: type: \u0026#34;postgres\u0026#34; host: \u0026#34;host.minikube.internal:5432\u0026#34; user: \u0026#34;postgres\u0026#34; password: \u0026#34;super-secret-pasword\u0026#34; name: \u0026#34;grafana\u0026#34; replicas: 2 Note : Since my database is accessible at localhost, I’m using host has host.minikube.internal. If you’re using AWS RDS or a similar service, you should put the DNS address in the host section.\nInstall Grafana helm chart using the below command : helm install grafana grafana/grafana -f custom-values.yaml Port forward Grafana service : kubectl port-forward svc/grafana 80:80\nYou can access the Grafana now by going to: http://localhost:80.\nAny number of pods you scale up for Grafana, they all now will connect to a shared Database.\nGotchas If you don’t have a shared database for Grafana and try to scale the replicas to \u0026gt; 1 it may result in unexpected results because by default each pod will have its own SQLite3 DB and they won\u0026rsquo;t be in sync.\nFor example, I have 2 Grafana replicas running without a shared database connection.\n\u0026gt; kubectl get pod NAME READY STATUS RESTARTS AGE grafana-975c48997-kw5vk 1/1 Running 0 65m grafana-975c48997-sq9wl 1/1 Running 0 65m After port-forwarding, I added a data source along with a dashboard for it.\nThe first pod is receiving traffic, and the second pod has no clue what’s happening on the other side.\n\u0026#34;Request Completed\u0026#34; method=POST path=/api/ds/query status=400 remote_addr=127.0.0.1 time_ms=3247 duration=3.247032415s size=110 referer=\u0026#34;http://localhost/d/rYdddlPWk/node-exporter-full?orgId=1\u0026amp;refresh=1m\u0026#34; traceID=00000000000000000000000000000000 logger=context traceID=00000000000000000000000000000000 userId=1 orgId=1 uname=admin t=2022-08-22T12:23:55.285644474Z level=info msg=\u0026#34;Request Completed\u0026#34; method=POST path=/api/ds/query status=400 remote_addr=127.0.0.1 time_ms=3247 duration=3.247181905s size=110 referer=\u0026#34;http://localhost/d/rYdddlPWk/node-exporter-full?orgId=1\u0026amp;refresh=1m\u0026#34; traceID=00000000000000000000000000000000 Whenever the request is sent to the second pod, connection gets logged out with auth error since in that pod, data doesn\u0026rsquo;t exist.\n#first pod level=info msg=\u0026#34;Successful Login\u0026#34; User=admin@localhost #second pod level=error msg=\u0026#34;Failed to look up user based on cookie\u0026#34; error=\u0026#34;user token not found\u0026#34; To keep it fair, I even observed the traffic flow to those pods using Linkerd. As you can see, the requests are almost equally balanced between pods :\nlinkerd viz stat pod grafana-55d88bb8b9-445dk grafana-55d88bb8b9-mrnf9 NAME STATUS MESHED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 TCP_CONN grafana-55d88bb8b9-445dk Running 1/1 100.00% 1.1rps 2ms 50ms 89ms 11 grafana-55d88bb8b9-mrnf9 Running 1/1 100.00% 1.6rps 2ms 18ms 77ms 9 Bonus You can run a load test on your Grafana setup using K6 and scale accordingly with the results.\nAdditional Note : You can run a local PostgreSQL and use that as your DB as well, no need to use AWS RDS or similar kind. The only added advantage with Managed Databases is that there’s no operational overhead.\nReferences https://grafana.com/docs/grafana/latest/setup-grafana/set-up-for-high-availability/\nhttps://fly.io/docs/reference/postgres/\nhttps://minikube.sigs.k8s.io/docs/handbook/host-access/\nhttps://github.com/grafana/grafana/blob/main/devenv/docker/ha_test\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-configure-grafana-to-use-remote-database-for-ha/","summary":"In this article, we’ll see how to setup Grafana with a remote database so that we can scale Grafana to N instances.\nThe default SQLite database will not work with scaling beyond 1 instance since the SQLite3 DB is embedded inside Grafana container.\nCreate Remote PostgreSQL using fly.io (Optional) For this demonstration, I’ll be using fly.io’s PostgreSQL service. Its free (no need to add credit card) and it has 1 GB of storage for DB and should be enough to try out with Grafana.","title":"How to Configure Grafana to Use Remote Database for HA"},{"content":"OAuth2 Proxy is a reverse proxy that provides authentication using Providers such as Google, GitHub, and others to validate accounts by email, domain or group.\nIn this article, we’ll setup and configure OAuth2 Proxy to enable Google SSO for Kibana in Kubernetes.\nPrerequisite: Kibana in Kubernetes Nginx Ingress Setup Google Credentials In Google Cloud Console, select OAuth consent screen Select the User Type as : “Internal” Complete the app registration form with your Authorized domain, then click Save. In the left Nav pane, choose Credentials In the center pane, choose \u0026ldquo;Credentials\u0026rdquo; tab. Open the \u0026ldquo;New credentials\u0026rdquo; drop down Choose \u0026ldquo;OAuth client ID\u0026rdquo; Choose \u0026ldquo;Web application\u0026rdquo; Application name can be anything, for simplicity, let’s say Kibana. Authorized JavaScript origins is your Kibana endpoint ex: https://kibana.example.com Authorized redirect URIs is the location of oauth2/callback ex: https://kibana.example.com.com/oauth2/callback Choose \u0026ldquo;Create\u0026rdquo; Download the Credentials JSON file. Setup Oauth2 Proxy Generate a random cookie secret with the below command and copy it to clipboard : docker run -ti --rm python:3-alpine python -c \u0026#39;import secrets,base64; print(base64.b64encode(base64.b64encode(secrets.token_bytes(16))));\u0026#39; Add the Helm repository : helm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests Install the helm chart with specified parameters as below : helm upgrade --install oauth2-proxy oauth2-proxy/oauth2-proxy \\ --set config.clientID=\u0026#34;GOOGLE_CLIENT_ID_HERE\u0026#34; \\ --set config.clientSecret=\u0026#34;GOOGLE_CLIENT_SECRET_HERE\u0026#34; \\ --set config.cookieSecret=\u0026#34;GENERATED_COOKIE_SECRET_HERE\u0026#34; \\ --set extraArgs.provider=\u0026#34;google\u0026#34; Use the Google Client_ID and SECRET from the downloaded JSON file. Create and apply the Ingress for Oauth2 Proxy using the below YAML manifest : cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: oauth2-proxy spec: ingressClassName: nginx rules: - host: https://kibana.example.com http: paths: - path: /oauth2 pathType: Prefix backend: service: name: oauth2-proxy port: number: 4180 EOF Setup Ingress annotation for Kibana This article assumes you already have a Kibana setup running in Kubernetes with an Ingress. Append the two nginx annotations mentioned below to existing Kibana Ingress. Once updated, the Ingress should look similar to : apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kibana annotations: nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth spec: ingressClassName: nginx rules: - host: kibana.example.com http: paths: - backend: service: name: kibana port: number: 5601 path: / pathType: ImplementationSpecific Now if you hit the endpoint kibana.example.com. It should ask you to login via Google. Since we have mentioned the OAuth type as Internal and also specified Authorised Domain, any google login except your specified Domain will be 401. References :\nhttps://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/\nhttps://oauth2-proxy.github.io/oauth2-proxy/\n","permalink":"https://tanmay-bhat.github.io/posts/google-sso-kibana/","summary":"OAuth2 Proxy is a reverse proxy that provides authentication using Providers such as Google, GitHub, and others to validate accounts by email, domain or group.\nIn this article, we’ll setup and configure OAuth2 Proxy to enable Google SSO for Kibana in Kubernetes.\nPrerequisite: Kibana in Kubernetes Nginx Ingress Setup Google Credentials In Google Cloud Console, select OAuth consent screen Select the User Type as : “Internal” Complete the app registration form with your Authorized domain, then click Save.","title":"How to enable Google SSO in Kibana using OAuth2 Proxy [Kubernetes]"},{"content":"In this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.\n1. Node exporter: Node exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started. Although 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like Grafana cloud, AWS Prometheus and Google Cloud for Prometheus, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent \u0026amp; stored. It’s not necessary that you should cut down on metric scraping if you’re sending metrics to any of the vendors mentioned above. This also implies to local storage of Prometheus data, since too many time series over time can hinder Prometheus performance. What’s a collector ? Main components of a node are referred to as collector, for example CPU, file-system, memory etc. Each collector exposes a set of metrics about the component it covers. Here’s the list of collectors that are enabled by default. Disable collectors : A collector can be disabled by providing the flag : --no-collector.\u0026lt;collector-name\u0026gt; #the command will look like this : node_exporter --no-collector.nfs Disable self metrics of node-exporter : Node-exporter exposes ~ 80 metrics about itself at /metrics along with node metrics. The metrics about node-exporter starts with prefix promhttp_*, process_*, go_*. Below is the list of some of them : process_cpu_seconds_total process_max_fds process_open_fds process_resident_memory_bytes process_start_time_seconds process_virtual_memory_bytes process_virtual_memory_max_bytes promhttp_metric_handler_errors_total promhttp_metric_handler_requests_in_flight promhttp_metric_handler_requests_total . . \u0026amp; around 68 go based metrics. You can disable all the above ~80 metrics by running node-exporter with flag -web.disable-exporter-metrics #the command will look like this : node_exporter --web.disable-exporter-metrics Enable only the collectors required : Opposite to disabling certain collectors, Node exporter has a flag --collector.disable-defaults which disables all collectors at once. Combining that flag with the collector of your choice will only collect the metrics of the collectors you enabled and discard everything else. For example, If you want to collect only the CPU and Memory metrics of a node, you can run the below command :\nnode_exporter --collector.disable-defaults --collector.cpu --collector.meminfo Filter collectors via scrape config: This is especially useful when you don’t have control over the nodes \u0026amp; have around 100’s of nodes where changing the config on each of them is not feasible. You can mention the names of the collectors which you want to enable (i.e., to be scraped) at scrape config of Prometheus, and all metrics from other collectors will be dropped. Below is an example Prometheus config: # scrape config to collect only cpu \u0026amp; memory metrics via node-exporter. scrape_configs: - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;localhost:9100\u0026#39;] params: collect[]: - cpu - meminfo 2. cAdvisor cAdvisor (Container Advisor) is a running daemon that collects, aggregates, processes, and exports information about running containers.\nDisable metrics: Similar to collectors, you can also disable metrics by passing flag : --disable_metrics Below is an example argument which disables a bunch of metric sources : --disable_metrics=accelerator,percpu,sched,resctrl,sched,process,hugetlb,referenced_memory,cpu_topology,memory_numa,tcp,advtcp,resctrl,udp\u0026#39; An example docker-compose file consisting Prometheus \u0026amp; cAdvisor :\nversion: \u0026#39;3.2\u0026#39; services: prometheus: image: prom/prometheus:latest container_name: prometheus ports: - 9090:9090 command: - --config.file=/etc/prometheus/prometheus.yml volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro depends_on: - cadvisor cadvisor: image: gcr.io/cadvisor/cadvisor:latest container_name: cadvisor ports: - 8080:8080 volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro command: - \u0026#39;--disable_metrics=accelerator,percpu,sched,resctrl,sched,process,hugetlb,referenced_memory,cpu_topology,memory_numa,tcp,advtcp,resctrl,udp\u0026#39; Prometheus config to scrape cAdvisor metrics, prometheus.yml :\nscrape_configs: - job_name: cadvisor static_configs: - targets: - cadvisor:8080 Drop costly metrics : Let’s find out what are the top 10 costly metrics by cAdvisor. We can get that result by running the below promQL expression :\n#syntax topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;,job=\u0026#34;cadvisor_job_name\u0026#34;})) #example topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;,job=\u0026#34;kubernetes-nodes-cadvisor\u0026#34;})) Which will look like this : Let’s say we want to drop container_memory_rss metrics. We can utilize Prometheus metric relabeling on this. Update the below to Prometheus scrape config : scrape_configs: - job_name: \u0026#34;cadvisor\u0026#34; scrape_interval: 15s static_configs: - targets: [\u0026#34;cadvisor:8080\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: \u0026#39;(container_memory_rss)\u0026#39; action: drop You can refer my article on Prometheus metrics drop \u0026amp; deletion to understand more about this.\n3. kube-state-metrics kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes, and pods.\nDisable Collectors: Just like the above-mentioned services, kube-state-metrics also has collectors which collect metrics about specific components like statefulset, daemonset, PVC etc. You can find the list of collectors here. All collectors are enabled by default, which in most scenarios not needed. You can enable only the collectors you need ( i.e., disable others) by mentioning them via flag --resources. Here’s how it looks :\nimage: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0 args: - --resources=deployments,persistentvolumeclaims,pods,services,statefulsets Metric denylist : Suppose you have a metric called kube_deployment_spec_strategy_rollingupdate_max_surge which created 1000s of time series which is not-useful. In this case, you can add that metric to denylist of kube-state-metrics with flag --metric-denylist and that metric won\u0026rsquo;t be scraped or collected. --metric-denylist flag also accepts regex if you want to deny multiple matching metrics at once. For example: image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0 args: - --metric-denylist=kube_deployment_spec_.* Resources : https://github.com/kubernetes/kube-state-metrics‣ https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics https://github.com/prometheus/node_exporter https://github.com/google/cadvisor ","permalink":"https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/","summary":"In this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.\n1. Node exporter: Node exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started. Although 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like Grafana cloud, AWS Prometheus and Google Cloud for Prometheus, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent \u0026amp; stored.","title":"How to prevent metrics explosion in Prometheus"},{"content":"Keeping your Prometheus optimized can be a tedious task over time, but it\u0026rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.\nIdentifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage \u0026amp; performance intact.\nIn this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.\nIdentifying the costly metrics : There are 3 ways in which you can get the top 10 costly metrics which are consuming your TSDB :\nVia promtool: Promtool is bundled with Prometheus, if you’re running Prometheus in Kubernetes, then you can just exec into the pod and run the below command. If you’re running Prometheus in VMs, the Promtool binary will be in the same directory as Prometheus. Command : promtool tsdb analyze path-to-data-directory/.\n/prometheus $ promtool tsdb analyze /data/ Block ID: 01FYV026P5AYM47XSSFT4WVG6X Duration: 1h59m59.999s Series: 173756 Label names: 321 Postings (unique label pairs): 18485 Postings entries (total label pairs): 3000877 Label pairs most involved in churning: 32894 namespace=default 14131 job=kubernetes-nodes-cadvisor 13606 job=kubernetes-service-endpoints Label names most involved in churning: 35344 __name__ 35022 instance 35022 job 34345 namespace 28503 pod 15626 container Most common label pairs: 82253 namespace=default 54338 job=linkerd-proxy 54338 control_plane_ns=linkerd 52074 job=kubernetes-service-endpoints 51637 app_kubernetes_io_managed_by=Helm 41292 kubernetes_io_os=linux Label names with highest cumulative label value length: 187875 id 116231 name 83663 __name__ 67642 path 67233 container_id 41271 image 31047 image_id 25053 filename 23436 uid 21800 pod Highest cardinality labels: 2227 __name__ 1846 id 1369 name 921 container_id 844 replicaset 733 owner_name Highest cardinality metric names: 20280 response_latency_ms_bucket 10140 route_response_latency_ms_bucket 5076 etcd_request_duration_seconds_bucket 3380 control_response_latency_ms_bucket 2079 stack_poll_total 1893 container_memory_usage_bytes 1893 container_memory_working_set_bytes 1893 container_cpu_user_seconds_total 1893 container_memory_max_usage_bytes 1893 container_memory_rss 1893 container_cpu_system_seconds_total 1745 tcp_open_total 1745 tcp_read_bytes_total 1745 tcp_open_connections 1745 tcp_write_bytes_total 1700 container_cpu_usage_seconds_total 1474 rest_client_request_latency_seconds_bucket 1424 route_request_total 1295 kube_pod_container_resource_requests 1260 http_client_request_latency_seconds_bucket Via /tsdb-status endpoint: Prometheus already provides outputs of the above tsdb analyze command in easy to understand manner at prometheus-example.com/tsdb-status endpoint.\nThere are more details about the label pairs etc. on that endpoint, but for the scope of this article, we’ll focus on the top 10 series count by metric name. Via PromQL query: If you’re more fond of PromQL to get things done, the above result can also be achieved by running the PromQL query : topk(10, count by (__name__)({__name__=~\u0026#34;.+\u0026#34;})) From the above pointers, now we know what are our costly metrics which we need to be aware of. Let’s assume we decided that we want to drop prometheus_http_request_duration_seconds_bucket \u0026amp; prometheus_http_response_size_bytes_bucket metric, as it\u0026rsquo;s of no practical use to us.\n1. Drop In order to drop the above-mentioned metrics, we need to add metric_relabel_configs in Prometheus scrape config with the metric name we need to drop :\nscrape_configs: - job_name: \u0026#34;prometheus\u0026#34; scrape_interval: 15s static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: \u0026#39;(prometheus_http_request_duration_seconds_bucket|prometheus_http_response_size_bytes_bucket)\u0026#39; action: drop complete flow will be like :\nmetric_relabel_configs : metric relabeling process starts once the metrics is scraped. The reason for that is, the main label __name__ will be generated post scraping.\nsource_labels: we’re utilizing the label __name__ to get the desired metric, since this itself is a label which has value of metric name. i.e __name__=prometheus_http_request_duration_seconds_bucket\nIf you have lots of metrics which you need to drop, a better approach would be to use the action “keep” and mention the metric names you need to keep and everything else will be dropped.\n2. Keep As mentioned above, let’s try the permissive approach of Prometheus. Here we will be only keeping metric prometheus_http_requests_total and drop everything else. The config looks like this : scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; scrape_interval: 5s static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] metric_relabel_configs: - source_labels: [__name__] regex: prometheus_http_requests_total action: keep Gotchas Once you have dropped the metric of choice, new samples won\u0026rsquo;t get stored for that metric. When a metric is not receiving any samples for ~5 min, it will be considered as a stale metric. You can read more about staleness here. Depending on your retention period, the old time series will be removed once it reaches the retention, by default it\u0026rsquo;s 15 days. However, if you want to delete the stored metrics to clear up space, you can follow the below steps to achieve that. Deletion of single metric: In order to delete a series based on label, first you need to enable Admin API. You can enable the flag -web.enable-admin-api to do that. The complete command will look like this : ./prometheus --web.enable-admin-api Send a POST request with the label selectors of your choice to delete the resulting time series from starting date till today, i.e, everything for that time series. For example, the below will delete prometheus_http_requests_total from all scrape job (if there were multiple).\ncurl -X POST -g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=prometheus_http_requests_total\u0026#39; In case you need to delete a metric from a specific scrape job, you can mention the job label.\n-g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=prometheus_http_requests_total{job=\u0026#34;prometheus\u0026#34;}\u0026#39; The above step will mark the time series for deletion, but won\u0026rsquo;t remove them from the disk at that moment. They will get flushed in future compaction. If you need to remove them from the disk to clear the space instantly, you can hit the tombstone endpoint to purge the time series from the disk : curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones Deletion of multiple metrics: If you’re cleaning / optimizing your Prometheus TSDB, deleting a single metric won\u0026rsquo;t do anything. Hence, deletion of 100s if not 1000s of unnecessary time series is evident. At of the time writing this article, you can\u0026rsquo;t delete n metrics which are matched by regular expression. I found a workaround for that via shell scripting for multiple metric deletion at a single shot. Example : There are around 66 go based metrics which are available at Prometheus /metrics endpoint.\nIf you want to delete all of them, then you can perform the below steps, since regex based deletion is not possible as of now. The below command will grab all the metrics which starts with go_ . # get all metric name with go_* into a txt file. curl http://localhost:9090/api/v1/label/__name__/values | tr \u0026#39;,\u0026#39; \u0026#39;\\n\u0026#39; | tr -d \u0026#39;\u0026#34;\u0026#39; | grep \u0026#39;^go_.*\u0026#39; \u0026gt; prom_go_metrics.txt The metrics collected in file will look like : cat prom_go_metrics.txt | head -n 5 go_gc_cycles_forced_gc_cycles_total go_gc_cycles_total_gc_cycles_total go_gc_duration_seconds go_gc_duration_seconds_count go_gc_duration_seconds_sum Create a bash script delete-metrics.sh, append the below contents and run it : #shell script to go over each metric in the file and request the API for deletion recursively. #!/bin/bash for i in `cat prom_go_metrics.txt`; do curl -X POST -g \u0026#39;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=\u0026#39;$i\u0026#39;\u0026#39; done If the deletion is successful, the response will be empty. Once the deletion is successful, use Promtool to query metrics via CLI or check via web UI to see if the metric returns any value. The response should be empty. promtool query series http://localhost:9090 --match=go_gc_heap_allocs_by_size_bytes_total References :\nhttps://prometheus.io/docs/prometheus/latest/querying/api/#delete-series https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs https://relabeler.promlabs.com/ ","permalink":"https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/","summary":"Keeping your Prometheus optimized can be a tedious task over time, but it\u0026rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.\nIdentifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage \u0026amp; performance intact.\nIn this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.","title":"How to drop and delete metrics in Prometheus"},{"content":"In this article, let\u0026rsquo;s try to estimate the Prometheus storage required for an environment.\nPrometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests \u0026amp; stores will increase too leading to disk space issues.\nFrom the docs:\nPrometheus stores an average of only 1-2 bytes per sample. Thus, to plan the capacity of a Prometheus server, you can use the rough formula :\nneeded_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample By default, the metrics retention period is 15 days. In this case, let’s assume you want to keep the metrics for 1 Month. To figure out the above formula, let\u0026rsquo;s check our Prometheus to understand the parameters which are mentioned above.\nHow many samples are stored/ingested This metric is exposed by the Prometheus server which represents a total number of appended samples i.e stored metric samples.\nTo get an average number of samples stored in Prometheus, you can run :\n(rate(prometheus_tsdb_head_samples_appended_total[1d]) Which looks like this:\nWe can see that around 15k metrics are stored in the Prometheus.\nCalculating byte per sample : To calculate, what’s the average size of each sample ingested, we can run the below query :\n(rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d]) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d])) Which is around 1.7 byte / sample in our case :\nPutting it all together : 2592000* (rate(prometheus_tsdb_head_samples_appended_total[1d]) * (rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d]) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]))) 2592000 : the retention period in seconds, in this case 30 days.\nThis gives us the value: 47335001801 bytes = 47.33GB\nTo lower the rate of ingested samples, you can either reduce the number of time series you scrape (fewer targets or fewer series per target), or you can increase the scrape interval. However, reducing the number of series is likely more effective, due to the compression of samples within a series. In case you want to play this around a bit, Grafana is generous enough to give a playground, you can run the queries there.\nReference :\nhttps://prometheus.io/docs/prometheus/latest/storage/#storage\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/","summary":"In this article, let\u0026rsquo;s try to estimate the Prometheus storage required for an environment.\nPrometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests \u0026amp; stores will increase too leading to disk space issues.\nFrom the docs:\nPrometheus stores an average of only 1-2 bytes per sample. Thus, to plan the capacity of a Prometheus server, you can use the rough formula :","title":"How to calculate the storage space required for Prometheus server"},{"content":"This article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.\nDefinition : Readiness Probe in Kubernetes is a probing mechanism to detect health (ready status) of a pod and if the health is intact, then allow the traffic to the pod.\nFrom the official doc,\nSometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don\u0026rsquo;t want to kill the application, but you don\u0026rsquo;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.\nHence, to detect readiness probe failure of our apps, lets configure an alert in Prometheus.\nPrerequisites : Prometheus\nkube-state-metrics\n1. Constructing PromQL expression: All Prometheus queries / expressions are written in a query language called PromQL. You can read more about how to write promQL queries here.\nThe main metrics which gives the status of readiness probe of a pod is : kube_pod_status_ready. The metric has a condition label whose value can be false or true. For the testing of this metric, let\u0026rsquo;s create a Kubernetes deployment with a incorrect readiness probe :\napiVersion: apps/v1 kind: Deployment metadata: name: readiness-test labels: app: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 readinessProbe: httpGet: host: scheme: HTTP path: / httpHeaders: - name: Host value: example.com port: 50 The complete promQL expression will look like this :\nsum by(pod)( kube_pod_info{created_by_kind!=\u0026#34;Job\u0026#34;} AND ON (pod, namespace) kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1) Result :\nExplanation : The below query will filter out all the pods which has failed readiness status. kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1 However, the above also returns the pod names which are in completed state, i.e. from a CronJob. To filter them out, we can use kube_pod_info metrics which got a label created_by_kind!=\u0026quot;Job\u0026quot;, we\u0026rsquo;ll use that to filter in all pods which are not from a cronJob. finally we\u0026rsquo;re aggregating the filtered values with label pod \u0026amp; namespace using function sum(). Alerting: To create a new alert in Prometheus, update the below config values in Prometheus.yml file.\n- alert: KubePodReadinessFailure annotations: description: Readiness probe for the Pod {{ $labels.pod }} is failing for last 10 minutes expr: sum by(pod)( kube_pod_info{created_by_kind!=\u0026#34;Job\u0026#34;} AND ON (pod, namespace) kube_pod_status_ready{condition=\u0026#34;false\u0026#34;} == 1) \u0026gt; 0 for: 10m labels: severity: warning The above alert will wait for 10 min to trigger to filter out misfire and returns the name of the pod which is facing the failure. References :\npod-metrics alerting_rules ","permalink":"https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/","summary":"This article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.\nDefinition : Readiness Probe in Kubernetes is a probing mechanism to detect health (ready status) of a pod and if the health is intact, then allow the traffic to the pod.\nFrom the official doc,\nSometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup.","title":"How to configure Readiness Probe alert in Prometheus"},{"content":"Who will monitor the monitoring system ? Itself\u0026hellip;\u0026hellip;\u0026hellip;sounds a bit magical.\nSince Prometheus monitors everything, it\u0026rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.\nIf Prometheus goes down, you won\u0026rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!\nConfiguring Prometheus to monitor itself Prometheus exposes metrics about itself at /metrics endpoint, hence it can scrape and monitor its own health.\nAdd a Prometheus scrape job in prometheus.yml config file : scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # prometheus endpoint address Restart the Prometheus \u0026amp; curl the /metrics endpoint of Prometheus server to verify: curl http://localhost:1256/metrics # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 7.3633e-05 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 9.2295e-05 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0.000100231 go_gc_duration_seconds{quantile=\u0026#34;0.75\u0026#34;} 0.000110334 go_gc_duration_seconds{quantile=\u0026#34;1\u0026#34;} 0.001204485 go_gc_duration_seconds_sum 43.914716791000004 go_gc_duration_seconds_count 191769 Looks good, let’s move ahead.\nTime Series Prometheus fundamentally stores all data as time series streams of timestamped values belonging to the same metric and the same set of labeled dimensions.\nUnderstanding valuable metrics 1. Active time Series count : prometheus_tsdb_head_series metric type : Gauge\nThis metric shows total number of active time series in the head block. A time series is considered active if new data points have been generated within the last 15 to 30 minutes. A head block in TSDB is an in-memory part of database where time series are stored for a shorter period and then later flushed to persistent disk storage. Read more about how head block works in Prometheus TSDB here. Total Available series in head block can be calculated using the below expression : max_over_time(prometheus_tsdb_head_series[1d]) 2. Time Series Created : prometheus_tsdb_head_series_created_total metric type: Counter\nThis metrics displays total number of time series created in the head block. Since it’s a counter, we can calculate its rate with expression : rate(prometheus_tsdb_head_series_created_total[5m]) 3. Time Series Deleted : prometheus_tsdb_head_series_removed_total metric type: Counter\nThis metrics displays total number of time series removed in the head block. To calculate, rate of deletion of time series with per second average, you can run : rate(prometheus_tsdb_head_series_removed_total[5m]) As you can see from the above graph, the time series are removed from the head and are flushed to persistent storage around every 2 hours. Samples: A sample is a combination of timestamp, value of a time series metric. For example, memory used by container A at time 1:50 \u0026amp; 1:51. These are two samples for the same metric or same time series. 4. Samples ingested : prometheus_tsdb_head_samples_appended_total metric type: Counter\nThis metric shows total number of appended samples into the head block. This metrics is different from the metric scrape_samples_scraped because the appended metric shows the number of samples added / ingested into head block, whereas samples_scraped shows how many samples were scraped. The difference comes into play when you are dropping a lot of metrics via relabel config in your Prometheus config. Sample ingestion rate can be calculated with the expression : rate(prometheus_tsdb_head_samples_appended_total[5m]) 5. Sample size : metric type: Gauge\nThe below expression will give the size of the sample ingested by the Prometheus. In an ideal scenario, the size of each sample will be around 1-2 byte. Sample size monitoring is essential because if there’s anomaly and size goes beyond 3-4 byte, Storage of the server will wear out quickly, leading to disaster. Size of sample can be calculated using the below : (rate(prometheus_tsdb_compaction_chunk_size_bytes_sum[1d])) / rate(prometheus_tsdb_compaction_chunk_samples_sum[1d]) 3. Samples scraped per job : scrape_samples_scraped metric type: Counter\nOver time, you might need to keep an eye on which job is contributing to the highest samples getting scraped and ingested. For that, the above metric comes to the rescue. In an easier way, this metric displays how many metrics are scraped by a job. Hence, you can write a promQL expression to configure an alert too if it breaches a certain threshold. Total samples scraped / job can be calculated by using the below expression : sum by (job)(scrape_samples_scraped) 3. Scrape duration : prometheus_target_interval_length_seconds metric type: Gauge\nYou might also need to monitor the scrape duration heath of your Prometheus. If the duration goes beyond a certain threshold value, the samples will get out of order. More details \u0026amp; debugging out of order samples here. P99 of the scrape duration can be calculated using the below expression :\nprometheus_target_interval_length_seconds{quantile=\u0026#34;0.9\u0026#34;} I’ve listed above some important metrics you can look for, there’s lot more of them at /metrics endpoint of your Prometheus server.\nGrafana Dashboard For quicker insights, I’ve made a Grafana dashboard from all the above expressions mentioned.\nWhich looks like this :\nYou can import the JSON file from here to your Grafana instance to get started.\nReferences : https://www.omerlh.info/2019/03/04/keeping-prometheus-in-shape/\nhttps://prometheus.io/docs/prometheus/latest/getting_started/#configuring-prometheus-to-monitor-itself\nhttps://ganeshvernekar.com/blog/prometheus-tsdb-the-head-block/\n","permalink":"https://tanmay-bhat.github.io/posts/prometheus-self-metrics/","summary":"Who will monitor the monitoring system ? Itself\u0026hellip;\u0026hellip;\u0026hellip;sounds a bit magical.\nSince Prometheus monitors everything, it\u0026rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.\nIf Prometheus goes down, you won\u0026rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!\nConfiguring Prometheus to monitor itself Prometheus exposes metrics about itself at /metrics endpoint, hence it can scrape and monitor its own health.","title":"Self monitoring Prometheus with Grafana"},{"content":"What\u0026rsquo;s on Image Updater A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.\nCapabilities : Argo CD Image Updater can check for new versions of the container images that are deployed with your Kubernetes workloads and automatically update them to their latest allowed version using Argo CD. It works by setting appropriate application parameters for Argo CD applications, i.e. similar to argocd app set --helm-set image.tag=v1.0.1 - but in a fully automated manner. Prerequisite : Kubernetes Cluster ArgoCD setup Steps Installation of ArgoCD Image Updater To install argocd image updater in your cluster ( same one as argocd), run the below command: kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml Once it\u0026rsquo;s installed, let’s check the logs of the pod: kubectl logs -n argocd -l app.kubernetes.io/name=argocd-image-updater time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Starting image update cycle, considering 0 annotated application(s) for update\u0026#34; time=\u0026#34;2022-01-16T06:21:00Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=0 images_considered=0 images_skipped=0 images_updated=0 errors=0\u0026#34; Looks clean, let\u0026rsquo;s move forward.\nConfigure Remote Repository Secret One of the main features of image updater is to write back / update the new image tag to the remote git repo i.e. it will update the image tag, for example, v1.2.3 in the manifest file every time a new image is pushed to image registry. I’ll be using Gitlab in this case. Feel free to use Github also. Obtain the Access token of your account from Gitlab . Once received, run the below command to create the secret : kubectl --namespace argocd \\ create secret generic gitlab-token \\ -from-literal=username=GITLAB_USERNAME \\ --from-literal=password=GITLAB_TOKEN Verify the secret that has been created, by running the below command : kubectl describe secret gitlab-token -n argocd Configure Container registry secret I’m using DigitalOcean Registry as my cloud provider for storing docker images. There are two ways to configure the secret that Kubernetes will use to authenticate to registry endpoint i.e registry.digitalocean.com. Automatic: Follow the on-screen instructions on the DigitalOcean console to auto-integrate the Kubernetes secret into every namespace. Manual: Download the Docker Credentials and follow steps on this link to manually create a Kubernetes secret and then patch to service account of your namespace, argocd in this scenario. Configure Image updater to watch for Image updates. Image updater needs access to container registry to watch the Image updates.\nOut of the box below Registries are supported :\nDocker Hub Registry Google Container Registry (gcr.io) RedHat Quay Registry (quay.io) GitHub Docker Packages (docker.pkg.github.com) GitHub Container Registry (ghcr.io) If you’re storing images in any other registry other than the above-mentioned, no need to worry, we can utilize the config map to add the registry configs.\nTo Digital Ocean configs, Add the below to configmap : argocd-image-updater-config in argocd namespace:\nregistries: - name: \u0026#39;Digital Ocean\u0026#39; api_url: https://registry.digitalocean.com ping: no credentials: pullsecret:argocd/SECRET_NAME ( configured from above step ) defaultns: library prefix: registry.digitalocean.com Here the credentials are in this order : credentials:pullsecret:namespace/secret-name\nThere are other forms of secret configuration as well. Please refer to them here.\nConfigure ArgoCD Application and Annotation Once the Gitlab secret is configured, let\u0026rsquo;s move to the application configuration part. Create the Application manifest file for your application. Here’s a sample definition for kube-ops-view application : Let’s understand the above file in detail :\nWe’re installing this resource in argocd namespace. ( this is mandatory for argocd resources) Annotations : image-list: tells the image updater, which docker image to watch \u0026amp; update. write-back-method: is updating the image tag back to manifest files via git commit git-branch: we’ll write back to the branch: main. server: we’re deploying this to the default cluster. namespace: the actual namespace in which the application will be deployed. path : the path of your helm chart inside the remote git repository. targetRevision: the branch name to which argocd syncs apps. Since we’re using helm, I’m updating to use values from values.yaml file. CreateNamespace: if the mentioned namespace is not found, argocd will create it. Once we’re good with the above configurations, to see the image updater in action\u0026hellip;\nPush a new image tag to your container registry : \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB \u0026gt; docker tag registry.digitalocean.com/tanmaybhat/kube-ops-view:latest registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 \u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE registry.digitalocean.com/tanmaybhat/kube-ops-view latest a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.3 a645de6a07a3 21 months ago 253MB registry.digitalocean.com/tanmaybhat/kube-ops-view v1.2.4 a645de6a07a3 21 months ago 253MB \u0026gt; docker push registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 Once the image is pushed, let\u0026rsquo;s check the logs of image updater pod logs, it should look like this:\ntime=\u0026#34;2022-01-18T13:09:11Z\u0026#34; level=info msg=\u0026#34;git push origin main\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 execID=mvIL8 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=Trace args=\u0026#34;[git push origin main]\u0026#34; dir=/tmp/git-kube-ops-view-demo405157222 operation_name=\u0026#34;exec git\u0026#34; time_ms=2058.666776 time=\u0026#34;2022-01-18T13:09:13Z\u0026#34; level=info msg=\u0026#34;Successfully updated the live application spec\u0026#34; application=kube-ops-view-demo time=\u0026#34;2022-01-18T13:09:14Z\u0026#34; level=info msg=\u0026#34;Processing results: applications=1 images_considered=1 images_skipped=0 images_updated=1 errors=0 Here’s what image updater did :\nDetected a new image tag update in the registry Cloned the repository Made the image update in the manifest file Pushed a commit back to the repository main branch For further verification, let\u0026rsquo;s go to the repo where the helm chart exists, we should be seeing a new commit :\nLet’s see what changed in the file :\nNow, all we have to do is wait for 3 min ( default sync period of argocd), and argocd notices that a new image tag has been updated and the same will be applied to the application.\nLet’s check the image tag as well :\nkubectl describe deploy kube-ops-view -n kube-ops-view | grep Image Image: registry.digitalocean.com/tanmaybhat/kube-ops-view:v1.2.4 ","permalink":"https://tanmay-bhat.github.io/posts/getting-started-with-argocd-image-updater/","summary":"What\u0026rsquo;s on Image Updater A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.\nCapabilities : Argo CD Image Updater can check for new versions of the container images that are deployed with your Kubernetes workloads and automatically update them to their latest allowed version using Argo CD. It works by setting appropriate application parameters for Argo CD applications, i.e. similar to argocd app set --helm-set image.","title":"ArgoCD Image Updater with Digital Ocean Container Registry"},{"content":"Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\nThere are other solutions that grew out of Prometheus for ex Thanos or Cortex.\nI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\nBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring. Here’s how they explain:\nPulling over HTTP offers several of advantages:\nYou can run your monitoring on your laptop when developing changes. You can more easily tell if a target is down. You can manually go to a target and inspect its health with a web browser. Overall, we believe that pulling is slightly better than pushing, but it should not be considered a major point when considering a monitoring system.\nOne of the main problem where pull based mechanism won\u0026rsquo;t work is when\nPrometheus cannot directly reach the server to scrape metrics from it.\nFor this problem, push based mechanism comes to play. A bit about it :\nWith a pull model, it is straightforward to determine whether a node is available using an up metric with a value of 1 when the target is reachable and 0 when it is not. With the push model, the up metric has a value of 1 when the server is running and no value at all when it is not. This distinction is important when monitoring whether your monitoring system is running as expected. Solution : Grafana agent + Remote write 1. Configuring Prometheus as remote receiver endpoint As of Prometheus v2.33.3, this feature is supported, and you can pass flag -web.enable-remote-write-receiver and your server endpoint example.com/api/v1/write will accept remote metrics. Here’s how the config looks if you\u0026rsquo;re running Prometheus in Kubernetes : - name: prometheus-server image: quay.io/prometheus/prometheus:v2.33.3 args: - \u0026#39;--storage.tsdb.retention.time=15d\u0026#39; - \u0026#39;--config.file=/etc/config/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/data\u0026#39; - \u0026#39;--web.console.libraries=/etc/prometheus/console_libraries\u0026#39; - \u0026#39;--web.console.templates=/etc/prometheus/consoles\u0026#39; - \u0026#39;--web.enable-lifecycle\u0026#39; - \u0026#39;--web.enable-remote-write-receiver\u0026#39; 2. Configuring Grafana Agent Grafana Agent is actually Prometheus lite 😛. It just collects and pushes metrics to a remote server.\nNote: I’ll be running Grafana Agent in docker, but there wont be much change to running it via systemd.\nFirst would be to create the agent configuration file ( agent.yaml) : server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 30s external_labels: environment: test-server configs: - name: default scrape_configs: - job_name: agent static_configs: - targets: [\u0026#39;grafana-agent:12345\u0026#39;] - job_name: \u0026#39;node_exporter\u0026#39; static_configs: - targets: [\u0026#39;node_exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] remote_write: - url: https://example.com/api/v1/write basic_auth: username: admin password: secret-password This config is almost identical to a regular Prometheus configuration file. scrape_interval : Time interval at which agent should scrape / collect the metrics. external_labels : Label which you can add for all metrics agent sends to Prometheus for easier identification and analysis later ( key-pair) . We have 3 scrape Jobs in scrape_configs . Each Job tells Prometheus what \u0026amp; where to scrape. remote_write is where the magic happens, its the location where agent should send the metrics it collected. Basic_auth : Basic Authentication for authenticating with the /api/v1/write endpoint. Basic Auth is not a Mandatory Option, but it sure is necessary, else you’re endpoint will be open to public !!! Prometheus needs to be configured with Basic auth initially. Please follow this doc to set it up. Once the config file is completed, you can use the below Docker-compose file to get started with the Grafana agent + cadvisor ( container metrics) + node-exporter ( machine metrics) :\nversion: \u0026#34;3\u0026#34; services: grafana-agent: image: grafana/agent:v0.23.0 container_name: grafana-agent volumes: - /path/to/data/:/etc/agent/data - /path/to/agent.yaml:/etc/agent/agent.yaml node-exporter: image: prom/node-exporter:v1.3.1 container_name: node_exporter command: - \u0026#39;--path.rootfs=/host\u0026#39; network_mode: host pid: host volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; cadvisor: image: gcr.io/cadvisor/cadvisor container_name: cadvisor volumes: - /sys:/sys:ro - /:/rootfs:ro - /var/run:/var/run:rw - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro For Grafana Agent, the volume folder needs to be created before running the docker-compose up command. Grafana agent needs data folder because in-case the agent container restarts, it can resend the metrics which couldn\u0026rsquo;t be sent earlier. It stores metrics up-to last 1 hour. in-case you think it’s too much you can configure metrics DROP for the time series which are not needed in Grafana config file itself Doc. Once both of the above steps are complete, you can check if metrics are being pushed to your Prometheus server by running the below queries :\nFor Node metrics, run the PromQL query :\nrate(node_cpu_seconds_total{environment=\u0026#34;test-server\u0026#34;}[5m]) which should look like : For Container metrics, run the PromQL query :\ncontainer_memory_usage_bytes{environment=\u0026#39;test-server\u0026#39;,name=\u0026#39;grafana-agent\u0026#39;} Now it\u0026rsquo;s up to the DevOps engineer to write useful PromQL queries, Create Grafana dashboards and configure alerts for the same to make use of these metrics.\n","permalink":"https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/","summary":"Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.\nThere are other solutions that grew out of Prometheus for ex Thanos or Cortex.\nI believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.\nBefore we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring.","title":"How to configure Prometheus server as a remote receiver"},{"content":"What\u0026rsquo;s Apps of Apps or Cluster bootstrapping ? The App of Apps Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.\nArgoCD will watch the root application and synchronize any applications that it generates.\nRequirement You may have this question as to why is this even needed, where as you can manaully create the application via argo cli or via argo UI. But once the number of application you manage increases, automation needs to be done. For example, if you have a new app that needs to be created \u0026amp; deployed in your cluster via ArgoCD, apart from argo CLI and UI ( both are manual ) we really dont have any automation on creation of apps except custom scripting. Behold : Apps of apps Actually, it took me a while to understand what exactly it is because the official doc is not well written.\nSince the concept is pretty new and as far as I know, not much of argocd users are utilising this.\nNow that we know why, what lets go over to how.\nStructure Components Root app : the single app you need to deploy to your cluster ( parent/root) child app : your actual microservice applications ( child) child app template : argocd Application kind template describing your application application manifests : the actual micro-service definitions. High level flow\nGit repo → root app → application templates → actual applications\nPrerequisites\nKubernetes cluster Helm v3 ArgoCD installed Demonstration For the purpose of demonstration, here are the components described above :\nroot app template : root.yaml application templates : manifests application definitions : charts/monitoring Let’s take a look at root.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: root-app namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: default project: default source: path: ./apps-of-apps/manifests/ repoURL: https://gitlab.com/Tanmay-Bhat/argocd.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true Name : suggests the name of my root application i.e root-app namespace : all argocd based resources should reside in argocd namespace itself Finaliser : its the kubernetes CRD finalizer which help in protection from accidental deletion of resources. Destination : the cluster onto which the resources should deploy. If you have multiple clusters , mention the required one here. namespace: the actual namespace in which you want to deploy your app path : path inside your Git Repo where the application manifests are. repoURL : the GIT repo address. targetRevision: if its HEAD, argocd will always sync to master branch, if you need to be a different one, update that here. For example, staging branch for Dev environment etc. prune : this enables argocd to auto remove your application and its created resources when you delete the manifest file from the Application templates directory. selfHeal : this enables argocd to auto patch the configs same as latest master branch changes even though new config updates has been done via kubectl. Let’s look at one of the actual application template inside manifests, kube-state-metrics.yaml :\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: kube-state-metrics namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: server: https://kubernetes.default.svc namespace: kube-system project: default source: path: ./Applicationset/charts/monitoring/kube-state-metrics repoURL: https://github.com/tanmay-bhat/ArgoCD.git targetRevision: HEAD syncPolicy: automated: prune: true selfHeal: true name: name of my application namespace : I’m deploying the app to kube-system namespace. path: the path inside GIT repo where hem charts are for the app targetRevision: I’m synching the changes to master branch. repoURL : repo where the helm charts / manifests of app is stored. Once, we understand the above, we can proceed to try them out :\nInstall the root app by running :\nkubectl apply -f root.yaml -n argocd\nOnce installed, go to your ArgoCD UI and you should be able to see:\nNow that our root app is created and healthy, give it a minute and any application template you create inside manifests folder (in my case), will be automatically synced and created for you in the specified namespace.\nIn the above picture, you can see root app is managing multiple apps and auto-syncs when you make changes to the helm charts (github repo in my case)\nThere’s a little arrow in each of the app, if you click on it, it will take you to the applciation page of it in argocd : If you need to add a new application, the flow goes like this, add the actual helm chart/manifest file to your GIT repo → add an argocd template to your templates directory → sit back and enjoy !!\nI’ve added the argocd template and bam !! app is automatically created : Problems Synching apps across multiple clusters. Lets say you want to have node-exporter in multiple clusters, its not possible via apps-of-apps directly. For that you can create a application template and in destination update your cluster name as registred in argocd. This problem is solved by Aplicationset from ArgoCD. Support for multiple values.yaml file for helm charts. Let’s say you want to apply node-selector to 2 clusters dev and prod each with their own customized values which is not possible at the moment. ","permalink":"https://tanmay-bhat.github.io/posts/2022-01-11-introduction-to-argocd-apps-of-apps/","summary":"What\u0026rsquo;s Apps of Apps or Cluster bootstrapping ? The App of Apps Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.","title":"Introduction to ArgoCD : apps of apps"},{"content":"You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster. Before we begin, here are the prerequisites :\nKubernetes cluster Cluster autoscaler Bit of patience Usecase : One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra. You maybe hosting workload in Kubernetes where you wont get traffic post business hours. Or in weekends, you just want to scale down as no traffic flows to your apps during that time. The cost to keep those worker nodes at off hours are pretty high if you calculate for a quarter or for a year. Solution : Though there isn\u0026rsquo;t any one click solution, Kubernetes finds a way or always Kubernetes Admin does !!\nStrangely, there isn\u0026rsquo;t any tool out of the BOX from AWS side, heck not even a blog on how can customers achieve that. Ahem, GCP aced in this scenario.\nBehold \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nKube downscaler : Kube downscaler is a FOSS project by Henning Jacobs who’s the creator of famous project kube-ops-view.\nThis project fits exactly to our requirement as it can scale down below resources in specified timeframe :\nStatefulsets Deployments (HPA too) Cronjobs Installation : Clone the repository: git clone https://codeberg.org/hjacobs/kube-downscaler Update the configmap file deploy/config.yaml to your Cluster TIMEZONE and desired uptime, here’s mine : apiVersion: v1 kind: ConfigMap metadata: name: kube-downscaler data: # timeframe in which your resources should be up DEFAULT_UPTIME: \u0026#34;Mon-Fri 09:30-06:30 Asia/Kolkata\u0026#34; Apply the manifest files : kubectl apply -f deploy/ Working and configuration: As soon as the downscaler pod runs, you can check the logs of it, it should look like ‘Scale down deployment/myapp to replicas 0 (dry-run)`\nAs a safety-plug no scaling operations will happen when the pod starts as --dry-run argument is enabled. Remove that by patching the deployment to start the scaling activity:\nkubectl patch deployment kube-downscaler --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/args\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;--interval=60\u0026#34; ]}]\u0026#39; Once, dry-run argument is removed, all the resources ( deployment, Statefulset \u0026amp; cronjob) wil be scaled down to 0 ( default replica) if current_time ≠ default_uptime mentioned in above mentioned configmap.\nIncase you need to exclude any app from being scaled down, you can annotate that depoyment/statefulset/cronjob with :\nkubectl annotate deploy myapp \u0026#39;downscaler/exclude=true\u0026#39; If you want to have minimum 1 replica after scale down activity, you can annotate the resource : kubectl annotate deploy myapp \u0026#39;downscaler/downtime-replicas=1\u0026#39; Note : No need to annotate uptime value in each deployment or statefulset since by default all pods will be scaled down.\nAdditional tunings like namespace based annotation etc are available at the readme Here.\nAchieving node scale down : Once the pods are scaled down, assuming you have cluster autoscaler configured, it should automatically remove the nodes that are unused or empty from your nodegroup.\nNote: Cluster autoscaler is mandatory since at the end of the day that’s what removes worker nodes to save your bill. ","permalink":"https://tanmay-bhat.github.io/posts/how-to-scale-down-kubernetes-cluster-workloads-during-off-hours/","summary":"You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster. Before we begin, here are the prerequisites :\nKubernetes cluster Cluster autoscaler Bit of patience Usecase : One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra. You maybe hosting workload in Kubernetes where you wont get traffic post business hours.","title":"How to scale down Kubernetes cluster workloads during off hours"},{"content":"One Loadbalancer to rule them all ? you heard it true, Its achievable !\nFor AWS LoadBalancer Controller Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money. Hence we thought how can we use a **single loadbalancer ** across all the namespaces.\nHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 Lets take a look at ArgoCD ingress :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Now, the problem with the above configuration is, by default AWS loadbalancer contoller will create ALB for each namespace. The cost of creating ALB for each namespace is $17.99 and data transfter charges are $0.02 per GB. If you got a lot of namespaces, the cost will be alot over the year. This is not Loadbalancer are supoosed to be used as ALB can handle alot of load in a single unit.\nAfter hunting for a while, we found that the solution lies in using a ALB annotation called group.name on the Ingress object.\nThe flow Assuming you got 5 different ingress with 5 ALB in backend in different namespaces, choose one ingress and add the annotation : alb.ingress.kubernetes.io/group.name: \u0026lt;ANYTHING_MEANINGFULL\u0026gt; Now, apply the same group name annotation to all the ingress objects in all namespaces. AWS LoadBalancer controller will use the first ingress Loadbalcner for all ingress objects and all other 4 ALB in this case will be removed. Below is an example for 2 different namespaces with single ALB :\n#default namespace ingress apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: deployment-A port: number: 80 #argocd namespace ingerss apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd annotations: alb.ingress.kubernetes.io/group.name: staging-lb alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;:80,\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - host: argocd.example.com http: paths: - path: /* pathType: ImplementationSpecific backend: service: name: argo-server port: number: 80 Notice how both ingress objects are using the same annotion with same Group name. Whenever next time you need to create a new ingress in differnt namesapce, you can just add the same *group.name * annotation and it works flawlessly.\nFor NGINX Ingress Controller If you are in any other cloud, lets say DigitalOcean, its cloud provider controller doesn\u0026rsquo;t have any custom LB controller. But dont get sad there buddy, NGINX to the rescue.\nYou can leverage NGINX ingress controller to create \u0026amp; manage LB for you.\nFor installing NGINX ingress controller, you can refer thier documentation which has all the steps to install it.\nLets say you got 2 different namespaces with 2 different ingress objects same as above one for default and one for ArgoCD.\nThe solution is to add ingressClassName: nginx in the ingress object **Spec ** section in each ingress file.\nSo the final ingress file looks like this for default namespace:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-default namespace: default spec: ingressClassName: nginx rules: - host: chartmuseum.example.com http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Argocd Ingress namespace ingress looks like this:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-argocd namespace: argocd spec: ingressClassName: nginx rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 80 Happy Loadbalancing :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-19-using-single-load-balancer-across-multiple-namespaces-in-kubernetes/","summary":"One Loadbalancer to rule them all ? you heard it true, Its achievable !\nFor AWS LoadBalancer Controller Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money. Hence we thought how can we use a **single loadbalancer ** across all the namespaces.\nHere\u0026rsquo;s an example of before migration, how ingress looked like for default namespace:","title":"Using Single Load Balancer across multiple namespaces in Kubernetes"},{"content":" Most DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\nWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\nHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces. I\u0026rsquo;ll list down why :\nSuper easy to set up. Damn cheap. $5 for 250GB storage. One-click public/private button. Easy to integrate CDN if you have any. That being said, Let\u0026rsquo;s look at how we can utilize Spaces as an AWS S3 replacement to host our helm charts.\nSetup \u0026amp; Configuration 1. Create Spaces Log in to your console and click on Spaces and select Create spaces for $5 The name has to be unique and make the permission (File Listing) private. Once done, you should have the endpoint of the spaces created like : Now, go to API settings and create Access keys for Spaces. Please note that the secret key will be displayed only once and hence keep it safe copied somewhere. With that said, you\u0026rsquo;re ready to move to the next step. 2. Install Chartmuseum Add helm repo : helm repo add chartmuseum https://chartmuseum.github.io/charts Update the repo : helm repo update Here, we can\u0026rsquo;t just run helm install since we need to tell chartmuseum to use Space as a holy place to store charts instead of local PVC ( default). Download the chart to your local system by running : helm pull chartmuseum/chartmuseum —untar Update the below fields in values.yaml file :\nSTORAGE: \u0026#34;amazon\u0026#34; STORAGE_AMAZON_BUCKET: \u0026lt; your space name \u0026gt; STORAGE_AMAZON_REGION: \u0026lt; REGION in which your space is created\u0026gt; STORAGE_AMAZON_ENDPOINT: \u0026#34;https://REGION_NAME.digitaloceanspaces.com\u0026#34; #enable API to interact with endpoint DISABLE_API: false #secret section BASIC_AUTH_USER: admin # password for basic http authentication BASIC_AUTH_PASS: secret_password123 #Spaces access key AWS_ACCESS_KEY_ID: \u0026#34;YOUR KEY\u0026#34; #spacess secret key AWS_SECRET_ACCESS_KEY: \u0026#34;YOUR SECRET KEY\u0026#34; That\u0026rsquo;s it, run the below command to install the chart :\nhelm install chartmuseum chartmuseum/ -f chartmuseum/values.yaml Next, add an entry in ingress to point to your Repository URL. For example :\nhost: chartmuseum.tanmaybhat.tk http: paths: - path: / pathType: Prefix backend: service: name: chartmuseum port: number: 80 Please note that you need to set False for Key Disable_API, else you cant send any data to your endpoint.\nThat\u0026rsquo;s it, you can now add your repo to your helm by typing :\nhelm repo add chartmuseum chartmuseum.example.com -u USERNAME -p PASSWORD If you want to push chart to this repo, you can do that by installing the helm push plugin:\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0 Once installed, push the chart by running :\nhelm push chart_directory chartmuseum Happy Helming !\nReferences Artifact Hub\nChart Museum\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-hosting-the-chartmuseum-in-digital-ocean-space/","summary":"Most DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.\nWell, there\u0026rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.\nHowever, if you\u0026rsquo;re an early developer / DevOps engineer or in a small startup who doesn\u0026rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces.","title":"Hosting the Chartmuseum in DigitalOcean Spaces"},{"content":" Introduction Though this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\nUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today.\nI learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.\nFrom Kaniko\u0026rsquo;s doc :\nkaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster.\nkaniko doesn\u0026rsquo;t depend on a Docker daemon and executes each command within a Dockerfile completely in userspace.\nThis enables building container images in environments that can\u0026rsquo;t easily or securely run a Docker daemon, such as a standard Kubernetes cluster.\nLet\u0026rsquo;s see how to achieve this in our pipeline. For simplicity, I\u0026rsquo;ll be using Gitlab CI in this example. You can use circle CI or GitHub actions or anything you like (a little bit of modification required ).\nPrerequisites\nAWS IAM credential ( Access-key and Secret-key) with ECR full access. Bit of time to implement the below 😀 Setup of AWS credentials Go to CI-CD settings of your project and set the below variables with appropriate values:\nAWS_ACCESS_KEY_ID = \u0026lt;your access key\u0026gt; AWS_SECRET_ACCESS_KEY = \u0026lt;your secret key\u0026gt; Gitlab-CI Create a Gitlab CI file that looks like :\nimage: alpine stages: - build_and_push build and push docker image: stage: build_and_push only: variables: - $CI_COMMIT_TAG =~ /^v[0-9]+\\.[0-9]+\\.[0-9]+$/ variables: AWS_DEFAULT_REGION: REGION_NAME CI_REGISTRY_IMAGE: YOUR_ID.dkr.ecr.REGION_NAME.amazonaws.com/REPO_NAME image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\u0026#34;\u0026#34;] script: - mkdir -p /kaniko/.docker - echo \u0026#34;{\\\u0026#34;credsStore\\\u0026#34;:\\\u0026#34;ecr-login\\\u0026#34;}\u0026#34; \u0026gt; /kaniko/.docker/config.json - /kaniko/executor --context \u0026#34;${CI_PROJECT_DIR}\u0026#34; --dockerfile \u0026#34;${CI_PROJECT_DIR}/Dockerfile\u0026#34; --destination \u0026#34;${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}\u0026#34; Let\u0026rsquo;s look at the above pipeline in detail :\nWe\u0026rsquo;re using the base image as alpine. We have one stage which is will build the Dockerfile and push to ECR using Kaniko As we have mentioned only constraint, the pipeline will only trigger when a new tag is pushed to Master branch. Next, we have 2 variables, in which we\u0026rsquo;re defining the default AWS region and our Registry address of ECR. ( please update with your values) Next, we\u0026rsquo;re using the Kaniko base image to build run the scripts mentioned and build our image. Then we\u0026rsquo;re making a docker folder that will have the registry to push credentials. Note that ECR regular login is a bit different than other container registries like Quay or GCR. You won\u0026rsquo;t get the regular username and password for this repo from AWS side. You must know that to log in to ECR, you need to run aws ecr get-login command which will give an authentication token that has a TTL of 12 hours, which doesn\u0026rsquo;t work in our case. Luckily was has created a new ECR login provider extension that will work through IAM permissions. Kaniko has built-in support for that provider, so you just need to add the variable of AWS creds in GitLab CI and Kaniko will take care of the rest. ( magic !) Then we provide Kaniko the path to Dockerfile which will be inside our current project, hence the use of CI_PROJECT_DIR which is a pre-defined variable from GitLab CI points to the current project context. Then I\u0026rsquo;m tagging the image with the latest tag from the repository and pushing to the ECR. Happy CI-CDing !!!\nReference :\nhttps://github.com/GoogleContainerTools/kaniko\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-12-using-kaniko-to-build-and-push-images-through-gitlab-ci-to-ecr/","summary":"Introduction Though this seems like an easy straight forward task by referring to the docs, it\u0026rsquo;s not trust me!\nUntil today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today.\nI learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.","title":"Using Kaniko to build and push images to ECR from Gitlab CI"},{"content":"This happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the us-east-1 region. I asked him about the error message and he said ssh is giving a time-out error.\nSo, I tried connecting to the server via EC2 connect feature (web shell) that AWS provides, and even that said connection timed out.\nTried telnet to the endpoint and was the same also.\nI thought maybe the server may be struck due to overload and restarted it. But the state was still the same once it came up. I saw the metrics of the server via Cloudwatch and everything was fine, saw system logs also and even that looked good.\nCuriously opened status.aws.amazon.com and saw that us-east-1 Region is having an outage.\nBeing a Reddit fan, opened r/sysadmin and I could see people all over the world complaining about AWS being down in that region and 1000\u0026rsquo;s of memes on that topic. I told myself this could be mostly due to the AWS outage and I\u0026rsquo;ll see once they fix it.\nCut to the next day because the outage took 19 long hours to fix the outage. long live SLA!\nI still was not able to connect to the instance post AWS fix. After digging for X time, turns out, the issue was with the subnet in which EC2 was launched. Someone mistakenly attached NAT gateway to the public subnet instead of the Internet gateway. Updated the correct config in the Route-table of the subnet and it worked.\nOne tiny missing detail can totally mess your mind up.\nAnother day of learning :D\nHappy DevOpsing !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-a-tale-of-ec2-connectivity-issue/","summary":"This happened 3 days ago. I received a message from one of our ML engineers that he can\u0026rsquo;t access the EC2 server in the us-east-1 region. I asked him about the error message and he said ssh is giving a time-out error.\nSo, I tried connecting to the server via EC2 connect feature (web shell) that AWS provides, and even that said connection timed out.\nTried telnet to the endpoint and was the same also.","title":"A tale of EC2 connectivity issue"},{"content":"Hey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a Kubernetes challenge and guess what they\u0026rsquo;re giving away free credits of $120 to try it out free!!!\nThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD. Let\u0026rsquo;s get started!\nChallenge Details Link of the challenge page Here Pick one challenge from the list mention in above link based on your knowledge. Create a GitHub or GitLab repo for your project Fill out the code challenge form to get DigitalOcean credits for your project Join the #kubernetes-challenge channel in the DigitalOcean Deploy Discord Complete your challenge Write about what you’ve built and share it on a blog or in your project README. Make a pull request against the Kubernetes Challenge Github Repo with information about your project Let them know you’ve completed your challenge by filling out this form Now that we\u0026rsquo;ve applied let\u0026rsquo;s take a look at one of the challenges I chose :\nDeploy a GitOps CI/CD implementation GitOps is the (only) way automate deployment pipelines for Kubernetes environments in 2022, and ArgoCD is currently one of the leading player. Install it to create a CI/CD solution, using GH Actions for actual image building.\nCluster Creation Sign in to your DO console. Click on NEW button and create a Kubernetes cluster with default values. You can customize the location of cluster nearest to your location to avoid altency issues to API server. Once you submit, it\u0026rsquo;ll take around 10-15 min for the worker nodes and API server to become ready. Click on the Actions button and download the kubeconfig file. Once you download, install kubectl binary by following steps in the Getting started section of *overview *tab. Once, kubectl in installed in your local, you can save / move the config file downloaded to your ~/.kube/config location. Now, you can connect to your API server, test it by running : kubectl get node -o wide Project setup Clone this repository using below command : git clone https://github.com/tanmay-bhat/DigitalOcean-Kubernetes-Challenge-argoCD This project contains below:\ngo app Dockerfile github actions file ( CI) Kubernetes manifest files Let\u0026rsquo;s Look mainly kustomize/base :\nHere, Deployment.yaml file contains the deployment resource YAML file. containers: - image: registry.digitalocean.com/tanmaybhat/saymyname name: saymyname ports: - name: http containerPort: 8080 imagePullSecrets: - name: tanmaybhat Notice the image registry I\u0026rsquo;m using is the Digital Ocean registry itself and not the mostly used Docker Hub.\nThe ImagePullSecrets has a name: Tanmay Bhat. This is the Kubernetes secret which has the Digital Ocean registry credentials which we will use to pull the image.\nNow let\u0026rsquo;s look at our Github actions config file :\nname: Go on: push: branches: [ main ] tags: - \u0026#39;v*.*.*\u0026#39; jobs: build: name: Build runs-on: ubuntu-latest steps: - name: Set up Go 1.x uses: actions/setup-go@v2 with: go-version: ^1.14 - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Login to Digitalocean uses: docker/login-action@v1 with: registry: registry.digitalocean.com username: ${{ secrets.DIGITAL_OCEAN_TOKEN }} password: ${{ secrets.DIGITAL_OCEAN_TOKEN }} - name: push image to digitalocean run: | docker build -t registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} . docker push registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} deploy: name: Deploy runs-on: ubuntu-latest needs: build steps: - name: Check out code uses: actions/checkout@v2 - name: Extract Git Tag run: echo \u0026#34;GIT_TAG=${GITHUB_REF/refs\\/tags\\//}\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: update image tag in manifest uses: imranismail/setup-kustomize@v1 - run: | cd kustomize/base kustomize edit set image registry.digitalocean.com/tanmaybhat/saymyname:${{ env.GIT_TAG }} - name: Commit files run: | git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -am \u0026#34;update image tag to ${{ env.GIT_TAG }}\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} I\u0026rsquo;m gonna explain the above section since the main goal of this article is to do CI/CD :\nThe on section says trigger this piepline if the chnages has been pushed to **Main branch with a tag in format : vx.x.x ( i.e v1.0.0 etc) On each tag push, pipeline will run 2 jobs. Build and Deploy. In Build section, the follwing steps will run on ubuntu image. Check out code step uses pre-build action actions/checkout@v2 to clone current repository into the piepline container i.e ubuntu. Extract Git Tag is used to get the latest tag pusued to main branch and store it in th environmental variable GIT_TAG. In Login to Digitalocean, since we need to push our build docker images to a private registry like Digital Ocean, I\u0026rsquo;m using docker login action to auttenticate to the DO registry. In push image to digitalocean, I\u0026rsquo;m buidling the docker image and tagging it to latest pushed tag version and pushing to my registry. Next comes, the Deploy section. here again I\u0026rsquo;m using ubuntu as base image and again getting the repository from main branch and extracting tag version from the repositiry. Once that is done, I\u0026rsquo;ll use a tool called Kustomize to update my manifest file\u0026rsquo;s docker image tag to the latest tag version. If you\u0026rsquo;re using helm charts only but not kustomize with Helm, you need to use Sed command and update the image tag in manifest file ( deployment.yaml). Later, I\u0026rsquo;m doing the commit of latest tag edit and pushing the changes back to my repo. To sum up, what the exact pipeline does whenever a new tag is pushed to main branch :\nclone the repository, build the docker image, tag it and push it to registry. update the tag in manifest file and push it back to gthe repository. You might have this question, Tanmay, this is just CI, where\u0026rsquo;s CD ? well, that\u0026rsquo;s the magic ArgoCD solves for us.\nArgoCD Setup ArgoCD by running the below commands : kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Once done, you can verify its running status by running the command Next step is to retrieve the password of argocd. For that, run : kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d By default argocd service type will be ClusterIP. That means you cant access argocd outside of your cluster. So, Let\u0026rsquo;s change that to LoadBalancer by running : kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Now, wait for couple more minutes for LoadBalancer to start in Digital Ocean and get the endpoint of it by running : Open the external IP in your browser and voila, you should see argocd UI login page. Login with username: admin password got from step 3. ArgoCD Configuration Once logged in to ArgoCD UI, click on new app and set the below values: application name : demo-argocd Project : default Sync Policy : Automatic Repository URL : \u0026lt;GITHUB REPO URL OF YOUR PROJECT\u0026gt; Rivision : HEAD Path : kustomize/base Destination Cluster : https://kubernetes.default.svc Namespace : default Click create and see your app glowing in your cluster.\nthe ArgoCD magic here is that, it watches for any new changes to your repo every 3 minutes ( default ) and new changes will be auto-applied in your cluster.\nSee the comment that says: update the tag to v1.0.12 ? that was my last commit. If a new tag is committed, here\u0026rsquo;s how it\u0026rsquo;s gonna update and look\nConclusion I found DOKS to be extreme easy to set up and straightforward. From click to integrate Registry to a Kubernetes cluster, easy cluster creation and scaling up.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-12-11-journey-to-the-kubernetes-world-with-digital-ocean/","summary":"Hey all! It\u0026rsquo;s been a long time since I haven\u0026rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a Kubernetes challenge and guess what they\u0026rsquo;re giving away free credits of $120 to try it out free!!!\nThis blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD.","title":"Journey to the Kubernetes world with Digital Ocean"},{"content":"History You may have faced this scenario where you wanna keep scaling up apps nodes but also under-keeping costs at a limit. Spot Instance is the way for that task. Now, how do we do that? let\u0026rsquo;s see.\nAs you know there are mainly 2 types of instances in AWS, called On-demand and Spot. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.\nSpot instances are a bit different. Spot instance is the unused capacity in AWS cloud sitting idle and AWS gives that to you at an extremely low price for like 80-90% cheaper than on-demand. The difference being whenever AWS needs the capacity to handle on-demand, it takes back the spot instances with ~2 min notice via notification to you.\nNode groups Now that we\u0026rsquo;ve learned about what spot offers, it makes total sense to include that in your workload to save quite a lot of money. So let\u0026rsquo;s learn about the node groups for on-demand and spot.\nDesigning node groups : Since spot can go down anytime, you should always run your critical workloads in on-demand instances. All stateful- sets should run in on-demand instances. Have multiple nodegroups for spot so that you can maximize chance of getting spot instances. Use CA for scaling up / down. Real-world scenario Now, let\u0026rsquo;s say you have critical apps running in on-demand NG and other cronjobs or monitoring stacks are in spot NG. If you wanted to schedule pods with the below architecture I got the answer.\nRequired architecture: If your app has 8 replicas, 4/5 of them should run in on-demand NG and 3/4 of them in spot NG such that even if the spot goes down, ondemand can handle the load until the new spot comes in and takes the load. In this way, you\u0026rsquo;ll have \u0026lsquo;Zero Downtime\u0026rsquo;.\nNow for the above problem, there isn\u0026rsquo;t a straightforward or clear-cut solution out of the box. But I\u0026rsquo;ll explain the way I\u0026rsquo;ve implemented it.\nSolution Once both the NG are created, let\u0026rsquo;s take a look at the label of a spot node.\nkubectl describe no ip-100-45-51-226.ap-south-1.compute.internal | grep SPOT eks.amazonaws.com/capacityType=SPOT Any node created by spot will have the above label and any node with ondemand will have label :\neks.amazonaws.com/capacityType=ON_DEMAND Once, they are done, you can create a sample Nginx deployment with the below configs:\nAll the other configs are pretty easy to understand and come under basic Kubernetes concepts. Since nodes created by spot and on-demand NG will have the above-mentioned labels, we can utilize that and request scheduler to try its best effort to schedule 40% of pods in this deployment to SPOT and 60% to ON_DEMAND.\nYou can change the above weight as per your needs. Once the above YAML is deployed, let\u0026rsquo;s take a look at the way pods are scheduled.\nIn my case, nodes with names 25 and 226 are Spot instances. If calculated correctly, 6 pods are running in on-demand and 4 pods are running in spot NG which is exactly as we expected.\nNOTE: This may not be always exactly the ratio you need since the scheduler gives pods to nodes on a best effort basis. But it\u0026rsquo;ll be almost a similar result.\nHappy Kuberneting !!!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-11-19-scheduling-pods-in-both-spot-and-on-demand-nodes-in-eks/","summary":"History You may have faced this scenario where you wanna keep scaling up apps nodes but also under-keeping costs at a limit. Spot Instance is the way for that task. Now, how do we do that? let\u0026rsquo;s see.\nAs you know there are mainly 2 types of instances in AWS, called On-demand and Spot. As the name suggests On-demand is priced highest because it\u0026rsquo;s literally on demand from your side to AWS about node requirement.","title":"Scheduling pods in both Spot and On-demand nodes in EKS"},{"content":"If you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in Skit.ai as a DevOps Engineer.\nOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\nDefinition Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:\nThere are pods that failed to run in the cluster due to insufficient resources. There are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes. Documents If you\u0026rsquo;re going to implement autoscaler in your EKS cluster, please read the FAQ .\nThe setting up of autoscaler in EKS is perfectly written by AWS document here\nOnce, things are set up, the logs should look like below :\nNow if you\u0026rsquo;re getting this, then it means the setup is clean. If we take a closer look at logs, it says node minimum size reached and cant scale down anymore.\nLet\u0026rsquo;s understand scaling up and scale down criteria and it\u0026rsquo;s working.\nScale-down flow Every 10 seconds (configurable by --scan-interval flag),Cluster Autoscaler checks which nodes are unneeded. A node is considered for removal when all below conditions hold:\nThe sum of cpu and memory requests of all pods running on this node is smaller than 50% of the node\u0026rsquo;s allocatable. All pods running on the node (except these that run on all nodes by default, like manifest-run pods or pods created by daemonsets) can be moved to other nodes. It doesn\u0026rsquo;t have scale-down disabled annotation (see How can I prevent Cluster Autoscaler from scaling down a particular node?) If a node is unneeded for more than 10 minutes, it will be terminated.\nCluster Autoscaler terminates one non-empty node at a time to reduce the risk of creating new unschedulable pods.\nThe next node may possibly be terminated just after the first one, if it was also unneeded for more than 10 min and didn\u0026rsquo;t rely on the same nodes in simulation (see below example scenario), but not together. Empty nodes, on the other hand, can be terminated in bulk, up to 10 nodes at a time.\nWhat happens when a non-empty node is terminated? As mentioned above, all pods should be migrated elsewhere.\nCluster Autoscaler does this by evicting them and tainting the node, so they aren\u0026rsquo;t scheduled there again.\nAlso, you should consider the below point :\nIf there\u0026rsquo;s a node which is under-utilized but that node counts towards minimum node group size, then CA wont terminate that node and the logs will be similar to above screenshot. Scale-up flow Scale-up creates a watch on the API server looking for all pods. It checks for any unschedulable pods every 10 seconds (configurable by --scan-interval flag). A pod is unschedulable when the Kubernetes scheduler is unable to find a node that can accommodate the pod. For example, a pod can request more CPU that is available on any of the cluster nodes. Unschedulable pods are recognized by their PodCondition. Whenever a Kubernetes scheduler fails to find a place to run a pod, it sets \u0026ldquo;schedulable\u0026rdquo; PodCondition to false and reason to \u0026ldquo;unschedulable\u0026rdquo;. If there are any items in the unschedulable pods list, Cluster Autoscaler tries to find a new place to run them. Testing the CA Let\u0026rsquo;s assume you got 2 t3.medium node and the min value of nodegroup is 2 with max value set to 5.\u0026lt; Run a nginx deployment with 500 replicas to see if cluster autoscaler scales up the nodes.. The command would be : kubectl create deployment cluster-killer --image=nginx --replicas=500 2 nodes of that size can\u0026rsquo;t handle 500 pods of nginx, so they should be in pending state and CA scans for pending state pods every 10 seconds which should start couple of nodes within minutes. You can verify from command : kubectl get node Once all pods are scheduled, to test scale down, you can either delete the deployment using : kubectl delete deployment cluster-killer Or scale down the replicas to zero with command : kubectl scale deployment cluster-killer --replicas=0 If you refer the logs of cluster autoscaler now, it will mention that X node is uneeded for X min etc. The cool down period by default is 10 min so, after that time, it\u0026rsquo;ll apply taint on that node with name DeletionCandidateOfClusterAutoscaler and ToBeDeletedByClusterAutoscaler and removes the nodes. It looks like below: ","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-a-closer-look-at-cluster-autoscaler-for-eks/","summary":"If you\u0026rsquo;re wondering why do I write about AWS that much, that\u0026rsquo;s because AWS is the cloud on which I spend most of my work hours in Skit.ai as a DevOps Engineer.\nOk, let\u0026rsquo;s take a look at what cluster autoscaler is and how does it work?\nDefinition Cluster Autoscaler is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:","title":"A closer look at Cluster Autoscaler for EKS"},{"content":"Definition Let\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\nIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\nProblem statement Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like : kubectl edit pv your_PV and just change the size, it used to work since you have storage class of kubernetes.io/aws-ebs.\nNow, You can\u0026rsquo;t resize your PV just by changing the size in the manifest file (\u0026gt; EKS v1.17)\nwhat should you do as a Kubernetes admin if you wanna resize your PV with above mentioned version ?\nPrerequisites: AWS EKS cluster Eleveated IAM permissions Understanding of PV in Kubernetes Solution Simple, Kubernetes team has a new tool called ebs-csi controller. What does it do? The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver provides a CSI interface that allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.\nYou can install the ebs-csi driver by referring to AWS document . Once you installation is done, you should see the pods similar to : And the ebs-csi-controller pod logs should look like : Looks good, now, for a test, lets edit a PV and increase its size. In my example, I\u0026rsquo;ll just increase the alert-manager PV to 3GB, Initial size was 2GB. If you are thinking how did I beautify Kubernetes editing ? all thanks to Lens IDE.\nLets verify the PV size now :\nHere comes the real test to see if the actual EBS volume is resized or not. For that let\u0026rsquo;s copy the volume id and search that volume size in AWS console or via AWS cli to verify the disk size.\nTo get the volume id of a PV, run the below command :\nkubectl describe pv PV_NAME | grep Volume Now if you prefer AWS CLI, you can use the following command, else can be verified in AWS console :\n","permalink":"https://tanmay-bhat.github.io/posts/2021-11-13-dynamic-pv-in-kubernetes-feat-eks-ebs/","summary":"Definition Let\u0026rsquo;s Understand what\u0026rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.\nIts the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).\nProblem statement Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like : kubectl edit pv your_PV and just change the size, it used to work since you have storage class of kubernetes.","title":"Dynamic PV in Kubernetes feat. EKS (EBS)"},{"content":"Definition what\u0026rsquo;s kubewatch ?\nkubewatch is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\nLets see how we can deploy it to our cluster.\nPre-requisites :\nKubernetes 1.12+ cluster Helm v3 A slack app and a channel to integrate kubewatch Steps Add Bitnami repo to your helm : helm repo add bitnami https://charts.bitnami.com/bitnami Verify that kubewatch chart is available in the repo : demo\u0026gt; helm search repo kubewatch NAME CHART VERSION APP VERSION DESCRIPTION bitnami/kubewatch 3.2.16 0.1.0 Kubewatch Customize the values like slack integration and enabling RBAC. If you directly do helm install chart-name you wont get any event notification as RBAC is set to false by default kin the helm chart.\nRun the below command to get the values.yaml to local :\nhelm show values bitnami/kubewatch \u0026gt; updated-values.yaml Now edit the yaml file as per your requirement. Here\u0026rsquo;s what I\u0026rsquo;ve changed :\nSlack Integration :\nslack: enabled: true channel: \u0026#34;kubewatch\u0026#34; #your slack channel name ## Create using: https://my.slack.com/services/new/bot and invite the bot to your channel using: /join @botname ## token: \u0026#34;your slack bot token here\u0026#34; Enable RBAC:\n## @section RBAC parameters ## @param rbac.create Whether to create use RBAC resources or not ## rbac: create: true Now lets deploy using the below command : helm install kubewatch bitnami/kubewatch -f ./updated-values.yaml Verify that kubewatch pod is running : demo\u0026gt; kubectl get pod NAME READY STATUS RESTARTS AGE kubewatch-c86656645-8znwk 1/1 Running 0 2m19s To test it out, lets create a nginx deployment with command : kubectl create deploy nginx --image=nginx Check your slack channel for notifications : The indication is as follows :\nGreen : for resources created Yellow : for resources updated Red : for resources deleted You can customize the notification a lot, for example, which namespace to monitor to ( default value is all namespace) , which resource to monitor to like deployment, pod, PV, service etc.\nYou can just edit the configmap : kubewatch-config and change the resources to monitor.\nHappy monitoring !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-15-monitoring-k8s-resource-changes-in-cluster-with-kubewatch/","summary":"Definition what\u0026rsquo;s kubewatch ?\nkubewatch is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.\nLets see how we can deploy it to our cluster.\nPre-requisites :\nKubernetes 1.12+ cluster Helm v3 A slack app and a channel to integrate kubewatch Steps Add Bitnami repo to your helm : helm repo add bitnami https://charts.bitnami.com/bitnami Verify that kubewatch chart is available in the repo : demo\u0026gt; helm search repo kubewatch NAME CHART VERSION APP VERSION DESCRIPTION bitnami/kubewatch 3.","title":"Monitoring K8S resource changes with kubewatch"},{"content":"Previously I have written https://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article about how AWS pushed broken image to Docker hub and we got screwed as we were using latest as image tag.\nWelp, this happened again in our CI/CD pipeline as we were using https://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push plugin from helm and using that to push charts to https://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum .\nSo we were using the below line to pull the helm push plugin :\nhelm plugin install https://github.com/chartmuseum/helm-push.git And were pushing to Chartmuseum via command :\nhelm push app-name repo-name\nIt turns out that command is not valid and as per their latest (v0.10.0) changes to the plugin, its been renamed to cm-push and we gotta use like helm cm-push app-name repo-name. Else we can use the same command with old version of plugin.\nHence our pipeline got screwed and I\u0026rsquo;ve fixed by pulling specific version from their repo by using -version argument. It goes like this :\nhelm plugin install https://github.com/chartmuseum/helm-push.git --version v0.9.0\nThe better solution to this is to replace the hard-coded version above to GitLab CI variable and update the version from there later.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-13-it-happened-again-in-production/","summary":"Previously I have written https://wp.me/pcknFJ-2F\u0026quot;\u0026gt;article about how AWS pushed broken image to Docker hub and we got screwed as we were using latest as image tag.\nWelp, this happened again in our CI/CD pipeline as we were using https://github.com/chartmuseum/helm-push\u0026quot;\u0026gt;push plugin from helm and using that to push charts to https://chartmuseum.com/\u0026quot;\u0026gt;chartmuseum .\nSo we were using the below line to pull the helm push plugin :\nhelm plugin install https://github.com/chartmuseum/helm-push.git And were pushing to Chartmuseum via command :","title":"It happened again in production !!"},{"content":"Question After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\nThere\u0026rsquo;s a reason at least why I had this requirement, Cost optimization in AWS.\nBackground When you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region. Now for staging / testing clusters the Inter Availability Zone data transfer fees we were getting was a hefty one, which was unnecessary as HA is not needed for the testing environment.\nI couldn\u0026rsquo;t find this anywhere else, so with an outage at staging cluster :D ( shhhhh!) I found out that the solution is to create a new node group with AZ hard-coded while creating it and any node you spawn in that node group using ASG (Auto Scaling Group) will be in that single AZ only keeping your inter AZ data transfer cost to 0.\nSolution eksctl create nodegroup --cluster=staging_cluster \\ --region=ap-south-1 \\ --node-zones=ap-south-1a \\ --name=M5.2xlarge_NG \\ --node-type=m5.2xlarge In the above snippet, I\u0026rsquo;m creating NG in Mumbai Region in AZ ap-south-1a with m5.2xlarge instance.\nIf you want to go with GUI way, then :\nGo to your cluster in EKS and then click on Add Node Group : Go with usual flow of giving it a name, taint if required, and IAM role.\nSelect AMI, disk size, instance family etc.\nIn Networking section, by default 3 subnets will be selected, untick 2 of them and keep 1 ( any desired AZ \u0026lsquo;a/b/c\u0026rsquo;). If you\u0026rsquo;re unsure about the name and AZ, you can verify that by going to VPC -\u0026gt; subnets.\nThat\u0026rsquo;s it, create the node group and all the instances will be spawned in that AZ only.\nYou can also do this in a hackish way by editing the ASG corresponding to the node group and removing 2 subnets from there.\nIt works but your node group will become Unhealthy and AWS wont do anything to the node groups which is having health issue. So better to create a new node group.\neksctl There\u0026rsquo;s one more way which I found out i.e to use ekctl command line and create from config file.\nYou can read more about eksctl and configure it by referring here .\nLet\u0026rsquo;s create a config file to create the node group : ap-south-la-NG.yaml :\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: Your_Cluster_Name region: ap-south-1 managedNodeGroups: - name: demo-nodegroup labels: { role: worker-nodes } instanceType: m5.xlarge desiredCapacity: 1 volumeSize: 50 availabilityZones: \u0026amp;#091; ap-south-1a ] minSize: 1 maxSize: 2 volumeType: gp3 privateNetworking: true Then you can apply using the below command :\neksctl create nodegroup --config-file ap-south-la-NG.yaml Finally, once the new Node groups is created, you can scale down your existing node group to 0 so that AWS will drain the nodes gracefully and all your workloads will be moved to newly created node groups.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-10-11-how-to-migrate-all-your-worker-nodes-from-multiple-az-to-single-az-in-aws-eks/","summary":"Question After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.\nThere\u0026rsquo;s a reason at least why I had this requirement, Cost optimization in AWS.\nBackground When you create a EKS cluster, it\u0026rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region.","title":"How to migrate a Node-Group from Multi AZ to single AZ in AWS EKS"},{"content":"History If you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful amusing for you.\nOn Thursday, I started receiving alerts that our CI pipeline is failing.\nI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\nInstalling docker Installation failed. Check that you have permissions to install. Cleaning up file based variables ERROR: Job failed: command terminated with exit code 1 After scratching the head for sometime, I found that the latest aws-cli image from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.\nSo I went to Docker hub and I saw on that day there was a new version pushed which was 2.2.39 tagged as latest. Since in our CI file, we didn\u0026rsquo;t mention specific image version to pull so it always assumes the tag to pull is latest.\nAs a temporary fix, I changed the image version to older one which was 2.2.38 and it worked fine.\nIf you ask me for a better a better solution, it would be always good to use a specific version in production since you know it will work for sure instead of using latest tag which could change every single day.\nElse push that image to your private container repositories like ECR and pull from there.\nI\u0026rsquo;m pretty sure AWS broke few thousand CI pipelines over the world whoever used latest as the image tag :D\nTo give an idea about how to install docker inside aws-cli image, you can just run the below command which should install docker from AWS hosted repo for a faster install :\namazon-linux-extras install docker DevOps story ends here. I\u0026rsquo;ll update more stories like this in future :)\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-19-story-of-keeping-ci-pipeline-from-getting-screwed-when-aws-pushes-broken-docker-image-to-docker-hub/","summary":"History If you\u0026rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful amusing for you.\nOn Thursday, I started receiving alerts that our CI pipeline is failing.\nI started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.\nInstalling docker Installation failed. Check that you have permissions to install. Cleaning up file based variables ERROR: Job failed: command terminated with exit code 1 After scratching the head for sometime, I found that the latest aws-cli image from amazon Docker hub repository is causing the issue as I haven\u0026rsquo;t changed anything else in the CI file in few weeks.","title":"Story of keeping CI pipeline from getting screwed when AWS pushes broken docker image to Docker hub"},{"content":"Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\nI wanted to add a user to my EKS, hence while adding the user to aws-auth configmap of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !!!\nStraight forward solution which I found out :\nFind out who created the EKS cluster ( owner) and ask them to edit the aws-auth configmap to correct your mistakes.\nThe user who created the cluster is the root user for entity. Hence regardless of aws-auth configmap mess, he/she can login via kubectl anytime.\nRead more here on solution by AWS.\nI wrote this because I made this mistake in my company and spent hours searching for answer before finding this info.\nOnce I found out the creator, she corrected it in 1 min. :D\nLong term solution :\nYou might be saying \u0026rsquo; Thats one solution to save my job, how do I make sure I dont do this mistake again ?'\nAlright, so here\u0026rsquo;s what you can follow from next time :\nFirst get the configmap yaml file by typing :\nkubectl get configmap aws-auth -n kube-system -o yaml \u0026gt; aws-auth-configmap.yml Once you get the yaml file, edit the file using your favorite text editor and update your changes.\nNow, update the configmap with your new updated file by typing :\nkubectl apply -n kube-system -f aws-auth-configmap.yml Remember, live editing is never a good option !!!\n","permalink":"https://tanmay-bhat.github.io/posts/2021-09-01-how-to-access-aws-eks-cluster-when-you-mess-up-the-aws-auth-configmap/","summary":"Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:\nI wanted to add a user to my EKS, hence while adding the user to aws-auth configmap of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster\u0026quot; whole cluster is gone, help me please !","title":"How to access AWS EKS cluster when you mess up the aws-auth configmap"},{"content":"Hey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of writing this since I spent hours searching and fixing this :/\nLets look at the problem or the requirement. It goes like this :\nI have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository. how do I pull from that repo ?\nGitlab CI Lets consider the below gitlab-ci.yml file :\nimage: \u0026#34;python:3.6\u0026#34; stages: - publish_image build and push docker image: stage: publish_image only: variables: - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+-\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/ - $CI_COMMIT_TAG =~ /^v\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+\\.\u0026amp;#091;0-9]+$/ variables: DOCKER_HOST: tcp://docker:2375 image: name: amazon/aws-cli entrypoint: \u0026#34;\u0026#34; services: - docker:dind before_script: - echo \u0026#34;$CI_COMMIT_TAG\u0026#34; - amazon-linux-extras install docker - docker login -u \u0026#34;$CI_REGISTRY_USER\u0026#34; -p \u0026#34;$CI_REGISTRY_PASSWORD\u0026#34; $CI_REGISTRY script: - docker build -t $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG . - aws ecr get-login-password | docker login --username AWS --password-stdin $DOCKER_REGISTRY - docker push $DOCKER_REGISTRY/$APP_NAME:$CI_COMMIT_TAG - docker push $DOCKER_REGISTRY/$APP_NAME:$DOCKER_TAG Here\u0026rsquo;s how the above CI file works :\nUses base image python on which the stages will run. has a single stage which will build and push images to ECR only section tells gitlab to run the stage only if the git tag is done and it matched the regex mentioned. in before_script section, we\u0026rsquo;re displaying the commit tag and installing docker in aws-cli image since that image doesn\u0026rsquo;t come preinstalled with docker. finally we\u0026rsquo;re doing docker login to with our dockerhub account before building Dockerfile. Later we build the Dockerfile and then push it to ECR Configure login to Docker hub in GitLab CI To configure the Dockerhub credentials, go to your GitLab project -\u0026gt; settings -\u0026gt; CI/CD In Variables section, add the below Key and their value : Key : CI_REGISTRY || Value : docker.io Key : CI_REGISTRY_USER || Value : your_dockerhub_username Key : CI_REGISTRY_PASSWORD || Value : your_dockerhub_password Now, to setup AWS credentials, configure the below values :\nKey : AWS_ACCESS_KEY_ID || Value : your_aws_accesskey Key : AWS_SECRET_ACCESS_KEY || Value : your_aws_secretkey That\u0026rsquo;s it, voila !! Now GitLab runner should get your docker credentials from variables and pull the image seamlessly.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-08-23-pulling-private-image-from-docker-hub-in-gitlab-ci/","summary":"Hey people ! I\u0026rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of writing this since I spent hours searching and fixing this :/\nLets look at the problem or the requirement. It goes like this :\nI have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository.","title":"Pulling private image from Docker hub in GitLab CI"},{"content":"Hey people, in this article, we\u0026rsquo;ll see how to configure TP -Link TL-WR740N (preferably old one) as repeater to extend your main WI-FI signal in your house.\nLets get into basics real quick.\nWhat\u0026rsquo;s a repeater ?\nDefinition : A WiFi repeater or extender is used to extend the coverage area of your Wi-Fi network. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.\nSteps on secondary router :\nDo a factory reset of your secondary router. You can refer https://youtu.be/4AkkPRE9ZBM\u0026quot;\u0026gt;this video for how-to steps. Once the router is up and running, connect to it wirelessly / through LAN cable. Go to admin console by typing this IP address in browser URL : 192.168.0.1 with credentials , username : admin password : admin ( super secure :D ) Lets first change the IP address of this router to something else rather than the default one as later this IP can cause IP allocation conflict due to DHCP set in primary router. To to that , lets go to Network -\u0026gt; LAN -\u0026gt; IP address and change it to something like 192.168.1.100 .and hit Save. ( you can change it to almost any IP you like in this subnet) Do a reboot of the router and connect back to router console using the new IP in browser URL i.e. in my case 192.168.1.100 or the IP given by you.\nLets configure the repeater mode. To do that, go to Wireless -\u0026gt; Enable WDS Bridging.\nClick on Survey and select the WIFI name which you want to repeat.\nType the password for that in Password field and hit Save. Later you may get alert on switching the repeater to be in same Wi-Fi channel as main router, select ok to that pop-up.\nNext thing would be to setup DHCP of the router. I\u0026rsquo;ll explain a bit here regarding the problem I faced. According to YouTube tutorials and articles out there, we need to disable the DHCP option in secondary router.\nWhat I faced after that is I cant connect any device to that router later as DHCP is disabled, the router wont be able to assign any IP address to any device asking for connection. So your device will be struck in \u0026ldquo;Obtaining IP address\u0026rdquo;.\nSo I found out the below trick and its working brilliantly for me.\nOk, lets through the settings one by one, DHCP Server : Keep it Enable\nStart IP Address: Enter : 192.168.1.101 OR the +1 IP of the assigned IP to your router. i.e\nIf you gave 192.168.1.10 to your router, mention here 192.168.1.11\nEnd IP Address: : Enter 192.168.1.199 or the IP range limit you need. I mentioned here 98 (199 - 101) Address limit assuming my number of devices wont exceed 98 devices :D\n[ Follow the start IP address logic if you mentioned any alternate IP address to router. ]\nAddress Lease Time: Keep the default value.\nDefault Gateway: Here, enter the IP address of your primary router. You can mostly find out by seeing the backside of your primary router.\nElse, you can run the below command via cmd to get the value ( after connecting to primary router) :\nipconfig /all | findstr Gateway\u0026lt;br /\u0026gt;Default Gateway . . . . . . . . . : 192.168.1.1\nDefault Domain: Keep the default value i.e. empty.\nPrimary DNS: You can mention the DNS resolver address. This is optional and same for below one also. if none is mentioned, DNS resolver given by ISP is used. which is not a good solution from privacy perspective. You can use Google Public DNS ( 8.8.8.8) , Quad DNS,(9.9.9.9) Cloud flare DNS (1.1.1.1) here.\nSecondary DNS: This value corresponds to what resolver to use if the request is not resolved by the first DNS. Its good to mention different service to ensure high reliability.\nThat\u0026rsquo;s it. hit Save and do a reboot of the server to get new changes.\nNote : there\u0026rsquo;s a high change you wont be able to connect to your repeater later if you\u0026rsquo;re in a Wi-Fi crowded place i.e. you are surrounded by lot of WIFI. When there are lot of Wifi nearby the router tries to get to channel which is less crowded.\nBut in repeater mode, both repeater and main router needs to be in same Wi-Fi channel. So I would highly advice you to go to your primary router set the Wi-Fi to a particular channel and keep the same channel in repeater also. rather than the default setting : Auto.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-08-11-configuring-tp-link-tl-wr740n-as-wi-fi-repeater/","summary":"Hey people, in this article, we\u0026rsquo;ll see how to configure TP -Link TL-WR740N (preferably old one) as repeater to extend your main WI-FI signal in your house.\nLets get into basics real quick.\nWhat\u0026rsquo;s a repeater ?\nDefinition : A WiFi repeater or extender is used to extend the coverage area of your Wi-Fi network. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.","title":"Configuring TP -Link TL-WR740N as WI-FI repeater"},{"content":"Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\nSteps Open the router login page, which is mostly : 192.168.1.1 in your case. After logging in, navigate to Network page, LAN IP Address tab. Change the Lan Dns Mode to : static Set the primary and secondary DNS address and click on Save/Apply. Perform a reboot of router to apply the changes. There are a lot of DNS providers out there most of them for free. However, please be wise while choosing them.\nI have chosen 1.1.1.1 DNS as my primary server which is provided by Cloudflare.\nI have set the secondary server to 8.8.8.8 which is provided by Google so that if one of the service is down, it will fallback to another.\nList of some of the best DNS providers list in r/sysadmin Psst\u0026hellip;\u0026hellip; Feeling Geeky ?\nPerform DNS benchmark tests : https://www.grc.com/dns/benchmark.htm\nCloudflare DNS validation test : https://1.1.1.1/help\nNeed more? Read the detailed guide for BSNL FTTH : (Fiber optimization\nWorth reading about 3rd party DNS resolvers\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-27-how-to-change-dns-server-for-syrotech-router-bsnl-ftth/","summary":"Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn\u0026rsquo;t find a direct article / steps to do that. Hence, this article.\nSteps Open the router login page, which is mostly : 192.168.1.1 in your case. After logging in, navigate to Network page, LAN IP Address tab. Change the Lan Dns Mode to : static Set the primary and secondary DNS address and click on Save/Apply.","title":"How to change DNS server for Syrotech Router [BSNL FTTH]"},{"content":"Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to Microsoft Events.\nThen you can register for the training of an exam of your choice by searching it in the page.\nSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\nAzure Data Fundamentals\nAzure Fundamentals\nAzure AI fundamentals.\nOnce you register the training and attend it, you can go to that certification website, schedule it through Pearson VUE. If the email used to register for the event is the same as the MSA (Microsoft Account) email used for certification, You should see a banner to claim 100% free voucher, click *claim *to get that, like below.\nThen, you can schedule the exam at your day of choice from home and get certified.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-04-26-how-to-claim-free-azure-certification-vouchers-after-attending-microsoft-events/","summary":"Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to Microsoft Events.\nThen you can register for the training of an exam of your choice by searching it in the page.\nSo far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.\nAzure Data Fundamentals\nAzure Fundamentals","title":"How to claim free Azure certification  vouchers after attending Microsoft Events"},{"content":"Even though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\nActually we should be using **ss **tool installed of netstat !\nAll common network related tools are bundled with package net-tools.\nnwlab:/etc # rpm -qa | grep net-tools net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64 However in openSUSE 15, the team decided to knock it off from net tools package!\nSo, the solution ?\nsudo zypper install net-tools-deprecated\nnwlab:/etc # rpm -qa | grep net-tools net-tools-deprecated-2.0+git20170221.479bb4a-lp152.5.5.x86_64 Once installed, netstat should work totally fine now !\nnwlab:/etc # netstat -ano | grep 9000 tcp6 0 0 :::9000 :::* LISTEN off (0.00/0/0) ","permalink":"https://tanmay-bhat.github.io/posts/2021-01-25-how-to-install-netstat-tool-in-opensuse-15/","summary":"Even though the **netstat **tool is depreciated, sometimes we can\u0026rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.\nActually we should be using **ss **tool installed of netstat !\nAll common network related tools are bundled with package net-tools.\nnwlab:/etc # rpm -qa | grep net-tools net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64 However in openSUSE 15, the team decided to knock it off from net tools package!","title":"How to install netstat tool in openSUSE 15"},{"content":"Hey all ! For those of you who don\u0026rsquo;t know what PWD is below is short explanation :\nSo, PWD stands for Play With Docker. You can deploy learn docker at free with time limit of each instance up-to 10 Hrs!\nFor more info, go to : Docker Labs\nThere are two ways you can access the docker instance.\nUse the web based console. SSH into that instance. I always love to do ssh as it gives me more freedom.\nIf you go straight away and do ssh from your terminal, you will get :\nlab-suse:~/.ssh # ssh ip-x-x-x-@direct.labs.play-with-docker.com ip-x-x-x-@direct.labs.play-with-docker.com: Permission denied (publickey). Why are we getting this ? because there is no fresh key generated in your host.\nLets create a fresh key, run the below command and use default values :\nlab-suse:~/# ssh-keygen After you complete the above command, try ssh again, it should work:\nlab-suse:~/.ssh# ssh ip-x-x-x-@direct.labs.play-with-docker.com The authenticity of host \u0026#39;direct.labs.play-with-docker.com (40.76.55.146)\u0026#39; can\u0026#39;t be established. RSA key fingerprint is SHA256:UyqFRi42lglohSOPKn6Hh9M83Y5Ic9IQn1PTHYqOjEA. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;direct.labs.play-with-docker.com,40.76.55.146\u0026#39; (RSA) to the list of known hosts. Connecting to Ip-x-x-x:8022 ########################### # WARNING!!!! # This is a sandbox environment. Using personal credentials # is HIGHLY! discouraged. Any consequences of doing so are completely # the user\u0026#39;s responsibilites. # The PWD team node1 root@192.168.0.28 Note : if you are using any ssh applications, save that key you generated to a file and load that file in authentication section.\n","permalink":"https://tanmay-bhat.github.io/posts/2021-01-22-how-to-ssh-into-docker-in-pwd-play-with-docker/","summary":"Hey all ! For those of you who don\u0026rsquo;t know what PWD is below is short explanation :\nSo, PWD stands for Play With Docker. You can deploy learn docker at free with time limit of each instance up-to 10 Hrs!\nFor more info, go to : Docker Labs\nThere are two ways you can access the docker instance.\nUse the web based console. SSH into that instance. I always love to do ssh as it gives me more freedom.","title":"How to SSH into docker in PWD (Play With Docker)"},{"content":"Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\nThere are 2 ways you can install mhvtl :\nInstall the rpm package. Directly compile the source code yourself. I have tried using the first method as its easy and fast :D.\nNote: I have tested this install in openSUSE 15.2\nSteps 1.First update the packages to latest version available by typing :\nsudo zypper up\nOnce the packages are up to date, install the below supporting packages :\nsudo zypper install gcc gcc-c++ kernel-devel zlib-devel mt-st mtx lzo-devel perl Now add the repository and install the package : zypper addrepo https://download.opensuse.org/repositories/openSUSE:Leap:15.2:Update/standard/openSUSE:Leap:15.2:Update.repo zypper refresh zypper install mhVTL start the mhvtl service ;\nservice mhvtl start check if mhvtl service is running :\nservice mhvtl status test-machine:/home/azureuser # service mhvtl status ● mhvtl.target - mhvtl service allowing to start/stop all vtltape@.service and vtllibrary@.service instances at once Loaded: loaded (/usr/lib/systemd/system/mhvtl.target; disabled; vendor preset: disabled) Active: active since Thu 2021-01-14 17:13:36 UTC; 50min ago verify if you are able to see the tape library and the drives configured( by default) : test-machine:/home/azureuser # lsscsi -g [1:0:0:0] cd/dvd Msft Virtual CD/ROM 1.0 /dev/sr0 /dev/sg3 [2:0:0:0] disk Msft Virtual Disk 1.0 /dev/sda /dev/sg0 [3:0:1:0] disk Msft Virtual Disk 1.0 /dev/sdb /dev/sg1 [5:0:0:0] disk Msft Virtual Disk 1.0 /dev/sdc /dev/sg2 [6:0:0:0] mediumx STK L700 0162 /dev/sch0 /dev/sg12 [6:0:1:0] tape IBM ULT3580-TD5 0162 /dev/st0 /dev/sg4 [6:0:2:0] tape IBM ULT3580-TD5 0162 /dev/st7 /dev/sg11 [6:0:3:0] tape IBM ULT3580-TD4 0162 /dev/st3 /dev/sg7 [6:0:4:0] tape IBM ULT3580-TD4 0162 /dev/st4 /dev/sg8 [6:0:8:0] mediumx STK L80 0162 /dev/sch1 /dev/sg13 [6:0:9:0] tape STK T10000B 0162 /dev/st2 /dev/sg6 [6:0:10:0] tape STK T10000B 0162 /dev/st5 /dev/sg9 [6:0:11:0] tape STK T10000B 0162 /dev/st1 /dev/sg5 [6:0:12:0] tape STK T10000B 0162 /dev/st6 /dev/sg10 For more details about mhvtl, please refer the below link :\nhttps://sites.google.com/site/linuxvtl2/\n","permalink":"https://tanmay-bhat.github.io/posts/2021-01-14-how-to-install-mhvtl-in-opensuse/","summary":"Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.\nThere are 2 ways you can install mhvtl :\nInstall the rpm package. Directly compile the source code yourself. I have tried using the first method as its easy and fast :D.\nNote: I have tested this install in openSUSE 15.2\nSteps 1.First update the packages to latest version available by typing :","title":"How to install mhVTL in openSUSE"}]