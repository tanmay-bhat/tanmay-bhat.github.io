<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tanmay Bhat</title>
    <link>https://tanmay-bhat.github.io/</link>
    <description>Recent content on Tanmay Bhat</description>
    <image>
      <title>Tanmay Bhat</title>
      <url>https://tanmay-bhat.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://tanmay-bhat.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.8</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://tanmay-bhat.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to modify span attributes in OpenTelemetry using Hooks and Span Processors</title>
      <link>https://tanmay-bhat.github.io/posts/modify-span-attribute-opentelemetry/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/modify-span-attribute-opentelemetry/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In OpenTelemetry, spans are the building blocks of distributed tracing. They represent a single operation within a trace and can have various attributes (labels) associated with them. Sometimes, you may need to modify these attributes such as adding extra attribute or even removing them. In this post, we will explore how to modify span attributes in OpenTelemetry.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I was recently working on adding OpenTelemetry to a Python ETL application. The application was fetching objects from an S3 bucket, and I wanted to modify the span attributes to include the bucket name in the span. As the application was using the &lt;code&gt;boto3&lt;/code&gt; library, and OpenTelemetry has out of the box instrumentation for &lt;code&gt;botocore&lt;/code&gt; using &lt;code&gt;BotocoreInstrumentor&lt;/code&gt; which adds a few span attributes like &lt;code&gt;cloud.region, rpc.method, rpc.service&lt;/code&gt; but not the bucket name. So I had to modify the span attributes to include the bucket name.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reducing Inter-AZ traffic in VictoriaMetrics with Zonekeeper</title>
      <link>https://tanmay-bhat.github.io/posts/zonekeeper/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/zonekeeper/</guid>
      <description>&lt;p&gt;As Kubernetes observability is going mainstream, it is important to understand and make an effort to reduce the cost of running these monitoring systems. One of the major costs in large scale clusters is the inter-AZ traffic. Whether using HA or not, vmagent scrapes metrics from all pods in the cluster, which are spread across multiple availability zones. This results in significant inter-AZ traffic that can become expensive.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine a scenario :&lt;/p&gt;</description>
    </item>
    <item>
      <title>Structs in Go for Mortals</title>
      <link>https://tanmay-bhat.github.io/posts/structs-in-go-for-mortals/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/structs-in-go-for-mortals/</guid>
      <description>&lt;p&gt;Structs in Go are used to group related data together under one type. They are similar to classes (sort of) in other languages. In this post, we will explore how to define and use structs in Go.&lt;/p&gt;
&lt;h3 id=&#34;defining-a-struct&#34;&gt;Defining a Struct&lt;/h3&gt;
&lt;p&gt;To define a struct, you use the &lt;code&gt;type&lt;/code&gt; keyword followed by the name of the struct you&amp;rsquo;d like to create. You then list the fields of the struct. Here&amp;rsquo;s an example of a struct that represents a car:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Negative Lookahead in Prometheus Relabel Config</title>
      <link>https://tanmay-bhat.github.io/posts/prometheus-relabel-negative-lookahead/</link>
      <pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/prometheus-relabel-negative-lookahead/</guid>
      <description>&lt;p&gt;Prometheus is written in Go and uses &lt;a href=&#34;https://github.com/google/re2/wiki/Syntax&#34;&gt;RE2&lt;/a&gt; and that does not support negative lookahead. From convenience perspective, its easier to write human-readable regex using that, but as its not supported, we&amp;rsquo;ll see a workaround for that.&lt;/p&gt;
&lt;p&gt;Say for example, you have a label &lt;code&gt;my_label&lt;/code&gt; and you&amp;rsquo;d like to relabel all the label values to &lt;code&gt;custom_value&lt;/code&gt; where it contains &lt;code&gt;my_value&lt;/code&gt; but only when it does not contain &lt;code&gt;foobar&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If negative lookahead was supported, you could have written something like this:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Go by Instrumenting a Go Application for Prometheus Metrics</title>
      <link>https://tanmay-bhat.github.io/posts/learning-go-by-instrumenting-a-go-application-for-prometheus-metrics/</link>
      <pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/learning-go-by-instrumenting-a-go-application-for-prometheus-metrics/</guid>
      <description>&lt;h3 id=&#34;a-beginners-perspective&#34;&gt;A Beginner&amp;rsquo;s Perspective&lt;/h3&gt;
&lt;p&gt;Before we dive into the details, I recently started learning Go. This article is just a beginner&amp;rsquo;s perspective on combining Go learning and building a simple Prometheus metrics exporter.&lt;/p&gt;
&lt;p&gt;I had a requirement to build this exporter because, at my workplace, we use &lt;a href=&#34;https://www.datadoghq.com/product/service-level-objectives/&#34;&gt;Datadog&amp;rsquo;s SLO&lt;/a&gt; product alongside RUM monitoring. However, since all our other analytics and metrics are in Prometheus, I built this to consolidate all SLOs in one place.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Validating ArgoCD Application Manifest With Open Policy Agent</title>
      <link>https://tanmay-bhat.github.io/posts/validate-argocd-with-opa/</link>
      <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/validate-argocd-with-opa/</guid>
      <description>&lt;p&gt;In the world of GitOps, even experienced DevOps engineers occasionally encounter issues stemming from simple typos or misconfigurations in these YAML files. To mitigate these risks and ensure compliance with organizational policies, we can bring in validation tools.&lt;/p&gt;
&lt;p&gt;This post explores two powerful options: Conftest, which leverages Open Policy Agent (OPA), and Kubeconform. We&amp;rsquo;ll dive into how these tools can be implemented to streamline your validation processes.&lt;/p&gt;
&lt;h3 id=&#34;argocd-custom-policy-validation-with-conftest-opa&#34;&gt;ArgoCD custom policy validation with Conftest (OPA)&lt;/h3&gt;
&lt;p&gt;Open Policy Agent&amp;rsquo;s Conftest serves as an excellent tool for custom config rule validation, offering a more flexible alternative to script-based validation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Leveraging ArgoCD ApplicationSet with Plugin Generator to Streamline Multi-Tenant Deployments</title>
      <link>https://tanmay-bhat.github.io/posts/argocd-plugin-generator-multitenant-deployment/</link>
      <pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/argocd-plugin-generator-multitenant-deployment/</guid>
      <description>&lt;p&gt;In this article, you&amp;rsquo;ll learn how to leverage ArgoCD ApplicationSets with custom generators to streamline multi-tenant deployments. By the end, you&amp;rsquo;ll understand how to create a custom generator plugin, set up an ApplicationSet, and use features like selective deployments and Go templating.&lt;/p&gt;
&lt;h3 id=&#34;prerequisite-&#34;&gt;Prerequisite :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ArgoCD installed in a Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;writing-custom-generator-plugin&#34;&gt;Writing custom generator plugin&lt;/h3&gt;
&lt;p&gt;Generators are responsible for generating &lt;em&gt;parameters&lt;/em&gt;, which are then rendered into the &lt;code&gt;template:&lt;/code&gt; fields of the ApplicationSet resource. See the &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/&#34;&gt;Introduction&lt;/a&gt; for an example of how generators work with templates, to create Argo CD Applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configure JWT Authentication with Grafana</title>
      <link>https://tanmay-bhat.github.io/posts/configure-jwt-auth-grafana/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/configure-jwt-auth-grafana/</guid>
      <description>&lt;p&gt;JSON Web Tokens (JWTs) offer seamless sign-in, allowing users to carry their authentication securely across different applications within the same ecosystem. In this article, let&amp;rsquo;s go through the process of configuring JWT-based authentication with Grafana for a smoother user experience.&lt;/p&gt;
&lt;h3 id=&#34;create-a-rsa-key-pair&#34;&gt;Create a RSA key pair&lt;/h3&gt;
&lt;p&gt;Begin by creating an RSA key pair. The private key will sign the JWT token, while the public key will verify it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ssh-keygen -t rsa -b &lt;span style=&#34;color:#ae81ff&#34;&gt;4096&lt;/span&gt; -m PEM -f grafana.key -N &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl rsa -in grafana.key -pubout -outform PEM -out grafana.key.pub
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;configure-jwt-authentication&#34;&gt;Configure JWT authentication&lt;/h3&gt;
&lt;p&gt;With the key pair in place, configure JWT authentication in Grafana. You can use environment variables or a configuration file. Here&amp;rsquo;s an example using environment variables:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Restricting Label Values in Prometheus via prom-label-proxy</title>
      <link>https://tanmay-bhat.github.io/posts/restrict-label-values-prometheus/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/restrict-label-values-prometheus/</guid>
      <description>&lt;p&gt;You might have come across the situation where you want to restrict certain cluster or environment specific data from being queried by users, for example finance data or other business critical data and not every Grafana user should be able to see this data. This is where label-proxy comes in handy.&lt;/p&gt;
&lt;p&gt;Label-proxy is a small proxy that sits between Grafana and Prometheus and restricts the label values that are being queried.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Switch Between Multiple AWS Profiles In Terminal Using ZSH Functions</title>
      <link>https://tanmay-bhat.github.io/posts/multiple-aws-profile-zsh/</link>
      <pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/multiple-aws-profile-zsh/</guid>
      <description>&lt;p&gt;ZSH functions are a great way to automate tasks and make your life easier. I’ve been using ZSH for a while now, and I recently started using functions to automate tasks that I do on a daily basis.&lt;/p&gt;
&lt;p&gt;Every day, I routinely switch between various AWS profiles, depending on the account and the role that I’m working on. So I wanted to make it easier to switch between profiles.&lt;/p&gt;
&lt;p&gt;In this blog, we’ll see how we can use ZSH functions to switch between AWS profiles. Along with customizing the prompt to show the current AWS profile that we’re using so that we are sure on the account before deleting a database.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Move a Terraform Resource Into a Module Using Moved Block</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-move-a-terraform-resource-into-a-module-using-moved-block/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-move-a-terraform-resource-into-a-module-using-moved-block/</guid>
      <description>&lt;p&gt;Starting from v1.1, Terraform provides a powerful feature known as the &lt;code&gt;moved&lt;/code&gt; block. This feature allows you to reorganize your Terraform configuration without causing Terraform to perceive the refactor as a deletion and creation of resources.&lt;/p&gt;
&lt;p&gt;In this article, we will walk through a few examples of Terraform refactoring using the &lt;code&gt;moved&lt;/code&gt; block.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Terraform (&amp;gt;=1.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;move-a-resource-into-module&#34;&gt;Move a Resource Into Module&lt;/h2&gt;
&lt;p&gt;First, we will create a sample S3 bucket to reference as a standalone resource. In your Terraform configuration directory, create a new &lt;code&gt;./lab-demo/s3.tf&lt;/code&gt; file.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Add Custom Label or Key to Records in Fluentd</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-add-custom-labels-or-keys-to-records-in-fluentd/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-add-custom-labels-or-keys-to-records-in-fluentd/</guid>
      <description>&lt;p&gt;Fluentd is a powerful log collection and processing tool. In this blog post, I will walk you through the process of adding custom label and key to the log records, so that you can better understand your logs and filter them more effectively.&lt;/p&gt;
&lt;p&gt;For example, you might want to add a custom label to your records to indicate the environment in which the log was generated, or the cluster name. This would allow you to filter your logs by environment or cluster name in Elasticsearch or another storage destination.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Overview of Syslog Parsing with Fluentd</title>
      <link>https://tanmay-bhat.github.io/posts/syslog-parsing-with-fluentd/</link>
      <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/syslog-parsing-with-fluentd/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Syslog logging is a widely used method for collecting and storing log data. It is a standard that is supported by many applications and platforms. In this blog post, we will take a look at the basics of syslog parsing with Fluentd.&lt;/p&gt;
&lt;h2 id=&#34;what-is-syslog&#34;&gt;What Is Syslog?&lt;/h2&gt;
&lt;p&gt;Syslog is a protocol used for collecting log data from various sources. It is usually used to collect log data from network devices such as routers and switches, as well as from OS and applications. The log data is then stored on a central syslog server.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Configure Alerting on Ingress-NGINX in Kubernetes</title>
      <link>https://tanmay-bhat.github.io/posts/slo-based-alert-on-ingress-nginx/</link>
      <pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/slo-based-alert-on-ingress-nginx/</guid>
      <description>&lt;p&gt;In this blog post, we will be discussing how to set up monitoring and alerting for Nginx ingress in a Kubernetes environment.&lt;/p&gt;
&lt;p&gt;We will cover the installation and configuration of ingress-nginx, Prometheus and Grafana, and the setup of alerts for key Ingress metrics.&lt;/p&gt;
&lt;h3 id=&#34;pre-requisites-&#34;&gt;Pre-requisites :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Helm v3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-prometheus-and-grafana&#34;&gt;Install Prometheus and Grafana&lt;/h2&gt;
&lt;p&gt;In this step, we will install Prometheus to collect metrics, and Grafana to visualize and create alerts based on those metrics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automate Your Helm Chart Testing Workflow with GitHub Actions</title>
      <link>https://tanmay-bhat.github.io/posts/helm-chart-testing-github-actions/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/helm-chart-testing-github-actions/</guid>
      <description>&lt;p&gt;Helm is a popular open-source package manager for Kubernetes that simplifies the process of installing, upgrading, and managing applications on a Kubernetes cluster. Helm packages, called charts, contain all the necessary resources and configuration for deploying an application on a Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;As with any software project, it&amp;rsquo;s important to test charts before deploying them to ensure that they are reliable and function as intended. Chart testing is the process of verifying the functionality and correctness of a Helm chart before it is deployed to a Kubernetes cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monitor Gaming Laptop Using Grafana and OhmGraphite</title>
      <link>https://tanmay-bhat.github.io/posts/monitor-gaming-laptop-using-grafana--ohmgraphite/</link>
      <pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/monitor-gaming-laptop-using-grafana--ohmgraphite/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Grafana laptop diagram&#34; loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/grafana-laptop-image.png&#34;&gt;
I’ve recently got a gaming laptop and monitoring the CPU &amp;amp; GPU temperature of it has been a tedious task, like install MSI Afterburner, configure statistics server, configure overlay etc.&lt;/p&gt;
&lt;p&gt;That made me use Grafana’s product suite to configure monitoring of key components such as CPU, GPU, Network, Disk and alerting for my laptop such that I can game in peace and when my laptop’s temperature reaches a certain threshold limit, I’ll get a phone call.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Configure Grafana to Use Remote Database for HA</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-configure-grafana-to-use-remote-database-for-ha/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-configure-grafana-to-use-remote-database-for-ha/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Grafana HA diagram&#34; loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/grafana-ha.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this article, we’ll see how to setup Grafana with a remote database so that we can scale Grafana to N instances.&lt;/p&gt;
&lt;p&gt;The default SQLite database will not work with scaling beyond 1 instance since the SQLite3 DB is embedded inside Grafana container.&lt;/p&gt;
&lt;h3 id=&#34;create-remote-postgresql-using&#34;&gt;Create Remote &lt;strong&gt;PostgreSQL using &lt;a href=&#34;http://fly.io&#34;&gt;fly.io&lt;/a&gt; (Optional)&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For this demonstration, I’ll be using &lt;a href=&#34;https://fly.io/docs/reference/postgres/&#34;&gt;fly.io&lt;/a&gt;’s PostgreSQL service.&lt;/li&gt;
&lt;li&gt;Its free (no need to add credit card) and it has 1 GB of storage for DB and should be enough to try out with Grafana.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : If you already have a remote DB such as AWS RDS (or local DB) running, you can skip this step.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to enable Google SSO in Kibana using OAuth2 Proxy [Kubernetes]</title>
      <link>https://tanmay-bhat.github.io/posts/google-sso-kibana/</link>
      <pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/google-sso-kibana/</guid>
      <description>&lt;p&gt;&lt;strong&gt;OAuth2 Proxy&lt;/strong&gt; is a reverse proxy that provides authentication using Providers such as Google, GitHub, and others to validate accounts by email, domain or group.&lt;/p&gt;
&lt;p&gt;In this article, we’ll setup and configure OAuth2 Proxy to enable Google SSO for Kibana in Kubernetes.&lt;/p&gt;
&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Kibana in Kubernetes&lt;/li&gt;
&lt;li&gt;Nginx Ingress&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;setup-google-credentials&#34;&gt;Setup Google Credentials&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In Google Cloud Console, select &lt;a href=&#34;https://console.cloud.google.com/apis/credentials/consent&#34;&gt;OAuth consent screen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select the User Type as : “&lt;strong&gt;Internal”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Complete the app registration form with your &lt;strong&gt;&lt;strong&gt;Authorized domain&lt;/strong&gt;&lt;/strong&gt;, then click &lt;strong&gt;Save.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;In the left Nav pane, choose &lt;a href=&#34;https://console.cloud.google.com/apis/credentials&#34;&gt;Credentials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In the center pane, choose &lt;strong&gt;&amp;ldquo;Credentials&amp;rdquo;&lt;/strong&gt; tab.
&lt;ul&gt;
&lt;li&gt;Open the &lt;strong&gt;&amp;ldquo;New credentials&amp;rdquo;&lt;/strong&gt; drop down&lt;/li&gt;
&lt;li&gt;Choose &lt;strong&gt;&amp;ldquo;OAuth client ID&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Choose &lt;strong&gt;&amp;ldquo;Web application&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Application name can be anything, for simplicity, let’s say Kibana.&lt;/li&gt;
&lt;li&gt;Authorized JavaScript origins is your Kibana endpoint ex: &lt;code&gt;https://kibana.example.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Authorized redirect URIs is the location of &lt;strong&gt;oauth2/callback&lt;/strong&gt; ex: &lt;code&gt;https://kibana.example.com.com/oauth2/callback&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Choose &lt;strong&gt;&amp;ldquo;Create&amp;rdquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Download the Credentials JSON file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;setup-oauth2-proxy&#34;&gt;Setup Oauth2 Proxy&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Generate a random cookie secret with the below command and copy it to clipboard :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -ti --rm python:3-alpine python -c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;import secrets,base64; print(base64.b64encode(base64.b64encode(secrets.token_bytes(16))));&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Add the Helm repository :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Install the helm chart with specified parameters as below :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm upgrade --install oauth2-proxy oauth2-proxy/oauth2-proxy &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--set config.clientID&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GOOGLE_CLIENT_ID_HERE&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--set config.clientSecret&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GOOGLE_CLIENT_SECRET_HERE&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--set config.cookieSecret&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;GENERATED_COOKIE_SECRET_HERE&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--set extraArgs.provider&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;google&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Use the Google Client_ID and SECRET from the downloaded JSON file.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Create and apply the Ingress for Oauth2 Proxy using the below YAML manifest :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yml&#34; data-lang=&#34;yml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;cat &amp;lt;&amp;lt;EOF | kubectl create -f -&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;oauth2-proxy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ingressClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://kibana.example.com&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/oauth2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;oauth2-proxy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4180&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;setup-ingress-annotation-for-kibana&#34;&gt;Setup Ingress annotation for Kibana&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;This article assumes you already have a Kibana setup running in Kubernetes with an Ingress.&lt;/li&gt;
&lt;li&gt;Append the two nginx annotations mentioned below to existing Kibana Ingress. Once updated, the Ingress should look similar to :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yml&#34; data-lang=&#34;yml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kibana&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/auth-signin&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://$host/oauth2/start?rd=$escaped_request_uri&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/auth-url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://$host/oauth2/auth&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ingressClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kibana.example.com&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kibana&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5601&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ImplementationSpecific&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Now if you hit the endpoint &lt;a href=&#34;http://kibana.example.com&#34;&gt;kibana.example.com&lt;/a&gt;. It should ask you to login via Google.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;kibana-sso&#34; loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/kibana-google.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to prevent metrics explosion in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/</guid>
      <description>&lt;p&gt;In this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.&lt;/p&gt;
&lt;h2 id=&#34;1-node-exporter&#34;&gt;1. Node exporter:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Node exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;node-exporter-total&#34; loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/node-exporter-total.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like &lt;a href=&#34;https://grafana.com/products/cloud/&#34;&gt;Grafana cloud&lt;/a&gt;, &lt;a href=&#34;https://aws.amazon.com/prometheus/&#34;&gt;AWS Prometheus&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud for Prometheus&lt;/a&gt;, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent &amp;amp; stored.&lt;/li&gt;
&lt;li&gt;It’s not necessary that you should cut down on metric scraping if you’re sending metrics to any of the vendors mentioned above.&lt;/li&gt;
&lt;li&gt;This also implies to local storage of Prometheus data, since too many time series over time can hinder Prometheus performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;whats-a-collector-&#34;&gt;What’s a collector ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Main components of a node are referred to as collector, for example CPU, file-system, memory etc.&lt;/li&gt;
&lt;li&gt;Each collector exposes a set of metrics about the component it covers. &lt;a href=&#34;https://github.com/prometheus/node_exporter&#34;&gt;Here’s&lt;/a&gt; the list of collectors that are enabled by default.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Disable collectors&lt;/strong&gt; :&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;A collector can be disabled by providing the flag : &lt;code&gt;--no-collector.&amp;lt;collector-name&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#the command will look like this : &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;node_exporter --no-collector.nfs
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Disable self metrics of node-exporter :&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Node-exporter exposes &lt;em&gt;~ 80&lt;/em&gt; metrics about itself at &lt;code&gt;/metrics&lt;/code&gt; along with node metrics.&lt;/li&gt;
&lt;li&gt;The metrics about node-exporter starts with prefix &lt;code&gt;promhttp_*, process_*, go_*&lt;/code&gt;. Below is the list of some of them :&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_cpu_seconds_total
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_max_fds
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_open_fds
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_resident_memory_bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_start_time_seconds
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_virtual_memory_bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;process_virtual_memory_max_bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;promhttp_metric_handler_errors_total
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;promhttp_metric_handler_requests_in_flight
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;promhttp_metric_handler_requests_total
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;amp; around &lt;span style=&#34;color:#ae81ff&#34;&gt;68&lt;/span&gt; go based metrics.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;You can disable all the above ~80 metrics by running node-exporter with flag &lt;code&gt;-web.disable-exporter-metrics&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#the command will look like this : &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;node_exporter --web.disable-exporter-metrics
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Enable only the collectors required&lt;/strong&gt; :&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Opposite to disabling certain collectors, Node exporter has a flag &lt;code&gt;--collector.disable-defaults&lt;/code&gt; which disables all collectors at once.&lt;/li&gt;
&lt;li&gt;Combining that flag with the collector of your choice will only collect the metrics of the collectors you enabled and discard everything else.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;For example&lt;/em&gt;, If you want to collect only the CPU and Memory metrics of a node, you can run the below command :&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to drop and delete metrics in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/</guid>
      <description>&lt;p&gt;Keeping your Prometheus optimized can be a tedious task over time, but it&amp;rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.&lt;/p&gt;
&lt;p&gt;Identifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage &amp;amp; performance intact.&lt;/p&gt;
&lt;p&gt;In this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to calculate the storage space required for Prometheus server</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/</guid>
      <description>&lt;p&gt;In this article, let&amp;rsquo;s try to estimate the Prometheus storage required for an environment.&lt;/p&gt;
&lt;p&gt;Prometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests &amp;amp; stores will increase too leading to disk space issues.&lt;/p&gt;
&lt;p&gt;From the docs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prometheus stores an average of only 1-2 bytes per sample. Thus, to
plan the capacity of a Prometheus server, you can use the rough formula :&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to configure Readiness Probe alert in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/</guid>
      <description>&lt;p&gt;This article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.&lt;/p&gt;
&lt;h3 id=&#34;definition-&#34;&gt;Definition :&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Readiness Probe&lt;/strong&gt; in Kubernetes is a probing mechanism to detect health (ready status)  of a pod and if the health is intact, then allow the traffic to the pod.&lt;/p&gt;
&lt;p&gt;From the official doc,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don&amp;rsquo;t want to kill the application, but you don&amp;rsquo;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self monitoring Prometheus with Grafana</title>
      <link>https://tanmay-bhat.github.io/posts/prometheus-self-metrics/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/prometheus-self-metrics/</guid>
      <description>&lt;p&gt;Who will monitor the monitoring system ? &lt;em&gt;Itself&lt;/em&gt;&amp;hellip;&amp;hellip;&amp;hellip;sounds a bit magical.&lt;/p&gt;
&lt;p&gt;Since Prometheus monitors everything, it&amp;rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.&lt;/p&gt;
&lt;p&gt;If Prometheus goes down, you won&amp;rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!&lt;/p&gt;
&lt;h3 id=&#34;configuring-prometheus-to-monitor-itself&#34;&gt;&lt;strong&gt;&lt;strong&gt;Configuring Prometheus to monitor itself&lt;/strong&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Prometheus exposes metrics about itself  at &lt;code&gt;/metrics&lt;/code&gt; endpoint, hence it can scrape and monitor its own health.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ArgoCD Image Updater with Digital Ocean Container Registry</title>
      <link>https://tanmay-bhat.github.io/posts/getting-started-with-argocd-image-updater/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/getting-started-with-argocd-image-updater/</guid>
      <description>&lt;h2 id=&#34;whats-on-image-updater&#34;&gt;What&amp;rsquo;s on Image Updater&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;A tool to automatically update the container images of Kubernetes workloads
that are managed by &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;Argo CD&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;capabilities-&#34;&gt;Capabilities :&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Argo CD Image Updater can check for new versions of the container images
that are deployed with your Kubernetes workloads and automatically update them
to their latest allowed version using Argo CD.&lt;/li&gt;
&lt;li&gt;It works by setting appropriate
application parameters for Argo CD applications, i.e. similar to
&lt;code&gt;argocd app set --helm-set image.tag=v1.0.1&lt;/code&gt; - but in a fully automated
manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisite-&#34;&gt;Prerequisite :&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Kubernetes Cluster&lt;/li&gt;
&lt;li&gt;ArgoCD setup&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;h3 id=&#34;installation-of-argocd-image-updater&#34;&gt;Installation of ArgoCD Image Updater&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To install argocd image updater in your cluster ( same one as argocd), run the below command:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Once it&amp;rsquo;s installed, let’s check the logs of the pod:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl logs -n argocd -l app.kubernetes.io/name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;argocd-image-updater
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2022-01-16T06:21:00Z&amp;#34;&lt;/span&gt; level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;info msg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Starting image update cycle, considering 0 annotated application(s) for update&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2022-01-16T06:21:00Z&amp;#34;&lt;/span&gt; level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;info msg&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Processing results: applications=0 images_considered=0 images_skipped=0 images_updated=0 errors=0&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looks clean, let&amp;rsquo;s move forward.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to configure Prometheus server as a remote receiver</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/</guid>
      <description>&lt;h2 id=&#34;push-vs-pull&#34;&gt;Push vs Pull&lt;/h2&gt;
&lt;p&gt;Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.&lt;/p&gt;
&lt;p&gt;There are other solutions that grew out of Prometheus for ex &lt;a href=&#34;https://thanos.io/&#34;&gt;Thanos&lt;/a&gt; or &lt;a href=&#34;https://cortexmetrics.io/&#34;&gt;Cortex&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.&lt;/p&gt;
&lt;p&gt;Before we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring. Here’s how they explain:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to ArgoCD : apps of apps</title>
      <link>https://tanmay-bhat.github.io/posts/2022-01-11-introduction-to-argocd-apps-of-apps/</link>
      <pubDate>Tue, 11 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2022-01-11-introduction-to-argocd-apps-of-apps/</guid>
      <description>&lt;h3 id=&#34;whats-apps-of-apps--or-cluster-bootstrapping-&#34;&gt;What&amp;rsquo;s Apps of Apps  or Cluster bootstrapping ?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://argoproj.github.io/argo-cd/operator-manual/cluster-bootstrapping/&#34;&gt;The App of Apps&lt;/a&gt; Pattern helps us define a root Application. So, rather than point to an application manifest fort every application creation, the Root App points to a folder  which contains the Application YAML definition for each child App. Each child app’s Application YAML then points to a directory containing the actual application manifests be it in manifest file, Helm or Kustomize.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to scale down Kubernetes cluster workloads during off hours</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-scale-down-kubernetes-cluster-workloads-during-off-hours/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/how-to-scale-down-kubernetes-cluster-workloads-during-off-hours/</guid>
      <description>&lt;h3 id=&#34;you-heard-it-right-everyone-needs-to-rest-once-a-while-even-our-little-kubernetes-cluster&#34;&gt;You heard it right, everyone needs to rest once a while, even our little Kubernetes cluster.&lt;/h3&gt;
&lt;p&gt;Before we begin, here are the prerequisites :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Cluster autoscaler&lt;/li&gt;
&lt;li&gt;Bit of patience&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usecase-&#34;&gt;Usecase :&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One of the most important aspect when it comes to running workload in cloud is to keep cost under control or tune it such that you can save extra.&lt;/li&gt;
&lt;li&gt;You maybe hosting workload in Kubernetes where you wont get traffic post business hours.&lt;/li&gt;
&lt;li&gt;Or in weekends, you just want to scale down as no traffic flows to your apps during that time.&lt;/li&gt;
&lt;li&gt;The cost to keep those worker nodes at off hours are pretty high if you calculate for a quarter or for a year.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;solution-&#34;&gt;Solution :&lt;/h2&gt;
&lt;p&gt;Though there isn&amp;rsquo;t any one click solution, Kubernetes finds a way or always Kubernetes Admin does !!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Single Load Balancer across multiple namespaces in Kubernetes</title>
      <link>https://tanmay-bhat.github.io/posts/2021-12-19-using-single-load-balancer-across-multiple-namespaces-in-kubernetes/</link>
      <pubDate>Sun, 19 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-12-19-using-single-load-balancer-across-multiple-namespaces-in-kubernetes/</guid>
      <description>&lt;p&gt;One Loadbalancer to rule them all ? you heard it true, Its achievable !&lt;/p&gt;
&lt;h2 id=&#34;for-aws-loadbalancer-controller&#34;&gt;For AWS LoadBalancer Controller&lt;/h2&gt;
&lt;p&gt;Until couple weeks ago, we were creating a loadbalancer for each namespace ( by default from AWS), which was a waste of resources and money.
Hence we thought how can we use a **single loadbalancer ** across all the namespaces.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example of before migration, how ingress looked like for default namespace:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hosting the Chartmuseum in DigitalOcean Spaces</title>
      <link>https://tanmay-bhat.github.io/posts/2021-12-12-hosting-the-chartmuseum-in-digital-ocean-space/</link>
      <pubDate>Sun, 12 Dec 2021 21:26:48 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-12-12-hosting-the-chartmuseum-in-digital-ocean-space/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;image info&#34; loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/vlz-1.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Most DevOps engineers who use Chartmuseum to store/host their helm charts use S3 as their storage medium. Well, I wanted to try Digital Ocean spaces as its S3 compatible storage option.&lt;/p&gt;
&lt;p&gt;Well, there&amp;rsquo;s an obvious reason why to use S3 in the first place. Beautiful integration with AWS other services, cheap, easy to access, versioning, MFA delete protection etc.&lt;/p&gt;
&lt;p&gt;However, if you&amp;rsquo;re an early developer / DevOps engineer or in a small startup who doesn&amp;rsquo;t wanna go through 1000 configurations in AWS just to create one single storage bucket in the cloud and again go through 1000 more security hurdles in case you want this bucket to be public, you should use DO Spaces. I&amp;rsquo;ll list down why :&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Kaniko to build and push images to ECR from Gitlab CI</title>
      <link>https://tanmay-bhat.github.io/posts/2021-12-12-using-kaniko-to-build-and-push-images-through-gitlab-ci-to-ecr/</link>
      <pubDate>Sun, 12 Dec 2021 12:28:22 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-12-12-using-kaniko-to-build-and-push-images-through-gitlab-ci-to-ecr/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/Kaniko-Logo.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Though this seems like an easy straight forward task by referring to the docs, it&amp;rsquo;s not trust me!&lt;/p&gt;
&lt;p&gt;Until today in my Gitlab CI, I used to use aws-cli image and later install amazon-linux extras install docker and then use DIND service to build docker images through Gitlab-CI. that will change from today.&lt;/p&gt;
&lt;p&gt;I learned about the tool called Kaniko from Google which is built to simplify the docker build process without using Docker daemon hence not giving root-level privileges to the runner hence security says top-notch during the build process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A tale of EC2 connectivity issue</title>
      <link>https://tanmay-bhat.github.io/posts/2021-12-11-a-tale-of-ec2-connectivity-issue/</link>
      <pubDate>Sat, 11 Dec 2021 17:58:40 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-12-11-a-tale-of-ec2-connectivity-issue/</guid>
      <description>&lt;p&gt;This happened 3 days ago. I received a message from one of our ML engineers that he can&amp;rsquo;t access the EC2 server in the &lt;em&gt;us-east-1&lt;/em&gt; region. I asked him about the error message and he said ssh is giving a time-out error.&lt;/p&gt;
&lt;p&gt;So, I tried connecting to the server via EC2 connect feature  &lt;em&gt;(web shell)&lt;/em&gt; that AWS provides, and even that said connection timed out.&lt;/p&gt;
&lt;p&gt;Tried telnet to the endpoint and was the same also.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Journey to the Kubernetes world with Digital Ocean</title>
      <link>https://tanmay-bhat.github.io/posts/2021-12-11-journey-to-the-kubernetes-world-with-digital-ocean/</link>
      <pubDate>Sat, 11 Dec 2021 15:38:19 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-12-11-journey-to-the-kubernetes-world-with-digital-ocean/</guid>
      <description>&lt;p&gt;Hey all! It&amp;rsquo;s been a long time since I haven&amp;rsquo;t written a blog about Kubernetes. So I was wandering in r/devops in Reddit and saw a post where the digital ocean is hosting a &lt;strong&gt;Kubernetes challenge&lt;/strong&gt; and guess what they&amp;rsquo;re giving away free credits of &lt;em&gt;$120&lt;/em&gt; to try it out free!!!&lt;/p&gt;
&lt;p&gt;This blog is written in multiple sections from steps to apply to steps to deploy your app in Digital Ocean Kubernetes via CI/CD. Let&amp;rsquo;s get started!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scheduling pods in both Spot and On-demand nodes in EKS</title>
      <link>https://tanmay-bhat.github.io/posts/2021-11-19-scheduling-pods-in-both-spot-and-on-demand-nodes-in-eks/</link>
      <pubDate>Fri, 19 Nov 2021 01:46:58 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-11-19-scheduling-pods-in-both-spot-and-on-demand-nodes-in-eks/</guid>
      <description>&lt;h3 id=&#34;history&#34;&gt;History&lt;/h3&gt;
&lt;p&gt;You may have faced this scenario where you wanna keep scaling up apps  nodes but also under-keeping costs at a limit.  Spot Instance is the way for that task. Now,  how do we do that? let&amp;rsquo;s see.&lt;/p&gt;
&lt;p&gt;As you know there are mainly 2 types of instances in AWS, called &lt;strong&gt;On-demand&lt;/strong&gt; and &lt;strong&gt;Spot&lt;/strong&gt;. As the name suggests On-demand is priced highest because it&amp;rsquo;s literally on demand from your side to AWS about node requirement.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A closer look at Cluster Autoscaler for EKS</title>
      <link>https://tanmay-bhat.github.io/posts/2021-11-13-a-closer-look-at-cluster-autoscaler-for-eks/</link>
      <pubDate>Sat, 13 Nov 2021 21:40:26 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-11-13-a-closer-look-at-cluster-autoscaler-for-eks/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re wondering why do I write about AWS that much, that&amp;rsquo;s because AWS is the cloud on which I spend most of my work hours in &lt;a href=&#34;https://skit.ai&#34;&gt;Skit.ai&lt;/a&gt; as a DevOps Engineer.&lt;/p&gt;
&lt;p&gt;Ok, let&amp;rsquo;s take a look at what cluster autoscaler is and how does it work?&lt;/p&gt;
&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Cluster Autoscaler&lt;/strong&gt; is a tool that automatically adjusts the size of the Kubernetes cluster when one of the following conditions is true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are pods that failed to run in the cluster due to insufficient resources.&lt;/li&gt;
&lt;li&gt;There are nodes in the cluster that have been underutilized for an extended period of time and their pods can be placed on other existing nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;documents&#34;&gt;Documents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you&amp;rsquo;re going to implement autoscaler in your EKS cluster, please read the &lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md&#34;&gt;FAQ&lt;/a&gt; .&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dynamic PV in Kubernetes feat. EKS (EBS)</title>
      <link>https://tanmay-bhat.github.io/posts/2021-11-13-dynamic-pv-in-kubernetes-feat-eks-ebs/</link>
      <pubDate>Sat, 13 Nov 2021 20:13:38 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-11-13-dynamic-pv-in-kubernetes-feat-eks-ebs/</guid>
      <description>&lt;h3 id=&#34;definition&#34;&gt;Definition&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s Understand what&amp;rsquo;s volume resizing mean for Persistent Volumes kin KUbernetes.&lt;/p&gt;
&lt;p&gt;Its the ability to dynamically increase the PV size as required ( EBS volume behind the scene ).&lt;/p&gt;
&lt;h4 id=&#34;problem-statement&#34;&gt;Problem statement&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Up until v1.16 EKS, you can just increase any ( PV ) EBS volume size just by running command like&lt;/strong&gt; : &lt;code&gt;kubectl edit pv your_PV&lt;/code&gt; &lt;strong&gt;and just change the size, it used to work since you have storage class  of&lt;/strong&gt; &lt;code&gt;kubernetes.io/aws-ebs&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monitoring K8S resource changes with kubewatch</title>
      <link>https://tanmay-bhat.github.io/posts/2021-10-15-monitoring-k8s-resource-changes-in-cluster-with-kubewatch/</link>
      <pubDate>Fri, 15 Oct 2021 21:01:04 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-10-15-monitoring-k8s-resource-changes-in-cluster-with-kubewatch/</guid>
      <description>&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;
&lt;p&gt;what&amp;rsquo;s kubewatch ?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/bitnami-labs/kubewatch%22%3E&#34;&gt;kubewatch&lt;/a&gt; is a Kubernetes watcher that currently publishes notification to Slack. Deploy it in your k8s cluster, and you will get event notifications in a slack channel.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Lets see how we can deploy it to our cluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-requisites :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.12+ cluster&lt;/li&gt;
&lt;li&gt;Helm v3&lt;/li&gt;
&lt;li&gt;A slack app and a channel to integrate kubewatch&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Add Bitnami repo to your helm :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm repo add bitnami https://charts.bitnami.com/bitnami
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Verify that kubewatch chart is available in the repo :&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;demo&amp;gt; helm search repo kubewatch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                    CHART VERSION   APP VERSION     DESCRIPTION
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bitnami/kubewatch       3.2.16          0.1.0           Kubewatch
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Customize the values like slack integration and enabling RBAC. If you directly do &lt;strong&gt;helm install chart-name&lt;/strong&gt; you wont get any event notification as RBAC is set to &lt;strong&gt;false&lt;/strong&gt; by default kin the helm chart.&lt;/p&gt;</description>
    </item>
    <item>
      <title>It happened again in production !!</title>
      <link>https://tanmay-bhat.github.io/posts/2021-10-13-it-happened-again-in-production/</link>
      <pubDate>Wed, 13 Oct 2021 21:07:07 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-10-13-it-happened-again-in-production/</guid>
      <description>&lt;p&gt;Previously I have written     &lt;a href=&#34;https://wp.me/pcknFJ-2F%22%3Earticle&#34;&gt;https://wp.me/pcknFJ-2F&amp;quot;&amp;gt;article&lt;/a&gt;  about how AWS pushed broken image to Docker hub and we got screwed as we were using &lt;em&gt;latest&lt;/em&gt; as image tag.&lt;/p&gt;
&lt;p&gt;Welp, this happened again in our CI/CD pipeline as we were using     &lt;a href=&#34;https://github.com/chartmuseum/helm-push%22%3Epush&#34;&gt;https://github.com/chartmuseum/helm-push&amp;quot;&amp;gt;push&lt;/a&gt;  plugin from helm and using that to push charts to     &lt;a href=&#34;https://chartmuseum.com/%22%3Echartmuseum&#34;&gt;https://chartmuseum.com/&amp;quot;&amp;gt;chartmuseum&lt;/a&gt; .&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;So we were using the below line to pull the helm push plugin :&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;helm plugin install https://github.com/chartmuseum/helm-push.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And were pushing to Chartmuseum via command :&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to migrate a Node-Group from Multi AZ to single AZ in AWS EKS</title>
      <link>https://tanmay-bhat.github.io/posts/2021-10-11-how-to-migrate-all-your-worker-nodes-from-multiple-az-to-single-az-in-aws-eks/</link>
      <pubDate>Mon, 11 Oct 2021 12:46:58 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-10-11-how-to-migrate-all-your-worker-nodes-from-multiple-az-to-single-az-in-aws-eks/</guid>
      <description>&lt;h3 id=&#34;question&#34;&gt;Question&lt;/h3&gt;
&lt;p&gt;After reading the above title you maybe thinking why though? moving the complete worker node fleet into single Availability Zone (AZ) is not a good solution when it comes to high availability of your Kubernetes cluster workload.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a reason at least why I had this requirement, &lt;strong&gt;Cost optimization in AWS&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;When you create a EKS cluster, it&amp;rsquo;ll have 3 subnets each correcting to a single AZ i.e 3 AZ in a region. Now for staging / testing clusters the Inter Availability Zone data transfer fees we were getting was a hefty one, which was unnecessary as HA is not needed for the testing environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Story of keeping CI pipeline from getting screwed when AWS pushes broken docker image to Docker hub</title>
      <link>https://tanmay-bhat.github.io/posts/2021-09-19-story-of-keeping-ci-pipeline-from-getting-screwed-when-aws-pushes-broken-docker-image-to-docker-hub/</link>
      <pubDate>Sun, 19 Sep 2021 20:15:04 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-09-19-story-of-keeping-ci-pipeline-from-getting-screwed-when-aws-pushes-broken-docker-image-to-docker-hub/</guid>
      <description>&lt;h2 id=&#34;history&#34;&gt;History&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re using aws-cli docker image in your CI pipeline then this story could be useful   amusing for you.&lt;/p&gt;
&lt;p&gt;On Thursday, I started receiving alerts that our CI pipeline is failing.&lt;/p&gt;
&lt;p&gt;I started checking the failed job error and it pointed out to docker is unable to install killing the pipeline.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Installing docker
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Installation failed. Check that you have permissions to install.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Cleaning up file based variables
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ERROR: Job failed: command terminated with exit code 1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After scratching the head for sometime, I found that the latest &lt;code&gt;aws-cli &lt;/code&gt;image from amazon Docker hub repository is causing the issue as I haven&amp;rsquo;t changed anything else in the CI file in few weeks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to access AWS EKS cluster when you mess up the aws-auth configmap</title>
      <link>https://tanmay-bhat.github.io/posts/2021-09-01-how-to-access-aws-eks-cluster-when-you-mess-up-the-aws-auth-configmap/</link>
      <pubDate>Wed, 01 Sep 2021 11:34:29 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-09-01-how-to-access-aws-eks-cluster-when-you-mess-up-the-aws-auth-configmap/</guid>
      <description>&lt;p&gt;Hey people, this is not a complete solution article, but rather a cut story and a probable solution for the below problem statement when it comes to locked out issue in EKS cluster:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wanted to add a user to my EKS, hence while adding the user to &lt;code&gt;aws-auth configmap&lt;/code&gt; of my EKS cluster, I made some syntax mistakes and now neither I nor anyone can login to EKS cluster&amp;quot; whole cluster is gone, help me please !!!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pulling private image from Docker hub in GitLab CI</title>
      <link>https://tanmay-bhat.github.io/posts/2021-08-23-pulling-private-image-from-docker-hub-in-gitlab-ci/</link>
      <pubDate>Mon, 23 Aug 2021 20:20:33 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-08-23-pulling-private-image-from-docker-hub-in-gitlab-ci/</guid>
      <description>&lt;p&gt;Hey people ! I&amp;rsquo;m back this time with a how-to on GitLab CI to make your life easy being DevOps Engineer. I thought of  writing this since I spent hours searching and fixing this :/&lt;/p&gt;
&lt;p&gt;Lets look at the problem or the requirement. It goes like this :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have a GitLab CI file integrated into my project which builds a Dockerfile and pushes that image into ECR. But the dockerfile has a base image which is from a private Docker hub repository. how do I pull from that repo ?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuring TP -Link TL-WR740N as WI-FI repeater</title>
      <link>https://tanmay-bhat.github.io/posts/2021-08-11-configuring-tp-link-tl-wr740n-as-wi-fi-repeater/</link>
      <pubDate>Wed, 11 Aug 2021 00:36:50 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-08-11-configuring-tp-link-tl-wr740n-as-wi-fi-repeater/</guid>
      <description>&lt;p&gt;Hey people, in this article, we&amp;rsquo;ll see how to configure TP -Link &lt;a href=&#34;https://www.tp-link.com/in/home-networking/wifi-router/tl-wr740n/&#34;&gt;TL-WR740N&lt;/a&gt; (preferably old one) as repeater to extend your main WI-FI signal in your house.&lt;/p&gt;
&lt;p&gt;Lets get into basics real quick.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&amp;rsquo;s a repeater ?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Definition : A WiFi repeater or extender is used &lt;strong&gt;to extend the coverage area of your Wi-Fi network&lt;/strong&gt;. It works by receiving your existing Wi-Fi signal, amplifying it and then transmitting the boosted signal.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to change DNS server for Syrotech Router [BSNL FTTH]</title>
      <link>https://tanmay-bhat.github.io/posts/2021-04-27-how-to-change-dns-server-for-syrotech-router-bsnl-ftth/</link>
      <pubDate>Tue, 27 Apr 2021 18:26:11 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-04-27-how-to-change-dns-server-for-syrotech-router-bsnl-ftth/</guid>
      <description>&lt;p&gt;Ok, to be honest, I searched a lot on the internet to change ISP DNS servers to 3rd party servers (which you should !) for my router and couldn&amp;rsquo;t find a direct article / steps to do that. Hence, this article.&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Open the router login page, which is mostly : &lt;a href=&#34;http://192.168.1.1&#34;&gt;192.168.1.1&lt;/a&gt; in your case.&lt;/li&gt;
&lt;li&gt;After logging in, navigate to Network page,  LAN IP Address tab.&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Change the &lt;em&gt;Lan Dns Mode&lt;/em&gt; to : &lt;strong&gt;static&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Set the primary and secondary DNS address and click on Save/Apply.&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Perform a reboot of router to apply the changes.&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/syro-dns.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to claim free Azure certification  vouchers after attending Microsoft Events</title>
      <link>https://tanmay-bhat.github.io/posts/2021-04-26-how-to-claim-free-azure-certification-vouchers-after-attending-microsoft-events/</link>
      <pubDate>Mon, 26 Apr 2021 11:26:06 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-04-26-how-to-claim-free-azure-certification-vouchers-after-attending-microsoft-events/</guid>
      <description>&lt;p&gt;Microsoft is offering fundamentals exam vouchers for those who attend and complete their virtual training. You can take a look at upcoming events and register by going to &lt;a href=&#34;https://events.microsoft.com&#34;&gt;Microsoft Events&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://tanmay-bhat.github.io/ms-event.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then you can register for the training of an exam of your choice by searching it in the page.&lt;/p&gt;
&lt;p&gt;So far I have seen that Microsoft offers free exam vouchers for all fundamentals exam i.e.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/learn/certifications/azure-data-fundamentals/&#34;&gt;Azure Data Fundamentals&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/learn/certifications/azure-fundamentals/#certification-exams&#34;&gt;Azure Fundamentals&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to install netstat tool in openSUSE 15</title>
      <link>https://tanmay-bhat.github.io/posts/2021-01-25-how-to-install-netstat-tool-in-opensuse-15/</link>
      <pubDate>Mon, 25 Jan 2021 01:03:12 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-01-25-how-to-install-netstat-tool-in-opensuse-15/</guid>
      <description>&lt;p&gt;Even though the **netstat **tool is depreciated, sometimes we can&amp;rsquo;t stop the old habit and we arrive at a situation where its difficult to adapt to new things.&lt;/p&gt;
&lt;p&gt;Actually we should be using **ss **tool installed of netstat !&lt;/p&gt;
&lt;p&gt;All common network related tools are bundled with package net-tools.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nwlab:/etc # rpm -qa | grep net-tools
net-tools-2.0+git20170221.479bb4a-lp152.5.5.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However in openSUSE 15, the team decided to knock it off from net tools package!&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to SSH into docker in PWD (Play With Docker)</title>
      <link>https://tanmay-bhat.github.io/posts/2021-01-22-how-to-ssh-into-docker-in-pwd-play-with-docker/</link>
      <pubDate>Fri, 22 Jan 2021 10:28:28 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-01-22-how-to-ssh-into-docker-in-pwd-play-with-docker/</guid>
      <description>&lt;p&gt;Hey all ! For those of you who don&amp;rsquo;t  know what PWD is below is short explanation :&lt;/p&gt;
&lt;p&gt;So, PWD stands for Play With Docker. You can deploy   learn docker at free with time limit of each instance up-to 10 Hrs!&lt;/p&gt;
&lt;p&gt;For more info, go to : &lt;a href=&#34;https://labs.play-with-docker.com/&#34;&gt;Docker Labs&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There are two ways you can access the docker instance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use the web based console.&lt;/li&gt;
&lt;li&gt;SSH into that instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I always love to do ssh as it gives me more freedom.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to install mhVTL in openSUSE</title>
      <link>https://tanmay-bhat.github.io/posts/2021-01-14-how-to-install-mhvtl-in-opensuse/</link>
      <pubDate>Thu, 14 Jan 2021 23:42:03 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/posts/2021-01-14-how-to-install-mhvtl-in-opensuse/</guid>
      <description>&lt;p&gt;Lets see how to install mhVTL (a FOSS VTL software) written by a super hero called : Mark Harvey.&lt;br&gt;
There are 2 ways you can install mhvtl :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the rpm package.&lt;/li&gt;
&lt;li&gt;Directly compile the source code yourself.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I have tried using the first method as its easy and fast :D.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: I have tested this install in openSUSE 15.2&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;
&lt;p&gt;1.First update the packages to latest version available by typing :&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>https://tanmay-bhat.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://tanmay-bhat.github.io/about/</guid>
      <description>&lt;p&gt;Hey there &amp;#x1f44b;&lt;/p&gt;
&lt;p&gt;My name is Tanmay and I am a SRE / DevOps Engineer based in Banglore, India.&lt;/p&gt;
&lt;h2 id=&#34;education-mortar_board&#34;&gt;Education &amp;#x1f393;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Aug 16 - Jun 19&lt;/em&gt; : Banglore University
&lt;ul&gt;
&lt;li&gt;Bachelor of Science (BSc) in Computer Science&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;skills-computer&#34;&gt;Skills &amp;#x1f4bb;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Cloud&lt;/em&gt; : AWS, GCP, DigitalOcean&lt;/li&gt;
&lt;li&gt;&lt;em&gt;IaC&lt;/em&gt; :  Terraform, CrossPlane&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Orchestration&lt;/em&gt; : Kubernetes, Docker, ECS&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CI/CD&lt;/em&gt; : ArgoCD, FluxCD, Gitlab CI, Github Action&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Monitoring&lt;/em&gt; &amp;amp; logging : Prometheus, Victoria Metrics, Thanos, Grafana, Loki, AWS Cloudwatch&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Service Mesh&lt;/em&gt; : Cilium, Linkerd&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
