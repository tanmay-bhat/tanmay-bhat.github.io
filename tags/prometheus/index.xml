<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Prometheus on Tanmay Bhat</title>
    <link>https://tanmay-bhat.github.io/tags/prometheus/</link>
    <description>Recent content in Prometheus on Tanmay Bhat</description>
    <image>
      <url>https://tanmay-bhat.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://tanmay-bhat.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 03 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://tanmay-bhat.github.io/tags/prometheus/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to prevent metrics explosion in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/how-to-prevent-metrics-explosion-in-prometheus/</guid>
      <description>In this article, let’s go over some common metric sources and how to prevent the explosion of the metrics over time from them in Prometheus.
1. Node exporter:  Node exporter by default exposes ~ 977 different metrics per node. Depending on labels, this can easily by default create 1000 time series the moment node-exporter is started.   Although 1000 metrics per node doesn’t look huge at the beginning, but if you’re sending these metrics to any cloud vendor like Grafana cloud, AWS Prometheus and Google Cloud for Prometheus, this can be unnecessary cost burn as all cloud vendors calculate cost based on number of time series sent &amp;amp; stored.</description>
    </item>
    
    <item>
      <title>How to drop and delete metrics in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/how-to-drop-and-delete-metrics-in-prometheus/</guid>
      <description>Keeping your Prometheus optimized can be a tedious task over time, but it&amp;rsquo;s essential in order to maintain the stability of it and also to keep the cardinality under control.
Identifying the unnecessary metrics at source, deleting the existing unneeded metrics from your TSDB regularly will keep your Prometheus storage &amp;amp; performance intact.
In this article we’ll look at both identifying, dropping them at source and deleting the already stored metrics from Prometheus.</description>
    </item>
    
    <item>
      <title>How to calculate the storage space required for Prometheus server</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/how-to-calculate-the-storage-space-required-for-prometheus-server/</guid>
      <description>In this article, let&amp;rsquo;s try to estimate the Prometheus storage required for an environment.
Prometheus stores data in a time-series format and over time the targets which send metrics to the Prometheus server will get increased hence the number of metrics Prometheus ingests &amp;amp; stores will increase too leading to disk space issues.
From the docs:
 Prometheus stores an average of only 1-2 bytes per sample. Thus, to plan the capacity of a Prometheus server, you can use the rough formula :</description>
    </item>
    
    <item>
      <title>How to configure Readiness Probe alert in Prometheus</title>
      <link>https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/configure-readiness-probe-alert-prometheus/</guid>
      <description>This article aims to explain the steps to configure Readiness Probe failure alert in Prometheus.
Definition : Readiness Probe in Kubernetes is a probing mechanism to detect health (ready status) of a pod and if the health is intact, then allow the traffic to the pod.
From the official doc,
 Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup.</description>
    </item>
    
    <item>
      <title>Self monitoring Prometheus with Grafana</title>
      <link>https://tanmay-bhat.github.io/posts/prometheus-self-metrics/</link>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/prometheus-self-metrics/</guid>
      <description>Who will monitor the monitoring system ? Itself&amp;hellip;&amp;hellip;&amp;hellip;sounds a bit magical.
Since Prometheus monitors everything, it&amp;rsquo;s essential that we keep an eye on Prometheus so that over observability pillar stays strong.
If Prometheus goes down, you won&amp;rsquo;t be having any metrics, hence no alert for any services, scary stuff along with a call from your boss !!
Configuring Prometheus to monitor itself Prometheus exposes metrics about itself at /metrics endpoint, hence it can scrape and monitor its own health.</description>
    </item>
    
    <item>
      <title>How to configure Prometheus server as a remote receiver</title>
      <link>https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tanmay-bhat.github.io/posts/how-to-configure-prometheus-server-as-a-remote-receiver/</guid>
      <description>Push vs Pull Prometheus is by far the best OSS you can get in 2022 for self-hosted / SaaS monitoring.
There are other solutions that grew out of Prometheus for ex Thanos or Cortex.
I believe the reason for this is the simplicity that Prometheus offers for querying the metrics and the way it handles millions of time series.
Before we jump into the implementation, let’s learn a bit about Prometheus Pull based mechanism for monitoring.</description>
    </item>
    
  </channel>
</rss>
